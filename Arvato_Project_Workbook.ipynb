{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import missingno as msno\n",
    "from googletrans import Translator\n",
    "import Levenshtein\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pc\n",
    "import pickle\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import sys\n",
    "# from IPython.display import display\n",
    "# import pprint\n",
    "# import itertools\n",
    "# import math\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "'''\n",
    "custom modules\n",
    "'''\n",
    "# function similar to Excel's vlookup\n",
    "from vlookup import vlookup\n",
    "# function to view all contents of a dataframe\n",
    "from view_all import view_all\n",
    "# function for the project\n",
    "from section_1 import msno_overview\n",
    "from section_2 import similar_feature, attribute_cat, str_to_num, ger_to_eng, pv_min_max, \\\n",
    "    pv_meaning_score, pv_verify_null\n",
    "from section_3 import dataset_summary, view_feature\n",
    "from section_4 import miss_val_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install googletrans==4.0.0-rc1\n",
    "# pip install missingno\n",
    "# pip install pyarrow\n",
    "# custom modules for convenience are in root folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 azdias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file overview\n",
    "# '''\n",
    "\n",
    "# azdias = pd.read_csv('../csv_pickle/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "# azdias.name = 'azdias'\n",
    "#\n",
    "# print (azdias.info())\n",
    "# azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = pd.read_pickle('../csv_pickle/azdias.pickle')\n",
    "\n",
    "print (azdias.info())\n",
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# azdias.name = 'azdias'\n",
    "# msno_overview(azdias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file overview\n",
    "# '''\n",
    "\n",
    "# customers = pd.read_csv('../csv_pickle/Udacity_CUSTOMERS_052018.csv', sep=';')\n",
    "# print (customers.info())\n",
    "# customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_pickle('../csv_pickle/customers.pickle')\n",
    "\n",
    "print (customers.info())\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# customers.name = 'customers'\n",
    "# msno_overview(customers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "ref_info = pd.read_excel(\n",
    "    'DIAS Information Levels - Attributes 2017.xlsx', header=1).iloc[:, 1:]\n",
    "\n",
    "# fillna of Information level as values are only in the 1st rows\n",
    "ref_info['Information level'] = ref_info['Information level'].fillna(method = 'ffill')\n",
    "\n",
    "print (ref_info.info())\n",
    "ref_info.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "ref_attr = pd.read_excel(\n",
    "    'DIAS Attributes - Values 2017.xlsx', header=1).iloc[:, 1:]\n",
    "# rename column Value to Score for easier documentation\n",
    "ref_attr = ref_attr.rename(columns = {'Value' : 'Score'})\n",
    "\n",
    "print (ref_attr.info())\n",
    "ref_attr[103:108]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Baseline understanding\n",
    "\n",
    "There are 4 files given:\n",
    "* 2 dataset files - azdias & customers\n",
    "* 2 reference files - info & attr:  \n",
    "  - info is for information on dataset features which are in unreadable German acronyms  \n",
    "  - attr is for information on dataset values which are in numbers and acronyms,  \n",
    "  and has corresponding meanings  \n",
    "\n",
    "To establish a baseline, this section aims to identify key factors in the reference files   \n",
    "that can help in understanding the contents of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 feature alignment\n",
    "\n",
    "As datasets do not have information on what each column feature exactly means,  \n",
    "values of reference files have to be mapped to the features.\n",
    "  \n",
    "feature_desc below is for this needs, and formed with 369 column features from customers. \n",
    "\n",
    "After merging information columns, there are 264 features from both customers and info,  \n",
    "along with 105 exclusive features of customers lacking Description values.\n",
    "\n",
    "< 2개 내용을 합침  6/7>\n",
    "\n",
    "<!-- #### 2.2 feature alignment -->\n",
    "\n",
    "The improved readability of feature_desc has enhanced the understanding  \n",
    "of the overall structure of the datasets and the meanings of the features,  \n",
    "although some translations still remain unclear.  \n",
    "\n",
    "Since there are a few features that have similar contents, it is necessary  \n",
    "to perform imputation steps to resolve collinearity and reduce dimensionality  \n",
    "before proceeding with the modeling process.\n",
    "\n",
    "The following cells provide an overview of the number of features in each file  \n",
    "and illustrates the feature intersection and difference between the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    'azdias' : set(azdias.columns.unique()),\n",
    "    'customers' : set(customers.columns.unique()),\n",
    "    'ref_info' : set(ref_info.Attribute.dropna().unique()),\n",
    "    'ref_attr' : set(ref_attr.Attribute.dropna().unique()),\n",
    "    }\n",
    "\n",
    "print ('Number of features by file')\n",
    "for k, v in feature_dict.items():\n",
    "    print (k, ':', len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_dict['attr'] = set(attr.Attribute.dropna().unique())\n",
    "\n",
    "print ('Feature intersection & difference between files')\n",
    "for i in range (0, 4):\n",
    "    for j in range (0, 4):\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        key_1 = list(feature_dict.keys())[i]\n",
    "        key_2 = list(feature_dict.keys())[j]\n",
    "        set_1 = list(feature_dict.values())[i]\n",
    "        set_2 = list(feature_dict.values())[j]\n",
    "        \n",
    "        intsec = set_1.intersection(set_2)\n",
    "        ft_diff = set_1 - set_2\n",
    "        feature_dict[f'{key_1}_intsec_{key_2}'] = intsec\n",
    "        feature_dict[f'in_{key_1}_notin_{key_2}'] = ft_diff\n",
    "        \n",
    "        intsec_print = f'{key_1} & {key_2} intersection:'\n",
    "        ft_diff_print = f' /  in {key_1} & not-in {key_2}:'        \n",
    "        print (intsec_print, str(len(intsec)).rjust(41 - len(intsec_print)),\n",
    "               ft_diff_print, str(len(ft_diff)).rjust(40 - len(ft_diff_print)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Attribute of ref_info:너무 많은 피쳐가 reference 파일에 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_customers_similarity = similar_feature(\n",
    "    'in_ref_info_notin_customers', 'in_customers_notin_ref_info', feature_dict)\n",
    "info_customers_similarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "많은 경우 _RZ 서픽스에 의한 차이이므로 info를 수정한\n",
    "'''\n",
    "org_info = ref_info.copy()\n",
    "info_customers_similarity = info_customers_similarity.query('Attribute.str.len() > 5')\n",
    "\n",
    "ref_col = 'in_customers_notin_ref_info'\n",
    "ref_info = vlookup(ref_info, info_customers_similarity, lookup_col = ref_col)\n",
    "ref_info.Attribute = np.where(ref_info[ref_col].isna(), ref_info.Attribute, ref_info[ref_col])\n",
    "ref_info = ref_info.drop(ref_col, axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Attribute of ref_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_customers_similarity = similar_feature(\n",
    "    'in_ref_attr_notin_customers', 'in_customers_notin_ref_attr', feature_dict)\n",
    "attr_customers_similarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "많은 경우 _RZ 서픽스에 의한 차이이므로 attr를 수정한\n",
    "'''\n",
    "org_attr = ref_attr.copy()\n",
    "attr_customers_similarity = attr_customers_similarity.query('Attribute.str.len() > 8')\n",
    "\n",
    "ref_col = 'in_customers_notin_ref_attr'\n",
    "ref_attr = vlookup(ref_attr, attr_customers_similarity, lookup_col = ref_col)\n",
    "ref_attr.Attribute = np.where(ref_attr[ref_col].isna(), ref_attr.Attribute, ref_attr[ref_col])\n",
    "ref_attr = ref_attr.drop(ref_col, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict['ref_info'] = set(ref_info.Attribute.dropna().unique())\n",
    "feature_dict['ref_attr'] = set(ref_attr.Attribute.dropna().unique())\n",
    "\n",
    "print ('Feature intersection & difference between files')\n",
    "for i in range (0, 4):\n",
    "    for j in range (0, 4):\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        key_1 = list(feature_dict.keys())[i]\n",
    "        key_2 = list(feature_dict.keys())[j]\n",
    "        set_1 = list(feature_dict.values())[i]\n",
    "        set_2 = list(feature_dict.values())[j]\n",
    "        \n",
    "        intsec = set_1.intersection(set_2)\n",
    "        ft_diff = set_1 - set_2\n",
    "        feature_dict[f'{key_1}_intsec_{key_2}'] = intsec\n",
    "        feature_dict[f'in_{key_1}_notin_{key_2}'] = ft_diff\n",
    "        \n",
    "        intsec_print = f'{key_1} & {key_2} intersection:'\n",
    "        ft_diff_print = f' /  in {key_1} & not-in {key_2}:'        \n",
    "        print (intsec_print, str(len(intsec)).rjust(41 - len(intsec_print)),\n",
    "               ft_diff_print, str(len(ft_diff)).rjust(40 - len(ft_diff_print)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 reference file modification\n",
    "\n",
    "<내용 수정 필요. 위 마크 업과 비교하여>\n",
    "As datasets do not have information on what each column feature exactly means,  \n",
    "values of reference files have to be mapped to the features.\n",
    "  \n",
    "feature_desc below is for this needs, and formed with 369 column features from customers. \n",
    "\n",
    "After merging information columns, there are 264 features from both customers and info,  \n",
    "along with 105 exclusive features of customers lacking Description values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create feature_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "feature_desc with column features of customers and added information from ref_info \n",
    "'''\n",
    "\n",
    "feature_desc = pd.DataFrame(customers.columns, columns=['Attribute'])\n",
    "feature_desc = vlookup(feature_desc, ref_info, 'Attribute')\n",
    "\n",
    "'''\n",
    "To fill missing Descriptions, add some values from attr and create translation using googletrans.\n",
    "The result is saved in the root folder due to the time-consuming code execution.\n",
    "'''\n",
    "# # for Attribute without Description, add 12 Description values from ref_attr\n",
    "# feature_desc = vlookup(feature_desc, ref_attr, 'Attribute', 'Description', intsec = 'update')\n",
    "\n",
    "# # add category of Attribute\n",
    "# feature_desc = attribute_cat(feature_desc)\n",
    "\n",
    "# # add translated information to Desc\n",
    "# feature_desc['eng_Desc'] = np.where(\n",
    "#     feature_desc.Description.isna(),\n",
    "#     feature_desc.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# feature_desc['Desc'] = feature_desc.Description.fillna('') + feature_desc.eng_Desc.fillna('')\n",
    "\n",
    "# # column order\n",
    "# feature_desc = feature_desc[[\n",
    "#     'category_large', 'category_small', 'Attribute', 'Information level', 'Additional notes', 'Description', 'Desc']]\n",
    "\n",
    "feature_desc = pd.read_excel('feature_desc.xlsx', index_col = [0])\n",
    "print(feature_desc.info())\n",
    "feature_desc.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* clean ref_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "change numbers in str form\n",
    "'''\n",
    "# 2 numbers in 1 str value\n",
    "ref_attr.Score = ref_attr.Score.apply(\n",
    "    lambda x: [int(i) for i in x.split(',')] if isinstance(x, str) and ',' in x else x)\n",
    "ref_attr = ref_attr.explode('Score').reset_index(drop=True)\n",
    "# '-1' to -1\n",
    "ref_attr.Score = ref_attr.Score.apply(str_to_num)\n",
    "\n",
    "'''\n",
    "merge add-on information in some cells below Description\n",
    "'''\n",
    "# create attribute and description_shift with shift(-1)\n",
    "ref_attr['attribute_shift'] = ref_attr.Attribute.shift(-1).fillna('')\n",
    "ref_attr['description_shift'] = ref_attr.Description.shift(-1).fillna('')\n",
    "\n",
    "# merge Description and add-on\n",
    "ref_attr.Description = ref_attr.Description.mask(\n",
    "    (~(ref_attr['Attribute'].isna()) & (ref_attr['attribute_shift'].isna())),\n",
    "    ref_attr.Description + ' ' + ref_attr.description_shift)\n",
    "\n",
    "# replace add-on with np.nan\n",
    "ref_attr.Description = ref_attr.Description.mask(\n",
    "    (ref_attr['Attribute'].isna()) & ~(ref_attr['Description'].isna()),\n",
    "    np.nan)\n",
    "# drop description shift\n",
    "ref_attr = ref_attr.drop(['attribute_shift', 'description_shift'], axis = 1)\n",
    "\n",
    "'''\n",
    "fill blank cells with corresponding values as only 1st lines\n",
    "of Attribute & Description have values\n",
    "'''\n",
    "\n",
    "ref_attr[['Attribute', 'Description']] = ref_attr[\n",
    "    ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "'''\n",
    "keep 272 intersection features between ref_attr and datasets,\n",
    "and drop exclusive features of datasets that are not useful at this step\n",
    "'''\n",
    "\n",
    "ref_attr = ref_attr[ref_attr.Attribute.isin(feature_dict['ref_attr_intsec_customers'])] # query not working\n",
    "\n",
    "ref_attr = attribute_cat(ref_attr)\n",
    "ref_attr = ref_attr[['category_large', 'category_small', 'Attribute', 'Description', 'Score', 'Meaning']]\n",
    "\n",
    "print (f'number of Attribute: {ref_attr.Attribute.nunique()}')\n",
    "ref_attr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create pre-study dataframes by value form of Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "add delimiter column Score_form\n",
    "'''\n",
    "\n",
    "ref_attr = ref_attr.copy() # to avoid SettingWithCopyWarning\n",
    "ref_attr['Score_form'] = ref_attr.Score.map(type)\n",
    "ref_attr.Score_form.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "attr_num containing Scores denoted in number\n",
    "'''\n",
    "\n",
    "attr_num = ref_attr[ref_attr.Score_form == int].copy()\n",
    "print (f'number of Attribute(s): {attr_num.Attribute.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "attr_str containing Scores denoted in string\n",
    "'''\n",
    "\n",
    "attr_str = ref_attr[ref_attr.Score_form == str].copy()\n",
    "print (f'number of Attribute(s): {attr_str.Attribute.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "there is 1 feature that Scores are in both numeric and string forms\n",
    "'''\n",
    "\n",
    "set(attr_num.Attribute.unique()).intersection(set(attr_str.Attribute.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 pre-study using reference files\n",
    "\n",
    "After identifying the dataset structure in parts 2.1 and 2.2 above,  \n",
    "part 2.3 focuses on examining the dataset contents.  \n",
    "In this part, various aspects such as the ranges of feature values,  \n",
    "values actually meaning null, data types, and more are to be verified,  \n",
    "and preliminary notes will be recorded on pre-processing requirements.\n",
    "\n",
    "<!-- ##### 2.3.0 attr modification -->\n",
    "\n",
    "Considering that info contains information about dataset features, attr contains  \n",
    "information about dataset values and the values within the datasets can be understood  \n",
    "by mapping them to the corresponding Meaning values in attr.\n",
    "\n",
    "However, it is necessary to initially modify attr since it is not in a neat form:\n",
    "* 2 numbers in 1 str value like '-1, 0' or number(s) in str like '-1'\n",
    "* add-on information cell below Description value cell\n",
    "* only 1st rows of Attribute & Description with values, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 ref_attr Scores denoted in number\n",
    "\n",
    "먼저 attr의 contents를 보면 대부분의 데이터는 categorical 하며, 좀 더 효과적으로 데이터의 Score를 살펴보기 위해 각 Attribute의 min/max 값을 카테고리화 해 본 결과 아래와 같이 나왔다. \n",
    "\n",
    "< 아래 내용은 나중에 생사 >  \n",
    "From min/max categories 를 만들었으며, to be further checked are:   \n",
    "* If Score -1 and 0 contain data equal to null or -inf  \n",
    "* If max Score values contain data equal to null or inf\n",
    "* If features with wider min/max gap are discrete or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "summary of attr_num with min/max value observed\n",
    "'''\n",
    "\n",
    "attr_num_score = pv_min_max(attr_num, feature_desc)\n",
    "attr_num_score.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **data type of features with wider min/max Score gap**\n",
    "  \n",
    "attr의 scoring 체계가 주어졌거니와 max/min 차이를 보면 대부분의 데이터가 discrete 임을 알 수 있음.  \n",
    "그러나 min/max gap이 10 이상이면 continuous 일 수 있으므로 살펴 보았는데 이 또한 All features that have min/max Score gap over 10 are found to be discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check features that have min/max Score gap over 10\n",
    "'''\n",
    "# from attr_num_score above, extract Attribute values having min/max Score gap over 10\n",
    "temp_over10_attribute = attr_num_score[\n",
    "    (attr_num_score['max'] - attr_num_score['min']) > 10].Attribute.unique()\n",
    "\n",
    "temp_over10 = pv_meaning_score(\n",
    "    attr_num.query('Attribute in @temp_over10_attribute'), feature_desc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Meaning 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "어떤 값이 unknown을 갖나?\n",
    "'''\n",
    "\n",
    "ref_attr.query('Meaning == \"unknown\"').Score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "when Score value is -1, the meaning is 100% unknown in reference file\n",
    "따라서 dataset에서도 -1은 모두 null 처리가 reasonable함\n",
    "'''\n",
    "\n",
    "attr_num.query('Score < 0').Meaning.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE dict p_process to note points to be pre-processed\n",
    "'''\n",
    "\n",
    "p_process = {'2.3.1-minus1' : 'replace / All / {-1: np.nan}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-1이 아닌 경우는 null인 경우와 아닌 경우가 혼재\n",
    "따라서 위에서 정리한 Attribute의 Category 기반으로 0 이나 9인\n",
    "datasets Score 중 반드시 null로 처리할 것이 있는지 파악해 본다.\n",
    "'''\n",
    "print (attr_num.query('Score == 0').Meaning.unique())\n",
    "print (attr_num.query('Score == 9').Meaning.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "먼저 0의 경우 Meaning == 'unknown'인 category_large는\n",
    "'''\n",
    "\n",
    "unknown_0 = attr_num.query('Score == 0 and Meaning == \"unknown\"').category_large.unique()\n",
    "unknown_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "there category_large에서는 Score 0이더라도 Meaning != \"unknown\"이 발견되므로\n",
    "Score 0 이라 해도 일괄적으로 null을 적용할 수는 없음\n",
    "'''\n",
    "\n",
    "attr_num.query(\n",
    "    'category_large in (@unknown_0) and Score == 0 and Meaning != \"unknown\"').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0의 경우 Meaning == 'unknown'인 category_small은\n",
    "'''\n",
    "\n",
    "unknown_9 = attr_num.query('Score == 9 and Meaning == \"unknown\"').category_small.unique()\n",
    "unknown_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "there category_small에서는 Score 9이면 100% Meaning == \"unknown\"이므로\n",
    "동 category의 Score 9는 모두 null 처리함\n",
    "'''\n",
    "\n",
    "attr_num.query('category_small in (@unknown_9) and Score == 9 and Meaning != \"unknown\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE pre-processing items\n",
    "'''\n",
    "\n",
    "p_process['2.3.1-unknown_9'] = 'mask / category_small in unknown_9 / Score 9 to -1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 언노운 외의 null에 상응하는 value는 없는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "최소값 -1은 모두 null이며, 최소값 0 또는 1일 경우\n",
    "\n",
    "아래 내용은 나중에 생사\n",
    "when Score value is 0, it means none in most cases,\n",
    "and there are also some corresponding Meaning values\n",
    "that seem to be definitely equal to null\n",
    "'''\n",
    "\n",
    "# 최소값 0 Meaning 점검\n",
    "temp_min_0 = attr_num_score[attr_num_score['min'] == 0].Attribute\n",
    "print (attr_num.query('Attribute in (@temp_min_0) and Score == 0').Meaning.unique())\n",
    "\n",
    "# 최소값 -1 Meaning 점검\n",
    "temp_min_1 = attr_num_score[attr_num_score['min'] == 1].Attribute\n",
    "print (attr_num.query('Attribute in (@temp_min_1) and Score == 1').Meaning.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "먼저 확실하게 null을 의미하는 2개 Score는 to_null 리스트에 넣어 later, pre-processing에 사용하고, \n",
    "context에 따라 정확한 의미 파악이 필요한 나머지 값들은 maybe_null에 넣어 상세 점검을 함\n",
    "'''\n",
    "to_null = ['unknown', 'unknown / no main age detectable']\n",
    "maybe_null = ['no transaction known', 'no transactions known', 'none']\n",
    "\n",
    "'''\n",
    "메이비널 값을 컨텍스트 속에서 살펴보면\n",
    "- maybe_null은 은 null이 아니며 0을 의미함\n",
    "'''\n",
    "for i in maybe_null:\n",
    "    print (i)\n",
    "    verify_maybe_null = pv_verify_null(attr_num, feature_desc, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE pre-processing items\n",
    "'''\n",
    "\n",
    "p_process['2.3.1-1'] = 'mask / All / replace Score to null if corresponding Meaning is in to_null'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 ref_attr Scores denoted in string\n",
    "\n",
    "///// 98 features in str type are analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "summary of attr_str with Meaning and Score values\n",
    "'''\n",
    "\n",
    "attr_str_score = pv_meaning_score(attr_str, feature_desc, display = False)\n",
    "view_all(attr_str_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **7 features are found as continuous.**  \n",
    "  These features will checked in detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possibly containing continuous features \n",
    "pre_study_continuous = list(\n",
    "    attr_str_score[attr_str_score.Score.apply(lambda x: x == ['…'])].Attribute.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset exploration\n",
    "\n",
    "Based on baseline understanding and frame works from reference files in section_2,  \n",
    "section_3 is dealing with actual datasets.  \n",
    "In this section, steps taken in the previous section will be repeated to fine-tune  \n",
    "and finalize pro-processing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create a concatenated dataframe of 2 datasets for temporary use\n",
    "'''\n",
    "\n",
    "concat_data = pd.concat([customers, azdias], axis=0).reset_index(drop = True)\n",
    "concat_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "datasets containing intersection features in both of datasets and ref_attr\n",
    "'''\n",
    "\n",
    "datasets = concat_data[\n",
    "    list(feature_dict['customers_intsec_ref_attr'])].copy()\n",
    "datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "datasets_excl containing exclusive features of datasets\n",
    "'''\n",
    "\n",
    "datasets_excl = concat_data[\n",
    "    list(feature_dict['in_customers_notin_ref_attr'])].copy()\n",
    "datasets_excl.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 intersection features in both of datasets and ref_attr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 numeric dataset Scores of intersection features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "datasets_num_score containing summary of numeric Score values\n",
    "'''\n",
    "\n",
    "datasets_num_score = dataset_summary(datasets, feature_desc, ['int', 'float'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check if Scores in actual datasets are within range given by ref_attr.\n",
    "'''\n",
    "\n",
    "# merge min/max values by Attribute in attr_num_score in 2.3.1\n",
    "temp_range_check = vlookup(\n",
    "    datasets_num_score, attr_num_score, 'Attribute', ['min', 'max', 'min_max_cat'], intsec = 'copy')\n",
    "\n",
    "temp_range_check['check'] = np.where(\n",
    "    temp_range_check['min_max_cat_'].isnull(), \n",
    "    'not in attr_num',\n",
    "    np.where(\n",
    "        (temp_range_check['min'] < temp_range_check['min_']),\n",
    "         'min lower',\n",
    "         np.where((temp_range_check['max'] > temp_range_check['max_']),\n",
    "         'max higher',\n",
    "         'within min/max range')))\n",
    "\n",
    "temp_range_check.check.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* features out of Score range of ref_attr  \n",
    "\n",
    "293 피쳐는 pre_study에서 살펴본 Range 안에 Score가 위치하므로 Section2에서 정리한 pre-processing을 진행하면 됨\n",
    "\n",
    "레인지를 벗어난 피쳐에 대해서는 아래에서 살펴보고 처리방안을 정리함. 또한 피쳐 이름에서 데이터 유사성이 발견되어 유사 데이터는 드롭하기로 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4 features are showing lower min value in dataset \n",
    "'''\n",
    "temp_range_check.query('check == \"min lower\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "these 4 features of datasets have 0 as min Score.\n",
    "as Meaning values of these 4 features in ref_attr do not cover null value,\n",
    "Score 0 should be replaced as null (see example below)\n",
    "'''\n",
    "\n",
    "ref_attr.query('Attribute == \"LP_LEBENSPHASE_GROB\"').Meaning.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE pre-processing items\n",
    "'''\n",
    "\n",
    "for i in temp_range_check.query('check == \"min lower\"').Attribute.unique():\n",
    "    p_process['3.1.1-'+ i] = 'replace / ' + i + ' / {0: -1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "위 피쳐의 이름을 보면 gross와 fine으로 나뉘어 있으며, 같은 피쳐에 그로스/파인이 들어 있는 항목은\n",
    "유사피쳐를 크게 나누느냐 작게 나누느냐의 차이임\n",
    "'''\n",
    "ref_attr.query('Attribute.str.contains(\"LP_FAMILIE\")')\n",
    "# attribute_without_meaning[attribute_without_meaning.Score > 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "유사한 gross와 fine이 있는 피쳐는\n",
    "'''\n",
    "suffix_gross_fine = ['GROB', 'FEIN']\n",
    "features_gross_fine = feature_desc[\n",
    "    feature_desc.Attribute.str.split('_').str[-1].isin(suffix_gross_fine)]\n",
    "features_gross_fine = list(features_gross_fine.Attribute.unique())\n",
    "features_gross_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "유사한 gross가 있는 피쳐만 list로 정리하여 drop 함\n",
    "'''\n",
    "\n",
    "to_drop = feature_desc.query('Attribute.str.contains(\"GROB\")')\n",
    "to_drop = list(to_drop.Attribute.unique())\n",
    "\n",
    "'''\n",
    "NOTE pre-processing items\n",
    "'''\n",
    "\n",
    "p_process['2.3.1-drop'] = 'drop / All / drop features in to_drop'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* continuous features:   \n",
    "  뭐뭐뭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7 features are continuous:\n",
    "in ref_attr, Score values of these continuous features have string form (see 2.3.2) \n",
    "'''\n",
    "continuous_in_datasets = temp_range_check.query('check == \"not in attr_num\"')\n",
    "print (set(continuous_in_datasets.Attribute.unique()) == set(pre_study_continuous))\n",
    "continuous_in_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_PERSONEN  \n",
    "- Value 0 might be strange or extreme but is possible\n",
    "- outliers: household with over 10 adult persons is highly extreme or data error,\n",
    "  that Value over 10 should be replaced to NaN\n",
    "'''\n",
    "\n",
    "view_feature (datasets, feature_desc, 'ANZ_PERSONEN', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.1.1-ANZ_PERSONEN'] = 'np.where / ANZ_PERSONEN / Score > 10 to -1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_TITEL\n",
    "- it seems that there is no pre-processing needed.\n",
    "  However, this feature is linked to ANZ_PERSONEN above\n",
    "  that Score should be replaced to null if corresponding ANZ_PERSONEN is null.\n",
    "'''\n",
    "\n",
    "view_feature (datasets, feature_desc, 'ANZ_TITEL', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.1.1-ANZ_TITEL'] = 'np.where / ANZ_TITEL / Score of ANZ_PERSONEN > 10 to -1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_HH_TITEL  \n",
    "- No pre-processing needed: max values might be strange or extreme but are possible\n",
    "'''\n",
    "\n",
    "view_feature (datasets, feature_desc, 'ANZ_HH_TITEL', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "KBA13_ANZAHL_PKW\n",
    "- in this feature values over 1250 is grouped by 100. \n",
    "  Log scaling is needed due to high skewness.\n",
    "'''\n",
    "\n",
    "view_feature (datasets, feature_desc, 'KBA13_ANZAHL_PKW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.1.1-KBA13_ANZAHL_PKW'] = 'log_scaling / KBA13_ANZAHL_PKW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN_GEBAEUDEJAHR   \n",
    "- No pre-processing needed\n",
    "'''\n",
    "\n",
    "view_feature (datasets, feature_desc, 'MIN_GEBAEUDEJAHR', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GEBURTSJAHR  \n",
    "- outliers: birth year can not be 0 and anyone born before 1900 is extreme\n",
    "  that Value < 1900 should be replaced to null\n",
    "'''\n",
    "\n",
    "view_feature (datasets, feature_desc, 'GEBURTSJAHR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.1.1-GEBURTSJAHR'] = 'np.where / GEBURTSJAHR / Score < 1900 to -1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_HAUSHALTE_AKTIV  \n",
    "- No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "- Log scaling is needed due to high skewness\n",
    "'''\n",
    "\n",
    "view_feature (datasets, feature_desc, 'ANZ_HAUSHALTE_AKTIV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.1.1-ANZ_HAUSHALTE_AKTIV'] = 'log_scaling / HAUSHALTE_AKTIV'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 object dataset Scores of intersection features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 뭐뭐뭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_obj_score = dataset_summary(datasets, feature_desc, ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check if Scores in actual datasets are within range given by attr.\n",
    "'''\n",
    "\n",
    "# merge min/max values by Attribute in attr_num_score in 2.3.1\n",
    "for i in datasets_obj_score.Attribute.unique():\n",
    "    score_in_datasets = set(datasets[i].unique())\n",
    "    score_in_ref_attr = set(ref_attr.query('Attribute == \"{}\"'.format(i)).Score.values)\n",
    "    \n",
    "    print (i)\n",
    "    print (f'a) scores in datasets: {score_in_datasets}')\n",
    "    print (f'b) scores in ref_attr: {score_in_ref_attr}')\n",
    "    print (f'a) b) intersection: {score_in_datasets.intersection(score_in_ref_attr)}')\n",
    "    print (f'a) - b): {score_in_datasets - score_in_ref_attr}')\n",
    "    print (f'b) - a): {score_in_ref_attr - score_in_datasets}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "피쳐 2개는 Score가 추치가 아니며 나머지 2개는 수치 Score를 string으로 기록한 경우가 있으며\n",
    "'XX', 'X'로 null이 표기되어 있음\n",
    "'''\n",
    "\n",
    "none_numeric_in_datasets = ['CAMEO_DEU_2015', 'OST_WEST_KZ']\n",
    "\n",
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.1.2-CAMEO_INTL_2015XX'] = 'replace / CAMEO_INTL_2015 / {\"XX\": -1}'\n",
    "p_process['3.1.2-CAMEO_DEU_2015'] = 'replace / CAMEO_DEU_2015 / {\"XX\": -1}'\n",
    "p_process['3.1.2-CAMEO_DEUG_2015X'] = 'replace / CAMEO_DEUG_2015 / {\"X\": -1}'\n",
    "p_process['3.1.2-dtype'] = 'astype / All features in numeric_discrete_features / change dtype to float'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 exclusive features NOT in reference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "datasets_excl containing exclusive features of datasets\n",
    "'''\n",
    "\n",
    "datasets_excl = concat_data[\n",
    "    list(feature_dict['in_customers_notin_ref_attr'])].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 numeric dataset Scores of exclusive features\n",
    "\n",
    "뭐뭐뭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "excl_num_score containing Scores denoted in number\n",
    "'''\n",
    "\n",
    "datasets_excl_num_score = dataset_summary(datasets_excl, feature_desc, ['int', 'float'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **data type of features with wider min/max Score gap**\n",
    "  \n",
    "min/max Score gap is not big. 아무 reference가 없으니 section_2에서 정리한 일반 원칙 적용하여 pre-processing\n",
    "\n",
    "All features that have min/max Score gap over 10 and null portion below 50% are to be checked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check features that have min/max Score gap over 10\n",
    "나머지 피쳐 중 드롭한 50% 이상 빈 피쳐를 제외하면 6개가 컨티뉴어스임 \n",
    "'''\n",
    "# from datasets_excl_num_score above, extract Attribute values having min/max Score gap over 10\n",
    "continuous_in_datasets_excl_attribute = datasets_excl_num_score[\n",
    "    (datasets_excl_num_score['max'] - datasets_excl_num_score['min']) > 10].Attribute.unique()\n",
    "\n",
    "continuous_in_datasets_excl = datasets_excl_num_score.query(\n",
    "    'Attribute in @continuous_in_datasets_excl_attribute and null_portion < 0.5')\n",
    "view_all(continuous_in_datasets_excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "디스크리트인 2개는 디스크립션을 보면 카테고리임\n",
    "'''\n",
    "continuous_in_datasets_excl = continuous_in_datasets_excl.query(\n",
    "    'Attribute != \"ALTERSKATEGORIE_FEIN\" and Attribute != \"GEMEINDETYP\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "LNR is the serial index of dataset 이므로 드롭함\n",
    "'''\n",
    "\n",
    "to_drop.append('LNR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_STATISTISCHE_HAUSHALTE   \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "  - Log scaling is needed due to high skewness\n",
    "'''\n",
    "\n",
    "view_feature (datasets_excl, feature_desc, 'ANZ_STATISTISCHE_HAUSHALTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.2.1-ANZ_STATISTISCHE_HAUSHALTE'] = 'log_scaling / ANZ_STATISTISCHE_HAUSHALTE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_KINDER  \n",
    "  - 0이 매우 많기는 하나 정확한 뜻을 알 수 없으니 살림\n",
    "'''\n",
    "\n",
    "view_feature (datasets_excl, feature_desc, 'ANZ_KINDER', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VK_DISTANZ  \n",
    "- No pre-processing needed although Eng translation not completely understandable\n",
    "'''\n",
    "\n",
    "view_feature (datasets_excl, feature_desc, 'VK_DISTANZ', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERDICHTUNGSRAUM   \n",
    "- No pre-processing needed although Eng translation not completely understandable\n",
    "'''\n",
    "\n",
    "view_feature (datasets_excl, feature_desc, 'VERDICHTUNGSRAUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "EINGEZOGENAM_HH_JAHR \n",
    "- outliers: Eng translation is not completely understandable, but 3 outliers can be replaced to null\n",
    "'''\n",
    "\n",
    "view_feature (datasets_excl, feature_desc, 'EINGEZOGENAM_HH_JAHR', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2.1-EINGEZOGENAM_HH_JAHR'] = 'np.where / EINGEZOGENAM_HH_JAHR / Score < 1980 to -1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 object dataset Scores of exclusive features\n",
    "\n",
    "///// For ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "datasets_excl_obj_score containing Scores denoted in obj\n",
    "'''\n",
    "\n",
    "datasets_excl_obj_score = dataset_summary(datasets_excl, feature_desc, ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4개 모두 디스크리트임\n",
    "'''\n",
    "\n",
    "none_numeric_in_datasets_excl = ['D19_LETZTER_KAUF_BRANCHE', 'CUSTOMER_GROUP', 'EINGEFUEGT_AM', 'PRODUCT_GROUP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "datetime으로 변환하데 년도만 남김 as there are too many unique values\n",
    "'''\n",
    "\n",
    "p_process['3.2.2-EINGEFUEGT_AM'] = 'astype / EINGEFUEGT_AM / change dtype to datetime only with year values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check if abnormal Scores exist in actual datasets.\n",
    "으나 전처리 내용은 없음\n",
    "'''\n",
    "# 제외\n",
    "datasets_excl_obj_score = datasets_excl_obj_score.query('Attribute != \"EINGEFUEGT_AM\"')\n",
    "\n",
    "# \n",
    "for i in datasets_excl_obj_score.Attribute.unique():\n",
    "    \n",
    "    print (i)\n",
    "    print (f'scores in datasets: {set(datasets_excl[i].unique())}', '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorize pre-processing items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create pre-processing items\n",
    "'''\n",
    "# transform p_process from dictionary to dataframe \n",
    "p_process_items = {'pre_processing_itmes' : p_process}\n",
    "p_process_items = pd.DataFrame(p_process_items).reset_index()\n",
    "p_process_items [['method', 'by_feature']] = p_process_items[\n",
    "    'pre_processing_itmes'].str.split(' / ', n=1, expand=True)\n",
    "p_process_items = p_process_items[['index', 'method', 'by_feature']].sort_values(['method', 'index'])\n",
    "view_all(p_process_items)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create pre-processing reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "replacements dictionary\n",
    "'''\n",
    "p_process_replace = p_process_items.query('method == \"replace\"').iloc[1:]\n",
    "p_process_replace[['col', 'dict_val']] = p_process_replace[\n",
    "    'by_feature'].str.split(' / ', expand=True)\n",
    "\n",
    "# replacements = dict(zip(p_process_replace['col'], p_process_replace['dict_val']))\n",
    "replacements = dict(map(lambda col, dict_val: (col, eval(dict_val)), \n",
    "                        p_process_replace['col'], p_process_replace['dict_val']))\n",
    "replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "continuous and discrete features\n",
    "'''\n",
    "\n",
    "# continuous_features form 3.1.1 and 3.2.1\n",
    "continuous_features = list(\n",
    "    continuous_in_datasets.Attribute.unique()) + list(continuous_in_datasets_excl.Attribute.unique())\n",
    "\n",
    "# discrete_features\n",
    "discrete_features = list(x for x in datasets.columns if x not in continuous_features)\n",
    "none_numeric_discrete_features = none_numeric_in_datasets + none_numeric_in_datasets_excl\n",
    "numeric_discrete_features = list(x for x in discrete_features if x not in none_numeric_discrete_features + to_drop)\n",
    "\n",
    "# save lists as pickle\n",
    "for i, j in zip(\n",
    "    [continuous_features, discrete_features, none_numeric_discrete_features, numeric_discrete_features],\n",
    "    ['continuous_features', 'discrete_features', 'none_numeric_discrete_features', 'numeric_discrete_features']):\n",
    "    \n",
    "    pickle_list = pickle.dumps(i)\n",
    "    with open(j + '.pkl', 'wb') as f:\n",
    "        f.write(pickle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(\n",
    "    [continuous_features, discrete_features, none_numeric_discrete_features, numeric_discrete_features],\n",
    "    ['continuous_features', 'discrete_features', 'none_numeric_discrete_features', 'numeric_discrete_features']):\n",
    "    \n",
    "    pickle_list = pickle.dumps(i)\n",
    "    with open(j + '.pkl', 'wb') as f:\n",
    "        f.write(pickle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mask to process Scores corresponding to Meaning in to_null\n",
    "'''\n",
    "\n",
    "mask_to_null = ref_attr.query('Meaning in (@to_null)')[['Attribute', 'Score']].set_index('Attribute')\n",
    "mask_to_null = mask_to_null[mask_to_null.Score != -1]\n",
    "\n",
    "features_unknown_9 = [i for i in feature_desc.Attribute.unique() if i.split('_')[0] in unknown_9]\n",
    "mask_unknown_9 = pd.DataFrame(index = features_unknown_9, columns = ['Score'], data = 9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all(p_process_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing (data):\n",
    "    '''\n",
    "    function to pre_process datasets\n",
    "    '''        \n",
    "    # drop\n",
    "    data = data.drop(to_drop, axis = 1)\n",
    "\n",
    "    # mask\n",
    "    mask = data.eq(mask_to_null['Score'], axis=1)\n",
    "    data = data.mask(mask)\n",
    "    \n",
    "    mask = data.eq(mask_unknown_9['Score'], axis=1)\n",
    "    data = data.mask(mask)\n",
    "\n",
    "    # np.where\n",
    "    data['ANZ_PERSONEN'] = np.where(data['ANZ_PERSONEN'] > 10, -1, data['ANZ_PERSONEN'])\n",
    "    data['ANZ_TITEL'] = np.where(data['ANZ_PERSONEN'] > 10, -1, data['ANZ_TITEL'])\n",
    "    data['GEBURTSJAHR'] = np.where(data['GEBURTSJAHR'] < 1900, -1, data['GEBURTSJAHR'])\n",
    "    data['EINGEZOGENAM_HH_JAHR'] = np.where(data['EINGEZOGENAM_HH_JAHR'] < 1980, -1, data['EINGEZOGENAM_HH_JAHR'])\n",
    "\n",
    "    # replacement\n",
    "    data = data.replace(replacements)\n",
    "    data = data.replace(-1, np.nan)\n",
    "\n",
    "    # astype\n",
    "    data[numeric_discrete_features] = data[numeric_discrete_features].astype(float)  \n",
    "    data['EINGEFUEGT_AM'] = pd.to_datetime(data['EINGEFUEGT_AM'], format = '%Y-%m-%d').dt.year \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_azdias = azdias.copy()\n",
    "azdias = pre_processing (azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_customers = customers.copy()\n",
    "customers = pre_processing (customers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* drop columns and rows with missing values over threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''missing column overview: azdias'''\n",
    "\n",
    "azdias.name = 'azdias'\n",
    "miss_val_summary (azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''missing column overview: customers'''\n",
    "\n",
    "customers.name = 'customers'\n",
    "miss_val_summary (customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "columns of azdias with missing values over 33%\n",
    "'''\n",
    "temp_missing_col = azdias.isnull().mean()\n",
    "azdias_missing_col = list(temp_missing_col[temp_missing_col > 0.33].index)\n",
    "print ('azdias missing columns:', azdias_missing_col)\n",
    "\n",
    "temp_missing_col = customers.isnull().mean()\n",
    "customers_missing_col = list(temp_missing_col[temp_missing_col > 0.33].index)\n",
    "print ('customers missing columns:', customers_missing_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''drop columns with missing values over 33%'''\n",
    "\n",
    "azdias = azdias.drop(azdias_missing_col, axis = 1)\n",
    "customers = customers.drop(customers_missing_col, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''missing row overview: azdias'''\n",
    "\n",
    "azdias.name = 'azdias'\n",
    "miss_val_summary (azdias, axis_val = 'row', bar_chart = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''missing row overview: customers'''\n",
    "\n",
    "customers.name = 'customers'\n",
    "miss_val_summary (customers, axis_val = 'row', bar_chart = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''drop columns with missing values over 33%'''\n",
    "\n",
    "temp_missing_row = azdias.isnull().sum(axis = 1) / azdias.shape[1]\n",
    "temp_missing_row = temp_missing_row > 0.33\n",
    "azdias = azdias[~temp_missing_row]\n",
    "\n",
    "temp_missing_row = customers.isnull().sum(axis = 1) / customers.shape[1]\n",
    "temp_missing_row = temp_missing_row > 0.33\n",
    "customers = customers[~temp_missing_row]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* save pre-processed files as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(azdias)\n",
    "pq.write_table(table, 'azdias.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(customers)\n",
    "pq.write_table(table, 'customers.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785421, 350)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''load pre-preocessed datasets'''\n",
    "\n",
    "azdias = pq.read_table('azdias.parquet').to_pandas()\n",
    "azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140400, 353)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers = pq.read_table('customers.parquet').to_pandas()\n",
    "customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load lists of feature category'''\n",
    "for i in ['continuous_features', 'discrete_features', 'none_numeric_discrete_features', 'numeric_discrete_features']:\n",
    "    \n",
    "    with open(i + '.pkl', 'rb') as f:\n",
    "        pickle_list = f.read()\n",
    "\n",
    "    globals()[i] = pickle.loads(pickle_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work / Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def col_val (df):\n",
    "#     '''\n",
    "#     function to check values of dataframe columns\n",
    "#     df : dataframe\n",
    "#     '''\n",
    "#     # for i in df.columns:\n",
    "#     #     print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#     #         df[i].value_counts(), '\\n', '*     *     *')\n",
    "#     for i in df.columns:\n",
    "#         print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#         list(df[i].unique()), '\\n', '*     *     *')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from googletrans import Translator\n",
    "\n",
    "# # Create an instance of the Translator\n",
    "# translator = Translator(service_urls=['translate.google.com'])\n",
    "\n",
    "# # Text to be translated\n",
    "# text = \"AGER_TYP\"\n",
    "\n",
    "# # Translate the text from German to English\n",
    "# translation = translator.translate(text, src='de', dest='en')\n",
    "\n",
    "# # Print the translated text\n",
    "# print(\"Original text (German):\", text)\n",
    "# print(\"Translated text (English):\", translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def miss_val_hist(df, axis_val, x_bin = 10):\n",
    "#     '''\n",
    "#     function to display missing value histogram by column or raw\n",
    "#     df: dataframe\n",
    "#     axis_val: str, one of 'column' or 'row'\n",
    "#     x_bin: size of xtick bin, 10 as default \n",
    "#     '''\n",
    "#     # axis value\n",
    "#     axis_num = 0 if axis_val == 'column' else 1\n",
    "    \n",
    "#     # % of missing values\n",
    "#     missing_pct = df.isnull().mean(axis = axis_num) * 100\n",
    "\n",
    "#     # max % of missing values by column\n",
    "#     missing_pct_max = missing_pct.max()\n",
    "#     print ('max % of missing values by ' + axis_val + ': ', missing_pct_max)\n",
    "\n",
    "#     # plot missing values by column\n",
    "    \n",
    "#     print (missing_pct.describe())\n",
    "    \n",
    "#     x_range = ((missing_pct_max + x_bin * 2) // x_bin) * x_bin\n",
    "\n",
    "#     ax = missing_pct.plot(\n",
    "#         kind = 'hist', figsize=(10, 3), color='gray',\n",
    "#         bins = np.arange(0, x_range, 10),\n",
    "#         title = (df.name + ': missing value by ' + axis_val)\n",
    "#         )\n",
    "#     ax.set_xlabel('% of missing value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이써닉 하지 못한 코드\n",
    "# # missing value overview\n",
    "# for i in range(0, ((azdias.shape[1] + 100) // 100) * 100, 100):\n",
    "#     msno.matrix(azdias.iloc[:, i : i + 99],\n",
    "#                 figsize=(10, 3), fontsize = 12, labels = False, sparkline = False)\n",
    "#     plt.title('missing value overview: col ' + str (i) + ' to ' + str (min(i + 99, azdias.shape[1] - 1)),\n",
    "#               fontsize = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of over 30%\n",
    "# (azdias.isnull().mean() * 100 > 30).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of 25% to 30%\n",
    "# ((azdias.isnull().mean() * 100 > 25) & (30 >= azdias.isnull().mean() * 100)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아마도 쓰지 않을 plot\n",
    "# plot = azdias_col_missing_pct.plot(\n",
    "#     kind = 'bar', figsize=(10, 3), color='dimgray', xticks = [],\n",
    "#     title = 'azdias_col_missing_pct',\n",
    "#     xlabel = '366 columns',\n",
    "#     ylabel = '% of missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_df = customers.select_dtypes(include=['float', 'int64']).iloc[:, 1:]\n",
    "# num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pd.unique(customers.select_dtypes(include='float').values.flatten()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\n",
    "#     'display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(pd.DataFrame(attr.apply(lambda x: x.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_not_null = ~(attr.Attribute.isna())\n",
    "# attr.loc[attr_not_null, 'Description'] = attr.loc[\n",
    "#     attr_not_null, 'Description'] + ' ' + attr.loc[attr_not_null, 'desc_shift']\n",
    "\n",
    "# desc_to_null = (attr.Attribute.isna()) & ~(attr.Description.isna())\n",
    "# attr.loc[desc_to_null, 'Description'] = np.nan\n",
    "# attr = attr.drop(columns = 'desc_shift')\n",
    "# attr.loc[attr_with_value.shift(-1, fill_value = True), 'Description']\n",
    "# attr_shift = attr_null.shift\n",
    "# attr[attr_null.shift, 'Description'] = attr.loc[\n",
    "#     attr_null.shift(fill_value = False), 'Description'] + ' ' + attr[attr_null, 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# info_mg = info_mg.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "# # attr_excl = attr[attr.Attribute.isin(attr_excl)].copy()\n",
    "# attr_not_null = attr.dropna(subset = 'Attribute').copy()\n",
    "# attr_not_null = attr_not_null.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_not_null[['Attribute', 'Description']]],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_excl)][['Attribute', 'Description']].copy()\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# # info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_all(info_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 2258 rows of Attribute in attr, add 13 values exclusively in info,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# attr_mg = attr.copy()\n",
    "# info_excl = info[info.Attribute.isin(info_excl)][['Attribute', 'Description']].copy()\n",
    "# info_excl['Value'] = 'form info'\n",
    "# info_excl['Meaning'] = 'form info'\n",
    "\n",
    "# attr_mg = pd.concat(\n",
    "#     [attr_mg, info_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# # info_mg = info_mg.drop_duplicates()\n",
    "\n",
    "# # info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(attr_mg.info())\n",
    "# attr_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_mg.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# there are 93 and 51 exclusive values in data and information files\n",
    "# '''\n",
    "\n",
    "# datafile_Attr = azdias_Attr.union(customers_Attr)\n",
    "# infofile_Attr = info_Attr.union(attr_Attr)\n",
    "\n",
    "# datafile_excl = datafile_Attr - infofile_Attr\n",
    "# infofile_excl = infofile_Attr - datafile_Attr\n",
    "\n",
    "# print (len(datafile_excl), 'Attribute value(s) exclusively in data files:',\n",
    "#        '\\n', datafile_excl)\n",
    "# print (len(infofile_excl), 'Attribute value(s) exclusively in information files:',\n",
    "#        '\\n', infofile_excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# info['Information level'] = info['Information level'].fillna(method = 'ffill')\n",
    "\n",
    "# info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# attr[['Attribute', 'Description']] = attr[\n",
    "#     ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "# attr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# To enhance readability of data set - over 100 features are without description -   \n",
    "# I added 14 Description values from attr, and made a column of translation   \n",
    "# (ger_to_eng) to df_feature using code below.\n",
    "# However as this code-running takes somewhat long time, I saved the dataframe processed\n",
    "# as df_feature.xlsx in root folder\n",
    "# '''\n",
    "\n",
    "# # For values of Attribute without Description, add 14 Description values from attr\n",
    "# df_feature.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# df_feature.update(attr_excl)\n",
    "\n",
    "# df_feature.reset_index(inplace = True)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# df_feature['ger_to_eng'] = np.where(\n",
    "#     df_feature.Description.isnull(),\n",
    "#     df_feature.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# df_feature['Desc'] = df_feature.Description.fillna('') + df_feature.ger_to_eng.fillna('')\n",
    "\n",
    "# # # sort by Attribute and Information level\n",
    "# # df_feature.sort_values(by = ['Attribute', 'Information level'], inplace= True)\n",
    "\n",
    "# # df_feature = pd.read_excel('df_feature.xlsx', index_col = [0])\n",
    "# # df_feature.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# df_feature.Description = df_feature.Description.mask(\n",
    "#     df_feature.Attribute == attr_excl.Attribute,\n",
    "#     attr_excl.Description\n",
    "#     )\n",
    "# print ('Attributes missing Description:', df_feature[df_feature.Description.isna()].shape[0])\n",
    "# print (df_feature.shape)\n",
    "# df_feature.head()\n",
    "\n",
    "# ValueError: Can only compare identically-labeled Series objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_desc.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_excl = attr[\n",
    "#     attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# feature_desc.update(attr_excl)\n",
    "\n",
    "# feature_desc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between data files: \n",
    "# df customers has 3 more exclusive columns\n",
    "# '''\n",
    "\n",
    "# azdias_Attr = set(azdias.columns)\n",
    "# customers_Attr = set(customers.columns)\n",
    "\n",
    "# print(azdias_Attr - customers_Attr)\n",
    "# print(customers_Attr - azdias_Attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between information files\n",
    "# '''\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "\n",
    "# # info_excl = info_Attr - attr_Attr\n",
    "# # attr_excl = attr_Attr - info_Attr\n",
    "\n",
    "# print (len(info_Attr - attr_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - info_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - info_Attr)\n",
    "# '''\n",
    "# alignment of features between customers and information files\n",
    "# '''\n",
    "# print ('Attribute between customers and info')\n",
    "# print (len(customers_Attr - info_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - info_Attr)\n",
    "# print (len(info_Attr - customers_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - customers_Attr)\n",
    "# print ('In', len(info_Attr), 'features of info,', \n",
    "#        len(info_Attr) - len(info_Attr - customers_Attr), 'features are in Attribute of customers', '\\n')\n",
    "\n",
    "# print ('Attribute between customers and attr')\n",
    "# print (len(customers_Attr - attr_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - customers_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - customers_Attr)\n",
    "# print ('In', len(attr_Attr), 'features of attr,',\n",
    "#        len(attr_Attr) - len(attr_Attr - customers_Attr), 'features are in Attribute of customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(feature_dict.keys())[0]\n",
    "# feature_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = np.array([1, 2, 3, 4])\n",
    "\n",
    "# subtractions = np.subtract.outer(values, values)[np.triu_indices(len(values), k=1)]\n",
    "\n",
    "# for result in subtractions:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (k, v) in enumerate(feature_dict.items()):\n",
    "#     for j in range(i + 1, 4):\n",
    "#             result = values[i] - values[j]\n",
    "#             print(f\"{values[i]} - {values[j]} = {result}\")\n",
    "#     print (i, k, v)\n",
    "    \n",
    "# for i, (k, v) in enumerate(zip(list(feature_dict.keys()), list(feature_dict.values()))):\n",
    "#     print (i, (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# customers에는 없고 attr에만 있는 42개 Attribute는 Desc가 없으으로\n",
    "# (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# print ('customers에는 없고 attr에만 있는 42개 Attribute 수:', \n",
    "#        attr_num[attr_num.Desc.isna() == True].Attribute.nunique(),\n",
    "#        '\\n')\n",
    "\n",
    "# attr_num = attr_num[attr_num.Desc.isna() == False]\n",
    "# print (attr_num.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# str data of column Value\n",
    "# '''\n",
    "# # attr_num with only numeric values in Value\n",
    "# attr_str = attr[attr['Value_dtype'] == str].copy()\n",
    "# print (attr_str.shape) \n",
    "\n",
    "# # add Desc and Information level\n",
    "# attr_str = vlookup(attr_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "# print (attr_str.info())\n",
    "# attr_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 3개 null attribute는 정수와 문자를 모두 값을로 갖는 것들로 42개는 유효하고\n",
    "# 이 42개는 다음 section에서 볼 예정이므로 (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# # attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나...\n",
    "\n",
    "# attr_str = attr_str[attr_str.Desc.isna() == False]\n",
    "# print (attr_str.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attr_str_to_check = pv_attr_str[pv_attr_str.Meaning.str.contains('numeric value')].Attribute\n",
    "# attr_str_to_check = pv_attr_str[\n",
    "#     pv_attr_str.index.get_level_values(2).str.contains('numeric value')].index.get_level_values(0)\n",
    "# # 먼저 추후 체크할 것들을 뽑아 놓고\n",
    "# attr_str_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pv_attr_str = pd.DataFrame(pv_attr_str[\n",
    "#     ~(pv_attr_str.index.get_level_values(0).isin(attr_str_to_check))\n",
    "#     &~(pv_attr_str.index.get_level_values(2) == 'unknown')\n",
    "#     ].to_records())\n",
    "\n",
    "# pv_attr_str = pd.pivot_table(\n",
    "#     pv_attr_str,\n",
    "#     index = ['Attribute', 'Desc'],\n",
    "#     values = 'Value',\n",
    "#     aggfunc = lambda x: list(x)\n",
    "#     )\n",
    "\n",
    "# view_all(pv_attr_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(not_in_attr_str.values.T.shape)\n",
    "# not_in_attr_str.values.T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + bin_interval, bin_interval)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 5.0\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 5.0 * iqr\n",
    "#     upper_threshold = q3 + 5.0 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(set([i for i in col_val if i < lower_threshold or i > upper_threshold]), reverse = True)\n",
    "#     for j in outliers:\n",
    "#         print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "#     print ('\\n', '==========' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     # bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + 10, 10)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 1.5\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 1.5 * iqr\n",
    "#     upper_threshold = q3 + 1.5 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(\n",
    "#         set([i for i in col_val if i < lower_threshold or i > upper_threshold]),\n",
    "#         reverse = True)\n",
    "#     # for j in outliers:\n",
    "#     #     print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "        \n",
    "#     outlier_list = [str(int(j)) + ': ' + '{:.1%}'.format((cc_num[i] == j).sum() / count_val)\n",
    "#                     for j in outliers]\n",
    "    \n",
    "#     print('Outliers (Value: %)')\n",
    "#     for j in range(0, len(outlier_list), 10):\n",
    "#         print (', '.join(outlier_list[j: j+10]))\n",
    "#     print ('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** score_meaning_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_meaning_val(data, pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes']):\n",
    "#     '''\n",
    "#     function to check unique values of Score and Meaning by Attribute\n",
    "#     data : dataframe to examine\n",
    "#     pv_idx: list of pivot_table index\n",
    "#     '''\n",
    "#     data = vlookup(data, feature_desc, 'Attribute', ['Desc', 'Additional notes'], nan_val = 'no_info')\n",
    "#     pv = pd.pivot_table(\n",
    "#         data,\n",
    "#         index = pv_idx,\n",
    "#         values = ['Meaning', 'Score'],\n",
    "#         aggfunc = lambda x: list(x))\n",
    "    \n",
    "#     return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def verify_null(data, null_list, pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes']):\n",
    "#     '''\n",
    "#     function to display the summary of Score and Meaning values,\n",
    "#     when Meaning values are in the list containing possibly null values.   \n",
    "    \n",
    "#     data: dataframe to examine\n",
    "#     null_list: list of possibly null values\n",
    "#     pv_idx: pivot_table index\n",
    "#     '''\n",
    "#     null_check_Attribute = data[data['Meaning'].isin(null_list)]['Attribute'].to_list()\n",
    "#     null_check = data[data['Attribute'].isin(null_check_Attribute)]\n",
    "    \n",
    "#     null_check = vlookup(null_check, feature_desc, 'Attribute', ['Desc', 'Additional notes'], fill_na = 'no_info')\n",
    "#     pv = pd.pivot_table(\n",
    "#         null_check,\n",
    "#         index = pv_idx,\n",
    "#         values = ['Score', 'Meaning'],\n",
    "#         aggfunc = lambda x: list(x))\n",
    "    \n",
    "#     return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For values of Attribute without Description, add 12 Description values from attr\n",
    "# attr_to_add = attr[['Attribute', 'Description']].copy()\n",
    "# attr_to_add.dropna(inplace = True)\n",
    "# attr_to_add = attr_to_add.rename(columns = {'Description': 'Description_to_add'})\n",
    "\n",
    "# feature_desc = vlookup(feature_desc, attr_to_add, 'Attribute')\n",
    "# feature_desc.Description = np.where(\n",
    "#     ((feature_desc.Description.isnull() == True) & (feature_desc.Description_to_add.isnull() == False)),\n",
    "#     feature_desc.Description_to_add,\n",
    "#     feature_desc.Description)\n",
    "# feature_desc = feature_desc.drop('Description_to_add', axis=1)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# feature_desc['ger_to_eng'] = np.where(\n",
    "#     feature_desc.Description.isnull(),\n",
    "#     feature_desc.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# feature_desc['Desc'] = feature_desc.Description.fillna('') + feature_desc.ger_to_eng.fillna('')\n",
    "\n",
    "# feature_desc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pv_min_max 2.3.1 변경 for merge\n",
    "# min_max_copy = pd.DataFrame(pv_min_max.to_records())\n",
    "# min_max_copy.columns = list(\n",
    "#     min_max_copy.columns[:2]) + list(eval(i)[0] for i in min_max_copy.columns[2:])\n",
    "# min_max_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from IPython.display import display\n",
    "# import sys\n",
    "# from contextlib import ExitStack\n",
    "\n",
    "# def view_all (data):\n",
    "#     '''\n",
    "#     function to display all contents of data\n",
    "#     '''\n",
    "#     # with ExitStack() as stack:\n",
    "#     #     stack.enter_context(pd.option_context(\n",
    "#     #     'display.max_rows', None,\n",
    "#     #     'display.max_colwidth', None,\n",
    "#     #     'display.max_seq_items', None\n",
    "#     #     ))\n",
    "#     #     stack.enter_context(np.printoptions(threshold=np.inf))\n",
    "    \n",
    "#     # np.set_printoptions(threshold = sys.maxsize)\n",
    "    \n",
    "#     with pd.option_context(\n",
    "#     'display.max_rows', None, \n",
    "#     'display.max_colwidth', None,\n",
    "#     'display.max_seq_items', None,\n",
    "#     ):\n",
    "#         display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# categorize pre-processing items\n",
    "# '''\n",
    "# # # categorize p_process_items\n",
    "# # p_process_all = p_process_items.query('by_feature.str.contains(\"All\")')\n",
    "# # p_process_items = p_process_items.query('not by_feature.str.contains(\"All\")')\n",
    "\n",
    "# # p_process_list = ['p_process_all']\n",
    "# p_process_list = []\n",
    "# for i in p_process_items.method.unique():\n",
    "#     globals()['p_process_' + i] = p_process_items.query('method == \"{}\"'.format(i))\n",
    "#     globals()['p_process_' + i].name = 'p_process_' + i\n",
    "#     p_process_list.append('p_process_' + i)\n",
    "\n",
    "# p_process_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix = customers.corr(numeric_only = True)\n",
    "# correlation_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
