{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import missingno as msno\n",
    "from googletrans import Translator\n",
    "\n",
    "# import sys\n",
    "# from IPython.display import display\n",
    "# import pprint\n",
    "# import itertools\n",
    "# import math\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "'''\n",
    "custom modules\n",
    "'''\n",
    "# function similar to Excel's vlookup\n",
    "from vlookup import vlookup\n",
    "# function to view all contents of a dataframe\n",
    "from view_all import view_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install googletrans==4.0.0-rc1\n",
    "# pip install missingno\n",
    "# custom modules for convenience are in root folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 azdias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file overview\n",
    "# '''\n",
    "\n",
    "# azdias = pd.read_csv('../csv_pickle/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "# azdias.name = 'azdias'\n",
    "# print (azdias.shape)\n",
    "# azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891221 entries, 0 to 891220\n",
      "Columns: 366 entries, LNR to ALTERSKATEGORIE_GROB\n",
      "dtypes: float64(267), int64(93), object(6)\n",
      "memory usage: 2.4+ GB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNR</th>\n",
       "      <th>AGER_TYP</th>\n",
       "      <th>AKT_DAT_KL</th>\n",
       "      <th>ALTER_HH</th>\n",
       "      <th>ALTER_KIND1</th>\n",
       "      <th>ALTER_KIND2</th>\n",
       "      <th>ALTER_KIND3</th>\n",
       "      <th>ALTER_KIND4</th>\n",
       "      <th>ALTERSKATEGORIE_FEIN</th>\n",
       "      <th>ANZ_HAUSHALTE_AKTIV</th>\n",
       "      <th>...</th>\n",
       "      <th>VHN</th>\n",
       "      <th>VK_DHT4A</th>\n",
       "      <th>VK_DISTANZ</th>\n",
       "      <th>VK_ZG11</th>\n",
       "      <th>W_KEIT_KIND_HH</th>\n",
       "      <th>WOHNDAUER_2008</th>\n",
       "      <th>WOHNLAGE</th>\n",
       "      <th>ZABEOTYP</th>\n",
       "      <th>ANREDE_KZ</th>\n",
       "      <th>ALTERSKATEGORIE_GROB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910215</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>910220</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>910225</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910226</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910241</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 366 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LNR  AGER_TYP  AKT_DAT_KL  ALTER_HH  ALTER_KIND1  ALTER_KIND2  \\\n",
       "0  910215        -1         NaN       NaN          NaN          NaN   \n",
       "1  910220        -1         9.0       0.0          NaN          NaN   \n",
       "2  910225        -1         9.0      17.0          NaN          NaN   \n",
       "3  910226         2         1.0      13.0          NaN          NaN   \n",
       "4  910241        -1         1.0      20.0          NaN          NaN   \n",
       "\n",
       "   ALTER_KIND3  ALTER_KIND4  ALTERSKATEGORIE_FEIN  ANZ_HAUSHALTE_AKTIV  ...  \\\n",
       "0          NaN          NaN                   NaN                  NaN  ...   \n",
       "1          NaN          NaN                  21.0                 11.0  ...   \n",
       "2          NaN          NaN                  17.0                 10.0  ...   \n",
       "3          NaN          NaN                  13.0                  1.0  ...   \n",
       "4          NaN          NaN                  14.0                  3.0  ...   \n",
       "\n",
       "   VHN  VK_DHT4A  VK_DISTANZ  VK_ZG11  W_KEIT_KIND_HH  WOHNDAUER_2008  \\\n",
       "0  NaN       NaN         NaN      NaN             NaN             NaN   \n",
       "1  4.0       8.0        11.0     10.0             3.0             9.0   \n",
       "2  2.0       9.0         9.0      6.0             3.0             9.0   \n",
       "3  0.0       7.0        10.0     11.0             NaN             9.0   \n",
       "4  2.0       3.0         5.0      4.0             2.0             9.0   \n",
       "\n",
       "   WOHNLAGE ZABEOTYP ANREDE_KZ ALTERSKATEGORIE_GROB  \n",
       "0       NaN        3         1                    2  \n",
       "1       4.0        5         2                    1  \n",
       "2       2.0        5         2                    3  \n",
       "3       7.0        3         2                    4  \n",
       "4       3.0        4         1                    3  \n",
       "\n",
       "[5 rows x 366 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azdias = pd.read_pickle('../csv_pickle/azdias.pickle')\n",
    "\n",
    "print (azdias.info())\n",
    "azdias.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** miss_val_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_val_overview(df, chunk_size = 100):\n",
    "    '''\n",
    "    function to display missing value using missingno(msno) library\n",
    "    df: dataframe\n",
    "    chunk_size: int, size of column chunk, 100 as default  \n",
    "    '''\n",
    "    # split df columns into chunks\n",
    "    chunk_size = chunk_size\n",
    "    column_chunks = [df.iloc[:, i : i + chunk_size] for i in range(0, df.shape[1], chunk_size)]\n",
    "\n",
    "    # generate and display missingno plots for each chunk\n",
    "    for i, j in enumerate(column_chunks):\n",
    "        msno.matrix(j, figsize = (10, 3), fontsize = 8, labels = False, sparkline = False)\n",
    "        plt.title(\n",
    "            f'{df.name}: missing value overview - column {i * 100} to {min (i * chunk_size + chunk_size - 1, df.shape[1] - 1)}',\n",
    "            fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# azdias.name = 'azdias'\n",
    "# miss_val_overview(azdias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file overview\n",
    "# '''\n",
    "\n",
    "# customers = pd.read_csv('../csv_pickle/Udacity_CUSTOMERS_052018.csv', sep=';')\n",
    "# print (customers.info())\n",
    "# customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 191652 entries, 0 to 191651\n",
      "Columns: 369 entries, LNR to ALTERSKATEGORIE_GROB\n",
      "dtypes: float64(267), int64(94), object(8)\n",
      "memory usage: 539.5+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNR</th>\n",
       "      <th>AGER_TYP</th>\n",
       "      <th>AKT_DAT_KL</th>\n",
       "      <th>ALTER_HH</th>\n",
       "      <th>ALTER_KIND1</th>\n",
       "      <th>ALTER_KIND2</th>\n",
       "      <th>ALTER_KIND3</th>\n",
       "      <th>ALTER_KIND4</th>\n",
       "      <th>ALTERSKATEGORIE_FEIN</th>\n",
       "      <th>ANZ_HAUSHALTE_AKTIV</th>\n",
       "      <th>...</th>\n",
       "      <th>VK_ZG11</th>\n",
       "      <th>W_KEIT_KIND_HH</th>\n",
       "      <th>WOHNDAUER_2008</th>\n",
       "      <th>WOHNLAGE</th>\n",
       "      <th>ZABEOTYP</th>\n",
       "      <th>PRODUCT_GROUP</th>\n",
       "      <th>CUSTOMER_GROUP</th>\n",
       "      <th>ONLINE_PURCHASE</th>\n",
       "      <th>ANREDE_KZ</th>\n",
       "      <th>ALTERSKATEGORIE_GROB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9626</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>COSMETIC_AND_FOOD</td>\n",
       "      <td>MULTI_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9628</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>SINGLE_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143872</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>COSMETIC_AND_FOOD</td>\n",
       "      <td>MULTI_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143873</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>COSMETIC</td>\n",
       "      <td>MULTI_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143874</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>MULTI_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LNR  AGER_TYP  AKT_DAT_KL  ALTER_HH  ALTER_KIND1  ALTER_KIND2  \\\n",
       "0    9626         2         1.0      10.0          NaN          NaN   \n",
       "1    9628        -1         9.0      11.0          NaN          NaN   \n",
       "2  143872        -1         1.0       6.0          NaN          NaN   \n",
       "3  143873         1         1.0       8.0          NaN          NaN   \n",
       "4  143874        -1         1.0      20.0          NaN          NaN   \n",
       "\n",
       "   ALTER_KIND3  ALTER_KIND4  ALTERSKATEGORIE_FEIN  ANZ_HAUSHALTE_AKTIV  ...  \\\n",
       "0          NaN          NaN                  10.0                  1.0  ...   \n",
       "1          NaN          NaN                   NaN                  NaN  ...   \n",
       "2          NaN          NaN                   0.0                  1.0  ...   \n",
       "3          NaN          NaN                   8.0                  0.0  ...   \n",
       "4          NaN          NaN                  14.0                  7.0  ...   \n",
       "\n",
       "   VK_ZG11  W_KEIT_KIND_HH  WOHNDAUER_2008  WOHNLAGE  ZABEOTYP  \\\n",
       "0      2.0             6.0             9.0       7.0         3   \n",
       "1      3.0             0.0             9.0       NaN         3   \n",
       "2     11.0             6.0             9.0       2.0         3   \n",
       "3      2.0             NaN             9.0       7.0         1   \n",
       "4      4.0             2.0             9.0       3.0         1   \n",
       "\n",
       "       PRODUCT_GROUP  CUSTOMER_GROUP ONLINE_PURCHASE ANREDE_KZ  \\\n",
       "0  COSMETIC_AND_FOOD     MULTI_BUYER               0         1   \n",
       "1               FOOD    SINGLE_BUYER               0         1   \n",
       "2  COSMETIC_AND_FOOD     MULTI_BUYER               0         2   \n",
       "3           COSMETIC     MULTI_BUYER               0         1   \n",
       "4               FOOD     MULTI_BUYER               0         1   \n",
       "\n",
       "  ALTERSKATEGORIE_GROB  \n",
       "0                    4  \n",
       "1                    4  \n",
       "2                    4  \n",
       "3                    4  \n",
       "4                    3  \n",
       "\n",
       "[5 rows x 369 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers = pd.read_pickle('../csv_pickle/customers.pickle')\n",
    "\n",
    "print (customers.info())\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# customers.name = 'customers'\n",
    "# miss_val_overview(customers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 313 entries, 0 to 312\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Information level  10 non-null     object\n",
      " 1   Attribute          313 non-null    object\n",
      " 2   Description        313 non-null    object\n",
      " 3   Additional notes   20 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 9.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Information level</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Additional notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>in cooperation with Kantar TNS; the informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Person</td>\n",
       "      <td>ALTERSKATEGORIE_GROB</td>\n",
       "      <td>age through prename analysis</td>\n",
       "      <td>modelled on millions of first name-age-referen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ANREDE_KZ</td>\n",
       "      <td>gender</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>CJT_GESAMTTYP</td>\n",
       "      <td>Customer-Journey-Typology relating to the pref...</td>\n",
       "      <td>relating to the preferred information, marketi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FINANZ_MINIMALIST</td>\n",
       "      <td>financial typology: low financial interest</td>\n",
       "      <td>Gfk-Typology based on a representative househo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FINANZ_SPARER</td>\n",
       "      <td>financial typology: money saver</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Information level             Attribute  \\\n",
       "0               NaN              AGER_TYP   \n",
       "1            Person  ALTERSKATEGORIE_GROB   \n",
       "2               NaN             ANREDE_KZ   \n",
       "3               NaN         CJT_GESAMTTYP   \n",
       "4               NaN     FINANZ_MINIMALIST   \n",
       "5               NaN         FINANZ_SPARER   \n",
       "\n",
       "                                         Description  \\\n",
       "0                                 best-ager typology   \n",
       "1                      age through prename analysis    \n",
       "2                                             gender   \n",
       "3  Customer-Journey-Typology relating to the pref...   \n",
       "4         financial typology: low financial interest   \n",
       "5                    financial typology: money saver   \n",
       "\n",
       "                                    Additional notes  \n",
       "0  in cooperation with Kantar TNS; the informatio...  \n",
       "1  modelled on millions of first name-age-referen...  \n",
       "2                                                NaN  \n",
       "3  relating to the preferred information, marketi...  \n",
       "4  Gfk-Typology based on a representative househo...  \n",
       "5                                                NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "info = pd.read_excel(\n",
    "    'DIAS Information Levels - Attributes 2017.xlsx', header=1).iloc[:, 1:]\n",
    "\n",
    "print (info.info())\n",
    "info.head(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2258 entries, 0 to 2257\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Attribute    314 non-null    object\n",
      " 1   Description  351 non-null    object\n",
      " 2   Score        2258 non-null   object\n",
      " 3   Meaning      2247 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 70.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9D</td>\n",
       "      <td>Mini-Jobber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9E</td>\n",
       "      <td>Socking Away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>CAMEO_DEUINTL_2015</td>\n",
       "      <td>CAMEO classification 2015 - international typo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(each German CAMEO code belongs to one interna...</td>\n",
       "      <td>11</td>\n",
       "      <td>Wealthy Households-Pre-Family Couples &amp; Singles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>Wealthy Households-Young Couples With Children</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Attribute                                        Description  \\\n",
       "103                 NaN                                                NaN   \n",
       "104                 NaN                                                NaN   \n",
       "105  CAMEO_DEUINTL_2015  CAMEO classification 2015 - international typo...   \n",
       "106                 NaN  (each German CAMEO code belongs to one interna...   \n",
       "107                 NaN                                                NaN   \n",
       "\n",
       "    Score                                          Meaning  \n",
       "103    9D                                      Mini-Jobber  \n",
       "104    9E                                     Socking Away  \n",
       "105    -1                                          unknown  \n",
       "106    11  Wealthy Households-Pre-Family Couples & Singles  \n",
       "107    12   Wealthy Households-Young Couples With Children  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "attr = pd.read_excel(\n",
    "    'DIAS Attributes - Values 2017.xlsx', header=1).iloc[:, 1:]\n",
    "# rename column Value to Score for easier documentation\n",
    "attr = attr.rename(columns = {'Value' : 'Score'})\n",
    "\n",
    "print (attr.info())\n",
    "attr[103:108]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Baseline understanding\n",
    "\n",
    "There are 4 files given:\n",
    "* 2 dataset files - azdias & customers\n",
    "* 2 reference files - info & attr:  \n",
    "  - info is for information on dataset features which are in unreadable German acronyms  \n",
    "  - attr is for information on dataset values which are in numbers and acronyms,  \n",
    "  and has corresponding meanings  \n",
    "\n",
    "To establish a baseline, this section aims to identify key factors in the reference files   \n",
    "that can help in understanding the contents of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As dataset contents can be understood by contents of reference files,  \n",
    "# this section is to find baseline factors in reference files as the first step.\n",
    "\n",
    "# In the first step, I will understand the contents of datasets using 2 information files,     \n",
    "# and record points to be pre-processed in the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 feature reference\n",
    "\n",
    "As datasets do not have information on what each column feature exactly means,  \n",
    "values of reference files have to be mapped to the features.\n",
    "  \n",
    "feature_desc below is for this needs, and formed with 369 column features from customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 369 entries, 0 to 368\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Attribute          369 non-null    object\n",
      " 1   Information level  9 non-null      object\n",
      " 2   Description        264 non-null    object\n",
      " 3   Additional notes   17 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 14.4+ KB\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "feature_desc with column features of customers and added information from info \n",
    "'''\n",
    "\n",
    "feature_desc = pd.DataFrame(customers.columns, columns=['Attribute'])\n",
    "feature_desc = vlookup(feature_desc, info, 'Attribute')\n",
    "# custom function vlookup is similar to Excel's vlookup\n",
    "\n",
    "feature_desc.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging information columns, there are 264 features from both customers and info,  \n",
    "along with 105 exclusive features of customers lacking Description values.  \n",
    "\n",
    "To fill the 105 missing Descriptions, 12 values are added from the reference file attr,  \n",
    "and translation values are created using the googletrans module.  \n",
    "The resulting dataframe is saved in the root folder due to the time-consuming code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 369 entries, 0 to 368\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Attribute          369 non-null    object\n",
      " 1   Information level  9 non-null      object\n",
      " 2   Description        276 non-null    object\n",
      " 3   Additional notes   17 non-null     object\n",
      " 4   ger_to_eng         93 non-null     object\n",
      " 5   Desc               369 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 20.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# # For values of Attribute without Description, add 12 Description values from attr\n",
    "# attr_to_add = attr[['Attribute', 'Description']].copy()\n",
    "# attr_to_add.dropna(inplace = True)\n",
    "# attr_to_add = attr_to_add.rename(columns = {'Description': 'Description_to_add'})\n",
    "\n",
    "# feature_desc = vlookup(feature_desc, attr_to_add, 'Attribute')\n",
    "# feature_desc.Description = np.where(\n",
    "#     ((feature_desc.Description.isnull() == True) & (feature_desc.Description_to_add.isnull() == False)),\n",
    "#     feature_desc.Description_to_add,\n",
    "#     feature_desc.Description)\n",
    "# feature_desc = feature_desc.drop('Description_to_add', axis=1)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# feature_desc['ger_to_eng'] = np.where(\n",
    "#     feature_desc.Description.isnull(),\n",
    "#     feature_desc.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# feature_desc['Desc'] = feature_desc.Description.fillna('') + feature_desc.ger_to_eng.fillna('')\n",
    "\n",
    "feature_desc = pd.read_excel('feature_desc.xlsx', index_col = [0])\n",
    "feature_desc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Information level</th>\n",
       "      <th>Description</th>\n",
       "      <th>Additional notes</th>\n",
       "      <th>ger_to_eng</th>\n",
       "      <th>Desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LNR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LNR</td>\n",
       "      <td>LNR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>in cooperation with Kantar TNS; the informatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>best-ager typology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AKT_DAT_KL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Act Dat KL</td>\n",
       "      <td>Act Dat KL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALTER_HH</td>\n",
       "      <td>Household</td>\n",
       "      <td>main age within the household</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>main age within the household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALTER_KIND1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Old child1</td>\n",
       "      <td>Old child1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Attribute Information level                    Description  \\\n",
       "0          LNR               NaN                            NaN   \n",
       "1     AGER_TYP               NaN             best-ager typology   \n",
       "2   AKT_DAT_KL               NaN                            NaN   \n",
       "3     ALTER_HH         Household  main age within the household   \n",
       "4  ALTER_KIND1               NaN                            NaN   \n",
       "\n",
       "                                    Additional notes  ger_to_eng  \\\n",
       "0                                                NaN         LNR   \n",
       "1  in cooperation with Kantar TNS; the informatio...         NaN   \n",
       "2                                                NaN  Act Dat KL   \n",
       "3                                                NaN         NaN   \n",
       "4                                                NaN  Old child1   \n",
       "\n",
       "                            Desc  \n",
       "0                            LNR  \n",
       "1             best-ager typology  \n",
       "2                     Act Dat KL  \n",
       "3  main age within the household  \n",
       "4                     Old child1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "understand all features one by one using custom function view_all\n",
    "\n",
    "not to run this cell, to save space in the final submission \n",
    "'''\n",
    "\n",
    "# view_all(feature_desc)\n",
    "\n",
    "feature_desc.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improved readability of feature_desc has enhanced the understanding  \n",
    "of the overall structure of the datasets and the meanings of the features,  \n",
    "although some translations still remain unclear.  \n",
    "\n",
    "Since there are a few features that have similar contents, it is necessary  \n",
    "to perform imputation steps to resolve collinearity and reduce dimensionality  \n",
    "before proceeding with the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_process below is a dict formed to record points to be pre-processed in the following sections.\n",
    "\n",
    "# '''\n",
    "# p_process dict to record points to be pre_processed\n",
    "# '''\n",
    "\n",
    "# p_process = {'2.1' : 'features of similar contents: collinearity resolution & dimensionality reduction'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 feature alignment\n",
    "\n",
    "The following cells provide an overview of the number of features in each file  \n",
    "and illustrates the feature intersection and difference between the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features by file\n",
      "azdias : 366\n",
      "customers : 369\n",
      "info : 313\n",
      "attr : 314\n",
      "feature_desc : 369\n"
     ]
    }
   ],
   "source": [
    "feature_dict = {\n",
    "    'azdias' : set(azdias.columns.unique()),\n",
    "    'customers' : set(customers.columns.unique()),\n",
    "    'info' : set(info.Attribute.dropna().unique()),\n",
    "    'attr' : set(attr.Attribute.dropna().unique()),\n",
    "    'feature_desc' : set(feature_desc.Attribute.dropna().unique())\n",
    "    }\n",
    "\n",
    "print ('Number of features by file')\n",
    "for k, v in feature_dict.items():\n",
    "    print (k, ':', len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature intersection & difference between files\n",
      "azdias & customers intersection:       366  /  in azdias & not-in customers:       0\n",
      "azdias & info intersection:            264  /  in azdias & not-in info:          102\n",
      "azdias & attr intersection:            272  /  in azdias & not-in attr:           94\n",
      "azdias & feature_desc intersection:    366  /  in azdias & not-in feature_desc:    0\n",
      "customers & azdias intersection:       366  /  in customers & not-in azdias:       3\n",
      "customers & info intersection:         264  /  in customers & not-in info:       105\n",
      "customers & attr intersection:         272  /  in customers & not-in attr:        97\n",
      "customers & feature_desc intersection: 369  /  in customers & not-in feature_desc: 0\n",
      "info & azdias intersection:            264  /  in info & not-in azdias:           49\n",
      "info & customers intersection:         264  /  in info & not-in customers:        49\n",
      "info & attr intersection:              300  /  in info & not-in attr:             13\n",
      "info & feature_desc intersection:      264  /  in info & not-in feature_desc:     49\n",
      "attr & azdias intersection:            272  /  in attr & not-in azdias:           42\n",
      "attr & customers intersection:         272  /  in attr & not-in customers:        42\n",
      "attr & info intersection:              300  /  in attr & not-in info:             14\n",
      "attr & feature_desc intersection:      272  /  in attr & not-in feature_desc:     42\n",
      "feature_desc & azdias intersection:    366  /  in feature_desc & not-in azdias:    3\n",
      "feature_desc & customers intersection: 369  /  in feature_desc & not-in customers: 0\n",
      "feature_desc & info intersection:      264  /  in feature_desc & not-in info:    105\n",
      "feature_desc & attr intersection:      272  /  in feature_desc & not-in attr:     97\n"
     ]
    }
   ],
   "source": [
    "print ('Feature intersection & difference between files')\n",
    "for i in range(0, 5):\n",
    "    for j in range (0, 5):\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        key_1 = list(feature_dict.keys())[i]\n",
    "        key_2 = list(feature_dict.keys())[j]\n",
    "        set_1 = list(feature_dict.values())[i]\n",
    "        set_2 = list(feature_dict.values())[j]\n",
    "        \n",
    "        intsec = set_1.intersection(set_2)\n",
    "        ft_diff = set_1 - set_2\n",
    "        feature_dict[f'{key_1}_intsec_{key_2}'] = intsec\n",
    "        feature_dict[f'in_{key_1}_notin_{key_2}'] = ft_diff\n",
    "        \n",
    "        intsec_print = f'{key_1} & {key_2} intersection:'\n",
    "        ft_diff_print = f' /  in {key_1} & not-in {key_2}:'        \n",
    "        print (intsec_print, str(len(intsec)).rjust(41 - len(intsec_print)),\n",
    "               ft_diff_print, str(len(ft_diff)).rjust(40 - len(ft_diff_print)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 dataset values\n",
    "\n",
    "After identifying the dataset structure in parts 2.1 and 2.2 above,  \n",
    "part 2.3 focuses on examining the dataset contents.  \n",
    "In this part, various aspects such as the ranges of feature values,  \n",
    "values actually meaning null, data types, and more are to be verified,  \n",
    "and preliminary notes will be recorded on pre-processing requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.0 attr modification\n",
    "\n",
    "Considering that info contains information about dataset features, attr contains  \n",
    "information about dataset values and the values within the datasets can be understood  \n",
    "by mapping them to the corresponding Meaning values in attr.\n",
    "\n",
    "However, it is necessary to initially modify attr since it is not in a neat form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2258 entries, 0 to 2257\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Attribute    2258 non-null   object\n",
      " 1   Description  2258 non-null   object\n",
      " 2   Score        2258 non-null   object\n",
      " 3   Meaning      2247 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 70.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>0</td>\n",
       "      <td>no classification possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>1</td>\n",
       "      <td>passive elderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>2</td>\n",
       "      <td>cultural elderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>3</td>\n",
       "      <td>experience-driven elderly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Attribute          Description Score                     Meaning\n",
       "0  AGER_TYP  best-ager typology     -1                     unknown\n",
       "1  AGER_TYP  best-ager typology      0  no classification possible\n",
       "2  AGER_TYP  best-ager typology      1             passive elderly\n",
       "3  AGER_TYP  best-ager typology      2            cultural elderly\n",
       "4  AGER_TYP  best-ager typology      3   experience-driven elderly"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "some Description cells have minor add-on information below\n",
    "which should be merged\n",
    "'''\n",
    " # keep original attr file\n",
    "attr_org = attr.copy()\n",
    "\n",
    "# merge minor information cells\n",
    "attr['description_shift'] = attr.Description.shift(-1).fillna('')\n",
    "attr.Description = attr.Description.mask(\n",
    "    ~(attr['Attribute'].isna()),\n",
    "    attr.Description + ' ' + attr.description_shift)\n",
    "attr.Description = attr.Description.mask(\n",
    "    (attr['Attribute'].isna()) & ~(attr['Description'].isna()),\n",
    "    np.nan)\n",
    "attr = attr.drop(columns = 'description_shift')\n",
    "\n",
    "attr['description_shift'] = attr['Description'].shift(-1).fillna('')\n",
    "attr['Description'] = attr.query('~Attribute.isna()')['Description'] + ' ' + attr['description_shift']\n",
    "attr['Description'] = attr.query('Attribute.isna() and not Description.isna()')['Description'].apply(lambda x: np.nan)\n",
    "attr = attr.drop(columns='description_shift')\n",
    "\n",
    "'''\n",
    "fill null as only 1st lines of Attribute & Description have values\n",
    "'''\n",
    "attr[['Attribute', 'Description']] = attr[\n",
    "    ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "print (attr.info())\n",
    "attr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modifying the attribute file to have well-organized data values,  \n",
    "it is filtered to retain only 272 intersection features with customers.  \n",
    "This filtering process involves dropping the exclusive features of attr  \n",
    "that are not useful for analyzing the datasets of customers and azdias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "extract 272 intersection features\n",
    "'''\n",
    "intsec_ft = feature_dict['attr_intsec_customers']\n",
    "attr = attr[attr.Attribute.isin(intsec_ft)]\n",
    "\n",
    "print (attr.Attribute.nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of modifying attr, column Score_form is added  \n",
    "to classify 272 features into 2 categories based on value forms in Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'int'>    1774\n",
       "<class 'str'>     142\n",
       "Name: Score_form, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr['Score_form'] = attr.Score.map(type)\n",
    "attr.Score_form.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 features with numeric Score values\n",
    "\n",
    "Out of 272 features, 264 features are extracted with the condition  \n",
    "that values within column Score are of numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1774 entries, 0 to 2257\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Attribute    1774 non-null   object\n",
      " 1   Description  1774 non-null   object\n",
      " 2   Score        1774 non-null   object\n",
      " 3   Meaning      1763 non-null   object\n",
      " 4   Score_form   1774 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 83.2+ KB\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "attr_num containing Score values in numeric form\n",
    "'''\n",
    "\n",
    "attr_num = attr[attr.Score_form == int]\n",
    "attr_num.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Score values lacking Meaning values**\n",
    "    \n",
    "  A small issue detected here is that there are 11 lines without value of Meaning.  \n",
    "  From the examination below, 2 features involved with these lines can be dropped,  \n",
    "  as they have other features with similar but more detailed Meaning values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Score_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>LP_FAMILIE_GROB</td>\n",
       "      <td>familytyp rough</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>LP_FAMILIE_GROB</td>\n",
       "      <td>familytyp rough</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>LP_FAMILIE_GROB</td>\n",
       "      <td>familytyp rough</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>LP_FAMILIE_GROB</td>\n",
       "      <td>familytyp rough</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>LP_FAMILIE_GROB</td>\n",
       "      <td>familytyp rough</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>LP_FAMILIE_GROB</td>\n",
       "      <td>familytyp rough</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>LP_STATUS_GROB</td>\n",
       "      <td>social status rough</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>LP_STATUS_GROB</td>\n",
       "      <td>social status rough</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>LP_STATUS_GROB</td>\n",
       "      <td>social status rough</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>LP_STATUS_GROB</td>\n",
       "      <td>social status rough</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>LP_STATUS_GROB</td>\n",
       "      <td>social status rough</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Attribute           Description Score Meaning     Score_form\n",
       "1906  LP_FAMILIE_GROB      familytyp rough      4     NaN  <class 'int'>\n",
       "1907  LP_FAMILIE_GROB      familytyp rough      5     NaN  <class 'int'>\n",
       "1909  LP_FAMILIE_GROB      familytyp rough      7     NaN  <class 'int'>\n",
       "1910  LP_FAMILIE_GROB      familytyp rough      8     NaN  <class 'int'>\n",
       "1912  LP_FAMILIE_GROB      familytyp rough     10     NaN  <class 'int'>\n",
       "1913  LP_FAMILIE_GROB      familytyp rough     11     NaN  <class 'int'>\n",
       "1977   LP_STATUS_GROB  social status rough      2     NaN  <class 'int'>\n",
       "1979   LP_STATUS_GROB  social status rough      4     NaN  <class 'int'>\n",
       "1980   LP_STATUS_GROB  social status rough      5     NaN  <class 'int'>\n",
       "1982   LP_STATUS_GROB  social status rough      7     NaN  <class 'int'>\n",
       "1984   LP_STATUS_GROB  social status rough      9     NaN  <class 'int'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2 features involved with lines without Meaning values\n",
    "'''\n",
    "attr_num[attr_num.Meaning.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Score_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>LP_FAMILIE_FEIN</td>\n",
       "      <td>familytyp fine</td>\n",
       "      <td>9</td>\n",
       "      <td>shared flat</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>LP_FAMILIE_FEIN</td>\n",
       "      <td>familytyp fine</td>\n",
       "      <td>10</td>\n",
       "      <td>two-generational household</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>LP_FAMILIE_FEIN</td>\n",
       "      <td>familytyp fine</td>\n",
       "      <td>11</td>\n",
       "      <td>multi-generational household</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>LP_FAMILIE_GROB</td>\n",
       "      <td>familytyp rough</td>\n",
       "      <td>9</td>\n",
       "      <td>multiperson household</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>LP_FAMILIE_GROB</td>\n",
       "      <td>familytyp rough</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>LP_FAMILIE_GROB</td>\n",
       "      <td>familytyp rough</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>LP_STATUS_FEIN</td>\n",
       "      <td>social status fine</td>\n",
       "      <td>9</td>\n",
       "      <td>houseowners</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>LP_STATUS_FEIN</td>\n",
       "      <td>social status fine</td>\n",
       "      <td>10</td>\n",
       "      <td>top earners</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>LP_STATUS_GROB</td>\n",
       "      <td>social status rough</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>LP_STATUS_GROB</td>\n",
       "      <td>social status rough</td>\n",
       "      <td>10</td>\n",
       "      <td>top earners</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Attribute           Description Score  \\\n",
       "1900  LP_FAMILIE_FEIN       familytyp fine      9   \n",
       "1901  LP_FAMILIE_FEIN       familytyp fine     10   \n",
       "1902  LP_FAMILIE_FEIN       familytyp fine     11   \n",
       "1911  LP_FAMILIE_GROB      familytyp rough      9   \n",
       "1912  LP_FAMILIE_GROB      familytyp rough     10   \n",
       "1913  LP_FAMILIE_GROB      familytyp rough     11   \n",
       "1974   LP_STATUS_FEIN   social status fine      9   \n",
       "1975   LP_STATUS_FEIN   social status fine     10   \n",
       "1984   LP_STATUS_GROB  social status rough      9   \n",
       "1985   LP_STATUS_GROB  social status rough     10   \n",
       "\n",
       "                           Meaning     Score_form  \n",
       "1900                   shared flat  <class 'int'>  \n",
       "1901    two-generational household  <class 'int'>  \n",
       "1902  multi-generational household  <class 'int'>  \n",
       "1911         multiperson household  <class 'int'>  \n",
       "1912                           NaN  <class 'int'>  \n",
       "1913                           NaN  <class 'int'>  \n",
       "1974                   houseowners  <class 'int'>  \n",
       "1975                  top earners   <class 'int'>  \n",
       "1984                           NaN  <class 'int'>  \n",
       "1985                  top earners   <class 'int'>  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "these 2 features can be dropped, as they have other features\n",
    "with similar but more detailed Meaning value\n",
    "'''\n",
    "lp_gros_fine = attr[attr.Attribute.str.contains('LP_FAMILIE') | attr.Attribute.str.contains('LP_STATUS')]\n",
    "lp_gros_fine[lp_gros_fine['Score'] > 8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Note]** p_process below is a dict formed to note points to be pre-processed in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process = {'2.3.1' : 'drop features LP_FAMILIE_GROB / LP_STATUS_GROB'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping 2 features, remaining features are further organized into groups  \n",
    "based on min/max values observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 features grouped by min/max values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['-1 to -1', '-1 to 2', '-1 to 3', '-1 to 4', '-1 to 5', '-1 to 6',\n",
       "       '-1 to 7', '-1 to 8', '-1 to 9', '0 to 1', '0 to 10', '0 to 21',\n",
       "       '0 to 3', '0 to 4', '0 to 5', '0 to 6', '1 to 10', '1 to 11',\n",
       "       '1 to 12', '1 to 15', '1 to 2', '1 to 3', '1 to 4', '1 to 40',\n",
       "       '1 to 5', '1 to 6', '1 to 7', '1 to 8', '1 to 9'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "drop 2 features from attr_num\n",
    "'''\n",
    "attr_num = attr_num[(~attr_num['Attribute'].isin(['LP_FAMILIE_GROB', 'LP_STATUS_GROB']))]\n",
    "\n",
    "'''\n",
    "pivot table summary of attr_num with min/max value categories\n",
    "'''\n",
    "print (f'{attr_num.Attribute.nunique()} features grouped by min/max values')\n",
    "pv = pd.pivot_table(\n",
    "    attr_num,\n",
    "    index = ['Attribute', 'Description'],\n",
    "    values = 'Score',\n",
    "    aggfunc = [min, max]\n",
    "    )\n",
    "pv['min_max_cat'] = pv['min'].astype(str) + ' to ' + pv['max'].astype(str)\n",
    "pv = pv.sort_values(by = 'min_max_cat')\n",
    "\n",
    "pv.min_max_cat.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From min/max categories above, to be further checked are:   \n",
    "* If Score -1 and 0 contain data equal to null or -inf  \n",
    "* If max Score values contain data equal to null or inf\n",
    "* If features with wider range of min/max gap are discrete or continuous"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Cases of Score -1 and 0**\n",
    "     \n",
    "  i) All Score value -1s in datasets should be replaced to null,  \n",
    "    as the reference of attr shows that it always means unknown.\n",
    "\n",
    "  ii) Few Score value 0s can be replaced to null if the corresponding  \n",
    "    value of Meaning is in list to_null.\n",
    "\n",
    "  iii) All other Score value 0s should be remained, as the reference\n",
    "    of attr shows value 0 is not meaning null in nearly all cases.  \n",
    "\n",
    "  Analyses for this conclusion are in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown    133\n",
       "Name: Meaning, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "when Score value is -1, the meaning is 100% unknown in reference file\n",
    "'''\n",
    "\n",
    "attr_num_mn1 = attr_num[attr_num.Score == -1]\n",
    "attr_num_mn1.Meaning.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none                                                136\n",
       "no transactions known                                10\n",
       "no Online-transactions within the last 12 months      3\n",
       "unknown                                               2\n",
       "no classification possible                            1\n",
       "unknown / no main age detectable                      1\n",
       "doesn't belong to the green avantgarde                1\n",
       "classification not possible                           1\n",
       "no 1-2 family homes                                   1\n",
       "no 3-5 family homes                                   1\n",
       "no 6-10 family homes                                  1\n",
       "no >10 family homes                                   1\n",
       "external supplied hedonists                           1\n",
       "no score calculated                                   1\n",
       "Name: Meaning, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "when Score value is 0, it means none in most occasion,\n",
    "and there are few cases that the corresponding value\n",
    "of Meaning is definitely equal to null\n",
    "'''\n",
    "\n",
    "attr_num_0 = attr_num[attr_num.Score == 0]\n",
    "attr_num_0.Meaning.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** score_meaning_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_meaning_val(data, pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes']):\n",
    "    '''\n",
    "    function to check unique values of Score and Meaning by Attribute\n",
    "    data : dataframe to examine\n",
    "    pv_idx: list of pivot_table index\n",
    "    '''\n",
    "    data = vlookup(data, feature_desc, 'Attribute', ['Desc', 'Additional notes'], nan_val = 'no_info')\n",
    "    pv = pd.pivot_table(\n",
    "        data,\n",
    "        index = pv_idx,\n",
    "        values = ['Meaning', 'Score'],\n",
    "        aggfunc = lambda x: list(x))\n",
    "    \n",
    "    return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Desc</th>\n",
       "      <th>Additional notes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AGER_TYP</th>\n",
       "      <th>best-ager typology</th>\n",
       "      <th>best-ager typology</th>\n",
       "      <th>in cooperation with Kantar TNS; the information basis is a consumer survey</th>\n",
       "      <td>[unknown, no classification possible, passive elderly, cultural elderly, experience-driven elderly]</td>\n",
       "      <td>[-1, 0, 1, 2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALTER_HH</th>\n",
       "      <th>main age within the household</th>\n",
       "      <th>main age within the household</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[unknown / no main age detectable, 01.01.1895 bis 31.12.1899, 01.01.1900 bis 31.12.1904, 01.01.1905 bis 31.12.1909, 01.01.1910 bis 31.12.1914, 01.01.1915 bis 31.12.1919, 01.01.1920 bis 31.12.1924, 01.01.1925 bis 31.12.1929, 01.01.1930 bis 31.12.1934, 01.01.1935 bis 31.12.1939, 01.01.1940 bis 31.12.1944, 01.01.1945 bis 31.12.1949, 01.01.1950 bis 31.12.1954, 01.01.1955 bis 31.12.1959, 01.01.1960 bis 31.12.1964, 01.01.1965 bis 31.12.1969, 01.01.1970 bis 31.12.1974, 01.01.1975 bis 31.12.1979, 01.01.1980 bis 31.12.1984, 01.01.1985 bis 31.12.1989, 01.01.1990 bis 31.12.1994, 01.01.1995 bis 31.12.1999]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D19_BANKEN_ANZ_12</th>\n",
       "      <th>transaction activity BANKS in the last 12 months</th>\n",
       "      <th>transaction activity BANKS in the last 12 months</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[no transactions known, very low activity, low activity, slightly increased activity, increased activity, high activity, very high activity]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D19_BANKEN_ANZ_24</th>\n",
       "      <th>transaction activity BANKS in the last 24 months</th>\n",
       "      <th>transaction activity BANKS in the last 24 months</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[no transactions known, very low activity, low activity, slightly increased activity, increased activity, high activity, very high activity]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D19_BANKEN_DATUM</th>\n",
       "      <th>actuality of the last transaction for the segment banks TOTAL</th>\n",
       "      <th>actuality of the last transaction for the segment banks TOTAL</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[highest activity within the last 12 months, very high activity within the last 12 months, high activity within the last 12 months, increased activity within the last 12 months, slightly increased activity within the last 12 months, activity elder than 1 year, activity elder than 1,5 years, activity elder than 2 years, activity elder than 3 years, no transactions known]</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Meaning  \\\n",
       "Attribute         Description                                                    Desc                                                          Additional notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "AGER_TYP          best-ager typology                                             best-ager typology                                            in cooperation with Kantar TNS; the information basis is a consumer survey                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [unknown, no classification possible, passive elderly, cultural elderly, experience-driven elderly]   \n",
       "ALTER_HH          main age within the household                                  main age within the household                                 no_info                                                                     [unknown / no main age detectable, 01.01.1895 bis 31.12.1899, 01.01.1900 bis 31.12.1904, 01.01.1905 bis 31.12.1909, 01.01.1910 bis 31.12.1914, 01.01.1915 bis 31.12.1919, 01.01.1920 bis 31.12.1924, 01.01.1925 bis 31.12.1929, 01.01.1930 bis 31.12.1934, 01.01.1935 bis 31.12.1939, 01.01.1940 bis 31.12.1944, 01.01.1945 bis 31.12.1949, 01.01.1950 bis 31.12.1954, 01.01.1955 bis 31.12.1959, 01.01.1960 bis 31.12.1964, 01.01.1965 bis 31.12.1969, 01.01.1970 bis 31.12.1974, 01.01.1975 bis 31.12.1979, 01.01.1980 bis 31.12.1984, 01.01.1985 bis 31.12.1989, 01.01.1990 bis 31.12.1994, 01.01.1995 bis 31.12.1999]   \n",
       "D19_BANKEN_ANZ_12 transaction activity BANKS in the last 12 months               transaction activity BANKS in the last 12 months              no_info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [no transactions known, very low activity, low activity, slightly increased activity, increased activity, high activity, very high activity]   \n",
       "D19_BANKEN_ANZ_24 transaction activity BANKS in the last 24 months               transaction activity BANKS in the last 24 months              no_info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [no transactions known, very low activity, low activity, slightly increased activity, increased activity, high activity, very high activity]   \n",
       "D19_BANKEN_DATUM  actuality of the last transaction for the segment banks TOTAL  actuality of the last transaction for the segment banks TOTAL no_info                                                                                                                                                                                                                                                                                                          [highest activity within the last 12 months, very high activity within the last 12 months, high activity within the last 12 months, increased activity within the last 12 months, slightly increased activity within the last 12 months, activity elder than 1 year, activity elder than 1,5 years, activity elder than 2 years, activity elder than 3 years, no transactions known]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Score  \n",
       "Attribute         Description                                                    Desc                                                          Additional notes                                                                                                                                            \n",
       "AGER_TYP          best-ager typology                                             best-ager typology                                            in cooperation with Kantar TNS; the information basis is a consumer survey                                                                [-1, 0, 1, 2, 3]  \n",
       "ALTER_HH          main age within the household                                  main age within the household                                 no_info                                                                     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]  \n",
       "D19_BANKEN_ANZ_12 transaction activity BANKS in the last 12 months               transaction activity BANKS in the last 12 months              no_info                                                                                                                              [0, 1, 2, 3, 4, 5, 6]  \n",
       "D19_BANKEN_ANZ_24 transaction activity BANKS in the last 24 months               transaction activity BANKS in the last 24 months              no_info                                                                                                                              [0, 1, 2, 3, 4, 5, 6]  \n",
       "D19_BANKEN_DATUM  actuality of the last transaction for the segment banks TOTAL  actuality of the last transaction for the segment banks TOTAL no_info                                                                                                                    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "select Meaning values which seems to be definitely null,\n",
    "and verify them\n",
    "'''\n",
    "to_null = ['unknown','no transactions known', 'no classification possible',\n",
    "           'unknown / no main age detectable', 'classification not possible',\n",
    "           'no score calculated'\n",
    "            ]\n",
    "\n",
    "null_check_Attribute = attr_num[attr_num['Meaning'].isin(to_null[1:])]['Attribute'].to_list()\n",
    "null_check = attr_num[attr_num['Attribute'].isin(null_check_Attribute)]\n",
    "\n",
    "pv = score_meaning_val(null_check)\n",
    "view_all(pv.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Desc</th>\n",
       "      <th>Additional notes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KBA05_ALTER1</th>\n",
       "      <th>share of car owners less than 31 years old</th>\n",
       "      <th>share of car owners less than 31 years old</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[none, low, average, high, very high]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KBA05_ALTER4</th>\n",
       "      <th>share of cars owners elder than 61 years</th>\n",
       "      <th>share of cars owners elder than 61 years</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[none, very low, low, average, high, very high]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KBA05_ANHANG</th>\n",
       "      <th>share of trailers in the microcell</th>\n",
       "      <th>share of trailers in the microcell</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[none, some, some more, very many]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KBA05_CCM4</th>\n",
       "      <th>share of cars with more than 2499ccm</th>\n",
       "      <th>share of cars with more than 2499ccm</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[none, low, average, high, very high]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KBA05_DIESEL</th>\n",
       "      <th>share of cars with Diesel-engine in the microcell</th>\n",
       "      <th>share of cars with Diesel-engine in the microcell</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[none, very low, low, average, high]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                            Meaning  \\\n",
       "Attribute    Description                                        Desc                                              Additional notes                                                    \n",
       "KBA05_ALTER1 share of car owners less than 31 years old         share of car owners less than 31 years old        no_info                     [none, low, average, high, very high]   \n",
       "KBA05_ALTER4 share of cars owners elder than 61 years           share of cars owners elder than 61 years          no_info           [none, very low, low, average, high, very high]   \n",
       "KBA05_ANHANG share of trailers in the microcell                 share of trailers in the microcell                no_info                        [none, some, some more, very many]   \n",
       "KBA05_CCM4   share of cars with more than 2499ccm               share of cars with more than 2499ccm              no_info                     [none, low, average, high, very high]   \n",
       "KBA05_DIESEL share of cars with Diesel-engine in the microcell  share of cars with Diesel-engine in the microcell no_info                      [none, very low, low, average, high]   \n",
       "\n",
       "                                                                                                                                                 Score  \n",
       "Attribute    Description                                        Desc                                              Additional notes                      \n",
       "KBA05_ALTER1 share of car owners less than 31 years old         share of car owners less than 31 years old        no_info              [0, 1, 2, 3, 4]  \n",
       "KBA05_ALTER4 share of cars owners elder than 61 years           share of cars owners elder than 61 years          no_info           [0, 1, 2, 3, 4, 5]  \n",
       "KBA05_ANHANG share of trailers in the microcell                 share of trailers in the microcell                no_info                 [0, 1, 2, 3]  \n",
       "KBA05_CCM4   share of cars with more than 2499ccm               share of cars with more than 2499ccm              no_info              [0, 1, 2, 3, 4]  \n",
       "KBA05_DIESEL share of cars with Diesel-engine in the microcell  share of cars with Diesel-engine in the microcell no_info              [0, 1, 2, 3, 4]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maybe_null = ['none']\n",
    "\n",
    "'''\n",
    "check if none is also equal to null and should be added to to_nul:\n",
    "none does not mean null in any case (see examples below).\n",
    "'''\n",
    "null_check_Attribute = attr_num[attr_num['Meaning'].isin(maybe_null)]['Attribute'].to_list()\n",
    "null_check = attr_num[attr_num['Attribute'].isin(null_check_Attribute)]\n",
    "\n",
    "pv = score_meaning_val(null_check)\n",
    "view_all(pv.iloc[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Note]** p_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['2.3.1-value_-1'] = 'All value -1s in datasets to be null'\n",
    "p_process['2.3.1-value_0'] = 'Value 0s to be null if corresponding Meaning value is in to_null'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **max Score values**  \n",
    "  most Values with Meanings equal to NaN are already in list to_null,   \n",
    "  except Meaning 'inactive' added to list to_null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['experience-driven elderly', 'uniformly distributed',\n",
       "       '01.01.1995 bis 31.12.1999', 'female', 'more than 100 km ',\n",
       "       'urban working class',\n",
       "       'Advertising-Enthusiast with restricted Cross-Channel-Behaviour ',\n",
       "       'very high activity', 'no transactions known',\n",
       "       '100% Online-transactions within the last 12 months', 'Inactive',\n",
       "       'more than 999 HH/²', 'unremarkable', 'very low',\n",
       "       'mixed building without actually known company ',\n",
       "       'residential cell', 'without vacation',\n",
       "       'belongs to the green avantgarde', 'jaunty hedonists ',\n",
       "       'very low income', 'distance to the city centre > 40 km',\n",
       "       'very high', 'very many', 'very high share of 1-2 family homes',\n",
       "       'very high share of 3-5 family homes',\n",
       "       'high share of 6-10 family homes',\n",
       "       'high share of >10 family homes', 'very high car quote',\n",
       "       'mainly business buildings in the microcell', 'high',\n",
       "       '>=23 buildings', 'new building', 'way above average',\n",
       "       'above average', 'elder than 60 years', 'since 2001', 'Asian',\n",
       "       'upper class car', '2 or more preowner', 'big engine', 'some',\n",
       "       'mainly very big engines', 'low',\n",
       "       'building is not located in a 10 x 10km-range to a a consumption cell',\n",
       "       'multi-generational household',\n",
       "       'top earners at retirement age from mulitperson households',\n",
       "       'high-income earners of higher age from multiperson households',\n",
       "       'top earners ', 'none', 'assimilated names', 'highest  ',\n",
       "       '> 700.000  inhabitants', 'unknown', 'very high share',\n",
       "       'high share', 'mainly business building',\n",
       "       'more than 449 buildings', 'more than 849 households',\n",
       "       '90ies - ecological awareness (Avantgarde, O+W)',\n",
       "       'marginal groups', 'determined Minimal-Returner',\n",
       "       'lowest affinity', 'demanding shopper', 'other',\n",
       "       'individualistic-accepting risks',\n",
       "       'length of residence more than 10 years',\n",
       "       'new building in rural neighbourhood', 'very unlikely',\n",
       "       'indifferent'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "extract Meaning values corresponding to max Score values\n",
    "'''\n",
    "attr_num['Score'] = attr_num['Score'].astype(float)\n",
    "max_idx = attr_num.groupby('Attribute')['Score'].idxmax()\n",
    "\n",
    "attr_num.loc[max_idx, 'Meaning'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniformly distributed', 'Inactive', 'unremarkable', 'other', 'indifferent']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "select Meaning values to be checked\n",
    "'''\n",
    "maybe_null = ['uniformly distributed', 'no transactions known', 'Inactive', 'unremarkable',\n",
    "              'unknown', 'other', 'indifferent']\n",
    "\n",
    "maybe_null = [i for i in maybe_null if i not in to_null]\n",
    "maybe_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Desc</th>\n",
       "      <th>Additional notes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALTERSKATEGORIE_GROB</th>\n",
       "      <th>age classification through prename analysis</th>\n",
       "      <th>age through prename analysis</th>\n",
       "      <th>modelled on millions of first name-age-reference data</th>\n",
       "      <td>[&lt; 30 years, 30 - 45 years, 46 - 60 years, &gt; 60 years, uniformly distributed]</td>\n",
       "      <td>[1.0, 2.0, 3.0, 4.0, 9.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D19_KONSUMTYP</th>\n",
       "      <th>consumption type</th>\n",
       "      <th>consumption type</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[Universal, Versatile, Gourmet, Family, Informed , Modern, Inactive]</td>\n",
       "      <td>[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 9.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FINANZTYP</th>\n",
       "      <th>best descirbing financial type for the person</th>\n",
       "      <th>best descirbing financial type for the peron</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[unknown, low finacial interest, money saver, main focus is the own house, be prepared, Investor, unremarkable]</td>\n",
       "      <td>[-1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TITEL_KZ</th>\n",
       "      <th>flag whether this person holds an academic title</th>\n",
       "      <th>flag whether this person holds an academic title</th>\n",
       "      <th>no_info</th>\n",
       "      <td>[Dr., Dr. Dr., Prof., Prof. Dr., other]</td>\n",
       "      <td>[1.0, 2.0, 3.0, 4.0, 5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZABEOTYP</th>\n",
       "      <th>typification of energy consumers</th>\n",
       "      <th>typification of energy consumers</th>\n",
       "      <th>modelled on different AZ DIAS data</th>\n",
       "      <td>[green, smart, fair supplied, price driven, seeking orientation, indifferent]</td>\n",
       "      <td>[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                         Meaning  \\\n",
       "Attribute            Description                                       Desc                                             Additional notes                                                                                                                                                           \n",
       "ALTERSKATEGORIE_GROB age classification through prename analysis       age through prename analysis                     modelled on millions of first name-age-reference data                                      [< 30 years, 30 - 45 years, 46 - 60 years, > 60 years, uniformly distributed]   \n",
       "D19_KONSUMTYP        consumption type                                  consumption type                                 no_info                                                                                             [Universal, Versatile, Gourmet, Family, Informed , Modern, Inactive]   \n",
       "FINANZTYP            best descirbing financial type for the person     best descirbing financial type for the peron     no_info                                                  [unknown, low finacial interest, money saver, main focus is the own house, be prepared, Investor, unremarkable]   \n",
       "TITEL_KZ             flag whether this person holds an academic title  flag whether this person holds an academic title no_info                                                                                                                          [Dr., Dr. Dr., Prof., Prof. Dr., other]   \n",
       "ZABEOTYP             typification of energy consumers                  typification of energy consumers                 modelled on different AZ DIAS data                                                         [green, smart, fair supplied, price driven, seeking orientation, indifferent]   \n",
       "\n",
       "                                                                                                                                                                                                                Score  \n",
       "Attribute            Description                                       Desc                                             Additional notes                                                                               \n",
       "ALTERSKATEGORIE_GROB age classification through prename analysis       age through prename analysis                     modelled on millions of first name-age-reference data               [1.0, 2.0, 3.0, 4.0, 9.0]  \n",
       "D19_KONSUMTYP        consumption type                                  consumption type                                 no_info                                                   [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 9.0]  \n",
       "FINANZTYP            best descirbing financial type for the person     best descirbing financial type for the peron     no_info                                                  [-1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]  \n",
       "TITEL_KZ             flag whether this person holds an academic title  flag whether this person holds an academic title no_info                                                             [1.0, 2.0, 3.0, 4.0, 5.0]  \n",
       "ZABEOTYP             typification of energy consumers                  typification of energy consumers                 modelled on different AZ DIAS data                             [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "extract Attributes of which max values are Meanings of list maybe_null\n",
    "'''\n",
    "# null_check_Attribute = attr_num[attr_num['Meaning'].isin(maybe_null)]['Attribute'].to_list()\n",
    "# null_check = attr_num[attr_num['Attribute'].isin(null_check_Attribute)]\n",
    "\n",
    "\n",
    "null_check_Attribute = attr_num[attr_num['Meaning'].isin(maybe_null)]['Attribute'].to_list()\n",
    "null_check = attr_num[attr_num['Attribute'].isin(null_check_Attribute)]\n",
    "\n",
    "max_check = attr_num.loc[max_idx]\n",
    "max_check = max_check[max_check['Meaning'].isin(maybe_null)]['Attribute'].tolist()\n",
    "max_check = attr_num[attr_num['Attribute'].isin(max_check)]\n",
    "\n",
    "pv = score_meaning_val(null_check)\n",
    "view_all(pv.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m max_check \u001b[39m=\u001b[39m attr_num[attr_num[\u001b[39m'\u001b[39m\u001b[39mAttribute\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(max_check)]\n\u001b[1;32m     10\u001b[0m max_check \u001b[39m=\u001b[39m vlookup(max_check, feature_desc, \u001b[39m'\u001b[39m\u001b[39mAttribute\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39m'\u001b[39m\u001b[39mDesc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAdditional notes\u001b[39m\u001b[39m'\u001b[39m], nan_val \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mno_info\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m pv_attr \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mpivot_table(\n\u001b[1;32m     12\u001b[0m     max_check,\n\u001b[1;32m     13\u001b[0m     index \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mAttribute\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mDescription\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mDesc\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mAdditional notes\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     14\u001b[0m     values \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mMeaning\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mValue\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     15\u001b[0m     aggfunc \u001b[39m=\u001b[39;49m \u001b[39mlambda\u001b[39;49;00m x: \u001b[39mlist\u001b[39;49m(x))\n\u001b[1;32m     16\u001b[0m view_all(pv_attr)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:97\u001b[0m, in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m     94\u001b[0m     table \u001b[39m=\u001b[39m concat(pieces, keys\u001b[39m=\u001b[39mkeys, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39m__finalize__(data, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpivot_table\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m table \u001b[39m=\u001b[39m __internal_pivot_table(\n\u001b[1;32m     98\u001b[0m     data,\n\u001b[1;32m     99\u001b[0m     values,\n\u001b[1;32m    100\u001b[0m     index,\n\u001b[1;32m    101\u001b[0m     columns,\n\u001b[1;32m    102\u001b[0m     aggfunc,\n\u001b[1;32m    103\u001b[0m     fill_value,\n\u001b[1;32m    104\u001b[0m     margins,\n\u001b[1;32m    105\u001b[0m     dropna,\n\u001b[1;32m    106\u001b[0m     margins_name,\n\u001b[1;32m    107\u001b[0m     observed,\n\u001b[1;32m    108\u001b[0m     sort,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39m__finalize__(data, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpivot_table\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:143\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m values:\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m data:\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(i)\n\u001b[1;32m    145\u001b[0m to_filter \u001b[39m=\u001b[39m []\n\u001b[1;32m    146\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m keys \u001b[39m+\u001b[39m values:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Value'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "extract Attributes of which max values are Meanings of list maybe_null\n",
    "'''\n",
    "max_check = attr_num.loc[max_idx]\n",
    "max_check = max_check[max_check['Meaning'].isin(maybe_null)]['Attribute'].tolist()\n",
    "'''\n",
    "check what maybe_null values to add to to_nul\n",
    "'''\n",
    "max_check = attr_num[attr_num['Attribute'].isin(max_check)]\n",
    "max_check = vlookup(max_check, feature_desc, 'Attribute', ['Desc', 'Additional notes'], nan_val = 'no_info')\n",
    "pv_attr = pd.pivot_table(\n",
    "    max_check,\n",
    "    index = ['Attribute', 'Description', 'Desc', 'Additional notes'],\n",
    "    values = ['Meaning', 'Value'],\n",
    "    aggfunc = lambda x: list(x))\n",
    "view_all(pv_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_null.append('Inactive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* max가 7 보다 큰 경우:   \n",
    "  모두 discrete로 파악됨, Max에 상은하는 Meaning에 null에 상응하는 데이터('no transactions known')가 있으나 이미 to_null에 포함되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pv_attr_num에서 max Value가 7보다 큰 Attribute를 추출함\n",
    "max_over_7 = pv_attr_num[pv_attr_num[('max', 'Value')] > 7].index.get_level_values(0)\n",
    "\n",
    "# max가 7보다 큰 Attribut의 Meaning을 List에 담아 데이터가 discrete / continuous 여부를 파악\n",
    "max_over_7 = in_attr_num[in_attr_num.Attribute.isin(max_over_7)]\n",
    "pv_max_over_7 = pd.pivot_table(\n",
    "    max_over_7,\n",
    "    index = 'Attribute',\n",
    "    values = 'Meaning',\n",
    "    aggfunc = lambda x: x)\n",
    "\n",
    "view_all(pv_max_over_7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.4 str values\n",
    "\n",
    "98 features in str type are analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_attr_str = in_attr[in_attr.Value_type == str]\n",
    "print ('number of features in str type:', in_attr_str.Attribute.nunique())\n",
    "print ('number of intersection features in int & str type:',\n",
    "       len(set(in_attr_str.Attribute.unique()).intersection(set(in_attr_num.Attribute.unique()))), '\\n')\n",
    "\n",
    "pv_attr = pd.pivot_table(\n",
    "    in_attr_str,\n",
    "    index = ['Attribute', 'Description', 'Meaning'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: list(x))\n",
    "\n",
    "pv_attr.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the case of 2 numbers paired to mean 'unknown' is detected, pivot table is remade disregarding this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_attr_str = in_attr_str[in_attr_str.Meaning != 'unknown']\n",
    "\n",
    "pv_attr_str = pd.pivot_table(\n",
    "    in_attr_str,\n",
    "    index = ['Attribute', 'Description'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: x)\n",
    "\n",
    "view_all(pv_attr_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMEO_DEU_2015 and OST_WEST_KZ contain discrete data.    \n",
    "There are 7 continuous data detected, of which skewness should be checked for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_feature = in_attr_str[in_attr_str['Meaning'].str.contains('numeric value')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_feature = in_attr_str[in_attr_str['Meaning'].str.contains('numeric value')]\n",
    "cont_ft = list(cont_feature['Attribute'].unique())\n",
    "p_process['3.2.2'] = 'check skewness for scaling of ' + ', '.join(cont_ft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset exploration\n",
    "\n",
    "Based on understandings on data contents from information files in section2 above,   \n",
    "I will explore 2 main datasets to finalize preparation for pre-processing.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create a concatenated dataframe of 2 datasets for temporary use\n",
    "'''\n",
    "concat_data = pd.concat([customers, azdias], axis=0)\n",
    "concat_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 numeric features\n",
    "\n",
    "For numeric features, concat_numeric including int and float type data is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_numeric = concat_data.select_dtypes(include = ['int', 'float']).copy()\n",
    "concat_num = concat_numeric.copy()\n",
    "concat_num.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And concat_num with the summary statistics is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract summary statistics of concat_numeric by applying describe and transpose\n",
    "concat_stat = concat_num.describe().T.reset_index()\n",
    "\n",
    "# merge Desc (information on Attribute) from feature_desc and add min_max_cat\n",
    "concat_stat = concat_stat.rename(columns = {'index' : 'Attribute'})\n",
    "concat_stat = vlookup(concat_stat, feature_desc, 'Attribute', 'Desc')\n",
    "concat_stat['min_max_cat'] = concat_stat[\n",
    "    'min'].apply(lambda x: '{:_.0f}'.format(x)).astype(str) + ' to ' + concat_stat[\n",
    "    'max'].apply(lambda x: '{:_.0f}'.format(x)).astype(str)\n",
    "\n",
    "concat_stat.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From min/max values\n",
    "* min Value -1 should be replaced to NaN - noted in 2.3.3\n",
    "* most features has max values not exceeding 40 and can be regarded as discrete - checked in 2.3.3\n",
    "* features with max values over 40 will be further analyzed below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_stat.min_max_cat.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 detailed feature check: numeric features with max over 40 & continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features with max values over 40 from concat_stat\n",
    "concat_high_max = concat_stat[concat_stat['max'] > 40][['Attribute', 'Desc']]\n",
    "ft_to_check = list(concat_high_max.Attribute.unique())\n",
    "\n",
    "# compare list of features of high max values with list of continuos features - cont_ft in 2.3.4 \n",
    "set (cont_ft) - set(ft_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_to_check.extend(['ANZ_HH_TITEL', 'ANZ_TITEL'])\n",
    "\n",
    "view_all(concat_stat[concat_stat['Attribute'].isin(ft_to_check)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LNR with the same count of concat_numeric seems to be the serial index of dataset, that it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2'] = 'drop LNR'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features in ft_to_check are analyzed one by one using function view_feature below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function] view_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_feature (feature, view_all = True, view_0_10 = True):\n",
    "    '''\n",
    "    function to view and check continuous numeric data\n",
    "    feature : str, feature name\n",
    "    view_0_10 : boolean for histogram display of value 0 to 10, default as True\n",
    "    '''\n",
    "\n",
    "    min_val = concat_numeric[feature].min()\n",
    "    max_val = concat_numeric[feature].max()\n",
    "    bin_edges = np.arange(min_val, max_val + 10, 10)\n",
    "    desc_val = concat_stat[concat_stat['Attribute'] == feature]['Desc'].values[0]\n",
    "\n",
    "    if view_all:\n",
    "        ax = concat_numeric[feature].plot(\n",
    "            kind = 'hist',\n",
    "            figsize=(10, 1.5),\n",
    "            color='gray',\n",
    "            bins = bin_edges,\n",
    "            align = 'mid',\n",
    "            title = ('histogram - ' + desc_val + ' - ' + feature)\n",
    "            );\n",
    "        ax.set_xlabel('Values - Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "        plt.show()\n",
    "\n",
    "    if view_0_10:    \n",
    "        ax = concat_numeric[feature].plot(\n",
    "            kind = 'hist',\n",
    "            figsize=(10, 1.5),\n",
    "            color='gray',\n",
    "            bins = np.arange(-0.5, 11.5, 1),\n",
    "            align = 'mid',\n",
    "            title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "            );\n",
    "        ax.set_xlabel('Values - Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "        plt.show()\n",
    "    \n",
    "    # define the outlier thresholds by applying multiplier 1.5\n",
    "    q1 = concat_stat[concat_stat['Attribute'] == feature]['25%'].values[0]\n",
    "    q3 = concat_stat[concat_stat['Attribute'] == feature]['75%'].values[0]\n",
    "    iqr = q3 - q1\n",
    "    lower_threshold = q1 - 1.5 * iqr\n",
    "    upper_threshold = q3 + 1.5 * iqr\n",
    "\n",
    "    # identify outliers\n",
    "    col_val = concat_numeric[feature].values\n",
    "    outliers = sorted(\n",
    "        set([feature for feature in col_val if feature < lower_threshold or feature > upper_threshold]),\n",
    "        reverse = True)\n",
    "\n",
    "    # print outliers\n",
    "    count_val = concat_stat[concat_stat['Attribute'] == feature].fillna(0)['count'].values[0]       \n",
    "    outlier_list = [\n",
    "        str(int(j)) + ': ' + '{:.1%}'.format((concat_numeric[feature] == j).sum() / count_val)\n",
    "        for j in outliers\n",
    "        ]\n",
    "    \n",
    "    print('Outliers (Value: %)')\n",
    "    for j in range(0, len(outlier_list), 10):\n",
    "        print (', '.join(outlier_list[j : j+10]))\n",
    "    print ('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_HAUSHALTE_AKTIV  \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "  - Log scale is needed due to high skewness  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_HAUSHALTE_AKTIV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_HAUSHALTE_AKTIV'] = '[Log scale]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_HH_TITEL  \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_HH_TITEL', False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_PERSONEN  \n",
    "  - Value 0: a household can not have 0 person, that Value 0 should be replaced to NaN\n",
    "  - outliers: household with over 10 persons is highly extreme or data error, that Value over 10 should be replaced to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_PERSONEN', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_PERSONEN'] = 'replace Value 0 to NaN / Value > 10 to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_STATISTISCHE_HAUSHALTE   \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "  - Log scale is needed due to high skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_STATISTISCHE_HAUSHALTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_STATISTISCHE_HAUSHALTE'] = '[Log scale]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_STATISTISCHE_HAUSHALTE   \n",
    "  Value 0 and max value might be strange or extreme but are possible, but this feature is linked to ANZ_PERSONEN above\n",
    "  - replace value to NaN if corresponding ANZ_PERSONEN is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_TITEL', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_TITEL'] = 'replace value to NaN if ANZ_PERSONEN is NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EINGEZOGENAM_HH_JAHR \n",
    "  - outliers: Eng translation is not completely understandable, but 3 outliers can be replaced to NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('EINGEZOGENAM_HH_JAHR', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_numeric[concat_numeric['EINGEZOGENAM_HH_JAHR'] < 1980]['EINGEZOGENAM_HH_JAHR'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-AEINGEZOGENAM_HH_JAHR'] = 'replace Value < 1980 to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EXTSEL992   \n",
    "  - No pre-processing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('EXTSEL992')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GEBURTSJAHR  \n",
    "  - outliers: birth year can not be 0, that Value < 1900 should be replaced to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('GEBURTSJAHR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-GEBURTSJAHR'] = 'replace Value < 1900 to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GEMEINDETYP   \n",
    "  - No pre-processing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('GEMEINDETYP', True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KBA13_ANZAHL_PKW   \n",
    "  - Log scale is needed due to high skewness as values over 1250 is grouped by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('KBA13_ANZAHL_PKW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-KBA13_ANZAHL_PKW'] = '[Log scale]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MIN_GEBAEUDEJAHR   \n",
    "  - No pre-processing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('MIN_GEBAEUDEJAHR', True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VERDICHTUNGSRAUM   \n",
    "  - No pre-processing needed with Eng translation not completely understandable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('VERDICHTUNGSRAUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2 str values\n",
    "\n",
    "extract 5 features in str type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_str = concat_data.select_dtypes(exclude = ['int', 'float'])\n",
    "print(concat_str.shape)\n",
    "concat_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type EINGEFUEGT_AM should be changed to datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_str = concat_str.drop('EINGEFUEGT_AM', axis = 1)\n",
    "p_process['3.3.2'] = 'change type of EINGEFUEGT_AM to datetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract values of Attribute in list\n",
    "concat_str = pd.DataFrame(\n",
    "    {'Attribute': concat_str.columns,\n",
    "     'value_list': concat_str.values.T.tolist()})\n",
    "concat_str['value_list'] = concat_str['value_list'].apply(\n",
    "    lambda x: list(pd.Series(x).drop_duplicates().dropna()))\n",
    "\n",
    "# merge Desc (information on Attribute) from feature_desc and add min_max_cat\n",
    "concat_str = vlookup(concat_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "\n",
    "view_all(concat_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.3.2-XX'] = 'replace Value X, XX to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process = {'pre_processing_itmes' : p_process}\n",
    "p_process_items = pd.DataFrame(p_process).reset_index()\n",
    "p_process_items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그 다음에 Preprocessing을 하고 나면 (필요시 극히 비슷한 컬럼 제외),   \n",
    "Imputing, Scaling 하면 PCA, Clustering, 앙상블로 나갈 수 있음** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function] miss_val_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_val_summary(df, axis_val, x_bin = 2, bar_chart = True):\n",
    "    '''\n",
    "    function to display summary, bar-chart (optional) and histogram\n",
    "    of missing value by column or raw\n",
    "    df: dataframe\n",
    "    axis_val: str, one of 'column' or 'row'\n",
    "    x_bin: size of x bin, 10 as default\n",
    "    bar_chart: option of bar chart display \n",
    "    '''\n",
    "    # index of axis\n",
    "    axis_idx = 0 if axis_val == 'column' else 1\n",
    "    \n",
    "    # % of missing values\n",
    "    missing_pct = df.isnull().mean(axis = axis_idx) * 100\n",
    "    df_desc = missing_pct.describe()\n",
    "\n",
    "    # summary of missing value\n",
    "    print (\n",
    "        '% of missing value in ' + str(int(df_desc[0])) + ' ' + axis_val + 's of ' + df.name)\n",
    "    print (df_desc[1:].to_string())\n",
    "    \n",
    "    # bar-chart of missing value\n",
    "    if bar_chart:\n",
    "        missing_pct.plot(\n",
    "            kind = 'bar', figsize=(10, 3), color='gray',\n",
    "            \n",
    "            title = ('bar chart - ' + df.name + ': missing value by ' + axis_val),\n",
    "            ylabel = '% of missing value',\n",
    "            xlabel = (str(int(df_desc[0])) + ' columns'),\n",
    "            xticks = [],\n",
    "            );\n",
    "        plt.show()\n",
    "    \n",
    "    # hist of missing value\n",
    "    x_range = ((df_desc[-1] + 10) // 10) * 10 + x_bin\n",
    "    ax = missing_pct.plot(\n",
    "        kind = 'hist', figsize=(10, 3), color='gray',\n",
    "        \n",
    "        bins = np.arange(0, x_range, x_bin),\n",
    "        title = ('histogram - ' + df.name + ': missing value by ' + axis_val)\n",
    "        )\n",
    "    ax.set_xlabel('% of missing value');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "missing value by columns of azdias\n",
    "'''\n",
    "\n",
    "miss_val_summary(azdias, 'column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "missing value by column\n",
    "'''\n",
    "\n",
    "miss_val_summary(customers, 'column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 alignment of features   \n",
    "as values of data files (azdias & customers) can be readable by explanations of information files (info & attr),   \n",
    "check alignment in column features of data files and equivalent values of column Attribute of information files at first   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 합쳐서 데이터 분석시 참조\n",
    "> 93 데이터에만 있는 Attr은 어떻게 할 것인가?\n",
    "> 데이터 파일에 없고 정보 파일에만 있는 51 Attr은 제외하여 simplify 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 feature description\n",
    "As the data sets do not have information on what each feature (column name) exactly means,   \n",
    "values of information files (info & attr) should be mapped to the features at first,   \n",
    "to see how the data sets are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to know what features datasets have, add information to column names of customers\n",
    "'''\n",
    "\n",
    "feature_desc = pd.DataFrame(customers.columns, columns=['Attribute'])\n",
    "feature_desc = vlookup(feature_desc, info, 'Attribute')\n",
    "\n",
    "'''\n",
    "df_feature has 369 unique Attribute values:\n",
    "105 exclusive values of customers and 264 shared values with customers\n",
    "(see 1.5 alignment of features)\n",
    "''' \n",
    "print ('Attributes missing Description:', feature_desc[feature_desc.Description.isna()].shape[0])\n",
    "print (feature_desc.shape)\n",
    "feature_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To enhance readability of data set - over 100 features are without description -   \n",
    "I added 14 Description values from attr, and made a column of translation   \n",
    "(ger_to_eng) to df_feature using code below.\n",
    "However as this code-running takes somewhat long time, I saved the dataframe processed\n",
    "as df_feature.xlsx in root folder\n",
    "'''\n",
    "\n",
    "# # For values of Attribute without Description, add 14 Description values from attr\n",
    "# df_feature.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# df_feature.update(attr_excl)\n",
    "\n",
    "# df_feature.reset_index(inplace = True)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# df_feature['ger_to_eng'] = np.where(\n",
    "#     df_feature.Description.isnull(),\n",
    "#     df_feature.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# df_feature['Desc'] = df_feature.Description.fillna('') + df_feature.ger_to_eng.fillna('')\n",
    "\n",
    "feature_desc = pd.read_excel('feature_desc.xlsx', index_col = [0])\n",
    "feature_desc.head(10)\n",
    "\n",
    "print ('Attributes missing Desc:', feature_desc[feature_desc.Desc.isna()].shape[0])\n",
    "print (feature_desc.shape)\n",
    "feature_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find all features one by one\n",
    "'''\n",
    "\n",
    "# # not to run\n",
    "\n",
    "# view_all(feature_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(attr.Attribute.unique()) - set(feature_desc.Attribute.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (list(set(attr.Attribute.unique()) - set(feature_desc.Attribute.unique())))\n",
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df = attr.iloc[:, :2].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df[short_df.Attribute.isin(diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[attr.iloc[:, :2].drop_duplicates().Attribute.isin(diff)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 살펴 봤음. 모든 Attribute에 대해서 Description을 completely 이해할 수는 없었으나   \n",
    "> 대체적인 내용 구성을 이해할 수는 있었음   \n",
    "> 비슷한 내용을 나타내는 중복열, 유사열이 많아 공선성 해소, 차원 축소가 필요함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 data values\n",
    "The values of the data sets can also be readable by mapping values of Value and Meaning of attr.   \n",
    "In this section, 데이터 종류 (연속/이산), 이상치, 사실상의 null value 등 데이터 전처리를 위한 데이터의 내용적 측면을 점검하겠음.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check types of values of attr Value, which contains information on data values\n",
    "'''\n",
    "\n",
    "attr['Value_dtype'] = attr.Value.map(type)\n",
    "print (attr.Value_dtype.value_counts())\n",
    "\n",
    "# attr['Meaning_dtype'] = attr.Meaning.map(type)\n",
    "# print (attr.Meaning_dtype.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 attr의 int 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5에서 살펴본 바와 같이 272개 피쳐는 attr을 통해 설명이 가능함 (물론 이것도 데이터를 따로 파악해 봐야 하나)    \n",
    "데이터 셋을 직접 살펴 보는 것은 뒤에 별도록 진행하고 우선 attr을 분석하여 데이터 내용이 어떻게 구성되어 있는지 Basis를 확보해야 함.   \n",
    "\n",
    "2113개의 정수 값이고, 145개는 object로 정수 값으로 정의된 value를 먼저 점검해 보겠음.\n",
    "\n",
    "* numeric data of column Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "numeric data of column Value\n",
    "'''\n",
    "# attr_num with only numeric values in Value\n",
    "attr_num = attr[attr['Value_dtype'] == int].copy()\n",
    "print (attr_num.shape) \n",
    "\n",
    "# add Desc and Information level\n",
    "attr_num = vlookup(attr_num, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "print (attr_num.info())\n",
    "attr_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "customers에는 없고 attr에만 있는 42개 Attribute는 Desc가 없으으로\n",
    "(1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "'''\n",
    "\n",
    "print ('customers에는 없고 attr에만 있는 42개 Attribute 수:', \n",
    "       attr_num[attr_num.Desc.isna() == True].Attribute.nunique(),\n",
    "       '\\n')\n",
    "\n",
    "attr_num = attr_num[attr_num.Desc.isna() == False]\n",
    "print (attr_num.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "numeric data of column Value where 1774 rows have data in int type 으로\n",
    "pivot을 돌려 int type Value가 어떤 min max 값을 가지는 지 정리함.\n",
    "이를 통해 각 feature의 data type을 추정하고, 효과적으로 비정상 값을 찾아내려 함\n",
    "'''\n",
    "\n",
    "# summary of numeric data of Value\n",
    "pv_attr_num = pd.pivot_table(\n",
    "    attr_num,\n",
    "    index = ['Attribute', 'Desc'],\n",
    "    values = 'Value',\n",
    "    aggfunc = [min, max]\n",
    "    )\n",
    "\n",
    "pv_attr_num['min_max_cat'] = pv_attr_num['min'].astype(str) + ' to ' + pv_attr_num['max'].astype(str)\n",
    "pv_attr_num = pv_attr_num.sort_values(by = 'min_max_cat')\n",
    "\n",
    "print (\n",
    "    'min_max category of numeric data in column Value', '\\n',\n",
    "    pv_attr_num.min_max_cat.value_counts())\n",
    "pv_attr_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find all numeric data of column Value one by one\n",
    "'''\n",
    "\n",
    "# # not to run\n",
    "\n",
    "# with pd.option_context(\n",
    "#     'display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(pv_attr_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. value -1, 0의 경우 null 또는 -inf에 상응하는 데이터가 있는지  \n",
    "> 2. max가 7까지는 descrete, 8 이상은 continue 인지 봐야하고 Max의 null 또는 -inf에 상응하는 데이터가 있는지\n",
    "> 3. binary cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. value -1, 0의 경우 null 또는 -inf에 상응하는 데이터가 있는지 \n",
    "'''\n",
    "attr_below_1 = attr_num[attr_num.Value < 1]\n",
    "print(attr_below_1.shape)\n",
    "attr_below_1.Meaning.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "위 값 중 null 가능성이 있는 값만 추려 본결과\n",
    "'''\n",
    "\n",
    "maybe_null = ['unknown',\n",
    "              'no classification possible',\n",
    "              'unknown / no main age detectable',\n",
    "              'no transactions known', \n",
    "              'no transaction known', \n",
    "              'classification not possible',\n",
    "              'none',\n",
    "              'no score calculated'\n",
    "              ]\n",
    "\n",
    "attr_below_1 = attr_below_1[attr_below_1.Meaning.isin(maybe_null)].sort_values(by = 'Meaning')\n",
    "print(attr_below_1.shape)\n",
    "attr_below_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not to run\n",
    "\n",
    "# view_all(attr_below_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Meaning이 maybe_null 이면 모두 nan 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. max가 7까지는 descrete, 8 이상은 continue 인지 봐야하고 Max의 null 또는 -inf에 상응하는 데이터가 있는지\n",
    "'''\n",
    "\n",
    "# pv_attr_num에서 max가 7보다 큰 Attr을 추출함\n",
    "max_over_7_Attr = pv_attr_num[pv_attr_num[('max', 'Value')] > 7].index.get_level_values(0)\n",
    "# view_all(attr_num[attr_num.Attribute.isin(max_over_7_Attr)])\n",
    "max_over_7 = attr_num[attr_num.Attribute.isin(max_over_7_Attr)]\n",
    "pv_max_over_7 = pd.pivot_table(\n",
    "    max_over_7,\n",
    "    index = 'Attribute',\n",
    "    values = 'Meaning',\n",
    "    aggfunc = lambda x: list(x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all(pv_max_over_7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> no continuous data detected, 그러나 최대값에 maybe null이 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_over_7['Value'] = pd.to_numeric(max_over_7['Value'], errors='coerce')\n",
    "\n",
    "max_over_7_idxmax = max_over_7.groupby('Attribute')['Value'].idxmax()\n",
    "\n",
    "max_over_7.loc[max_over_7_idxmax, 'Meaning'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> maybe_null에 'uniformly distributed', ... 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "3. binary cells\n",
    "'''\n",
    "\n",
    "# pv_attr_num에서 max가 4보다 작은 Attr을 추출함\n",
    "max_under_4_Attr = pv_attr_num[pv_attr_num[('max', 'Value')] < 4].index.get_level_values(0)\n",
    "# view_all(attr_num[attr_num.Attribute.isin(max_over_7_Attr)])\n",
    "max_under_4 = attr_num[attr_num.Attribute.isin(max_under_4_Attr)]\n",
    "pv_max_under_4 = pd.pivot_table(\n",
    "    max_under_4,\n",
    "    index = 'Attribute',\n",
    "    values = 'Meaning',\n",
    "    aggfunc = lambda x: list(x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all(pv_max_under_4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 뭐뭐뭐가 이진으로 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 attr의 str 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "str data of column Value\n",
    "'''\n",
    "# attr_num with only numeric values in Value\n",
    "attr_str = attr[attr['Value_dtype'] == str].copy()\n",
    "print (attr_str.shape) \n",
    "\n",
    "# add Desc and Information level\n",
    "attr_str = vlookup(attr_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "print (attr_str.info())\n",
    "attr_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3개 null attribute는 정수와 문자를 모두 값을로 갖는 것들로 42개는 유효하고\n",
    "이 42개는 다음 section에서 볼 예정이므로 (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "'''\n",
    "\n",
    "# attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나...\n",
    "\n",
    "attr_str = attr_str[attr_str.Desc.isna() == False]\n",
    "print (attr_str.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pivot을 돌려 attribute 별로 어떤 str 값을 가지는 지 정리함.\n",
    "이를 통해 각 feature의 data type을 추정하고, 효과적으로 비정상 값을 찾아내려 함\n",
    "'''\n",
    "\n",
    "pv_attr_str = pd.pivot_table(\n",
    "    attr_str,\n",
    "    index = ['Attribute', 'Desc', 'Meaning'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: x\n",
    "    )\n",
    "\n",
    "pv_attr_str.head(10)\n",
    "# view_all(pv_attr_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. unknown을 표현하는 경우, drop에 포함    \n",
    "> 2. 연속형 수치를 표현하는 경우 ... 이는 data set을 직접 보고 파악해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "순수하게 str인 경우만 추출\n",
    "'''\n",
    "# pv_attr_str = pd.DataFrame(pv_attr_str.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_str_to_check = pv_attr_str[pv_attr_str.Meaning.str.contains('numeric value')].Attribute\n",
    "attr_str_to_check = pv_attr_str[\n",
    "    pv_attr_str.index.get_level_values(2).str.contains('numeric value')].index.get_level_values(0)\n",
    "# 먼저 추후 체크할 것들을 뽑아 놓고\n",
    "attr_str_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_attr_str = pd.DataFrame(pv_attr_str[\n",
    "    ~(pv_attr_str.index.get_level_values(0).isin(attr_str_to_check))\n",
    "    &~(pv_attr_str.index.get_level_values(2) == 'unknown')\n",
    "    ].to_records())\n",
    "\n",
    "pv_attr_str = pd.pivot_table(\n",
    "    pv_attr_str,\n",
    "    index = ['Attribute', 'Desc'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: list(x)\n",
    "    )\n",
    "\n",
    "view_all(pv_attr_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CAMEO_DEU_2015 정상적인 카테고리 데이터... 피쳐   \n",
    "> OST_WEST_KZ은 2진"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 customers에만 있는 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**상당히 해깔리게 되어 있는데, 데이터의 컬럼과 정보 파일의 Attribute 숫자를 좀 정확하게 정리하고   \n",
    "하던데로 커스터머에만 있는 데이터를 정리하면 Wrangling이 끝남** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그 다음에 Preprocessing을 하고 나면 (필요시 극히 비슷한 컬럼 제외),   \n",
    "Imputing, Scaling 하면 PCA, Clustering, 앙상블로 나갈 수 있음** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. attributes_xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_val(attributes_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = list(attributes_xlsx.Meaning.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_words = []\n",
    "\n",
    "for synset in w_list:\n",
    "    if len(synset.lemmas()) > 1:\n",
    "        ambiguous_words.append(synset.name().split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(wordnet.all_synsets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wordnet.synsets('unknown')\n",
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "\n",
    "for synset in synsets:\n",
    "    for lemma in synset.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the target word\n",
    "target_word = 'unidentified'\n",
    "\n",
    "# Retrieve synsets for the target word\n",
    "synsets = wordnet.synsets(target_word)\n",
    "\n",
    "# # Retrieve synonyms for each synset and filter out synonyms containing the target word\n",
    "# filtered_synonyms = []\n",
    "\n",
    "# for synset in synsets:\n",
    "#     synonyms = synset.lemmas()\n",
    "#     filtered_synonyms.extend([synonym.name() for synonym in synonyms if target_word not in synonym.name()])\n",
    "\n",
    "# # Remove duplicate synonyms and sort the list\n",
    "# filtered_synonyms = sorted(set(filtered_synonyms))\n",
    "\n",
    "# print(filtered_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_synonyms = []\n",
    "\n",
    "for synset in synsets:\n",
    "    synonyms = synset.lemmas()\n",
    "    filtered_synonyms.extend([synonym.name() for synonym in synonyms if target_word not in synonym.name()])\n",
    "\n",
    "# Remove duplicate synonyms and sort the list\n",
    "filtered_synonyms = sorted(set(filtered_synonyms))\n",
    "\n",
    "print(filtered_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_word in filtered_synonyms:\n",
    "    # # Define the target word\n",
    "    # target_word = 'unidentified'\n",
    "\n",
    "    # Retrieve synsets for the target word\n",
    "    synsets = wordnet.synsets(target_word)\n",
    "\n",
    "    for synset in synsets:\n",
    "        synonyms = synset.lemmas()\n",
    "        filtered_synonyms.extend([synonym.name() for synonym in synonyms if target_word not in synonym.name()])\n",
    "\n",
    "    # Remove duplicate synonyms and sort the list\n",
    "    filtered_synonyms = sorted(set(filtered_synonyms))\n",
    "\n",
    "print(filtered_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customers\n",
    "\n",
    "print (customers.info())\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes_xlsx\n",
    "\n",
    "print (attributes_xlsx.info())\n",
    "attributes_xlsx.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify attributes_xlsx\n",
    "\n",
    "attributes_xlsx = attributes_xlsx.iloc[:, 1:] # 1st column has no info\n",
    "attributes_xlsx[['Attribute', 'Description']] = attributes_xlsx[\n",
    "    ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "print (attributes_xlsx.info())\n",
    "attributes_xlsx.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information_xlsx\n",
    "\n",
    "print (information_xlsx.info())\n",
    "information_xlsx.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work / Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def col_val (df):\n",
    "#     '''\n",
    "#     function to check values of dataframe columns\n",
    "#     df : dataframe\n",
    "#     '''\n",
    "#     # for i in df.columns:\n",
    "#     #     print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#     #         df[i].value_counts(), '\\n', '*     *     *')\n",
    "#     for i in df.columns:\n",
    "#         print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#         list(df[i].unique()), '\\n', '*     *     *')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from googletrans import Translator\n",
    "\n",
    "# # Create an instance of the Translator\n",
    "# translator = Translator(service_urls=['translate.google.com'])\n",
    "\n",
    "# # Text to be translated\n",
    "# text = \"AGER_TYP\"\n",
    "\n",
    "# # Translate the text from German to English\n",
    "# translation = translator.translate(text, src='de', dest='en')\n",
    "\n",
    "# # Print the translated text\n",
    "# print(\"Original text (German):\", text)\n",
    "# print(\"Translated text (English):\", translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def miss_val_hist(df, axis_val, x_bin = 10):\n",
    "#     '''\n",
    "#     function to display missing value histogram by column or raw\n",
    "#     df: dataframe\n",
    "#     axis_val: str, one of 'column' or 'row'\n",
    "#     x_bin: size of xtick bin, 10 as default \n",
    "#     '''\n",
    "#     # axis value\n",
    "#     axis_num = 0 if axis_val == 'column' else 1\n",
    "    \n",
    "#     # % of missing values\n",
    "#     missing_pct = df.isnull().mean(axis = axis_num) * 100\n",
    "\n",
    "#     # max % of missing values by column\n",
    "#     missing_pct_max = missing_pct.max()\n",
    "#     print ('max % of missing values by ' + axis_val + ': ', missing_pct_max)\n",
    "\n",
    "#     # plot missing values by column\n",
    "    \n",
    "#     print (missing_pct.describe())\n",
    "    \n",
    "#     x_range = ((missing_pct_max + x_bin * 2) // x_bin) * x_bin\n",
    "\n",
    "#     ax = missing_pct.plot(\n",
    "#         kind = 'hist', figsize=(10, 3), color='gray',\n",
    "#         bins = np.arange(0, x_range, 10),\n",
    "#         title = (df.name + ': missing value by ' + axis_val)\n",
    "#         )\n",
    "#     ax.set_xlabel('% of missing value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이써닉 하지 못한 코드\n",
    "# # missing value overview\n",
    "# for i in range(0, ((azdias.shape[1] + 100) // 100) * 100, 100):\n",
    "#     msno.matrix(azdias.iloc[:, i : i + 99],\n",
    "#                 figsize=(10, 3), fontsize = 12, labels = False, sparkline = False)\n",
    "#     plt.title('missing value overview: col ' + str (i) + ' to ' + str (min(i + 99, azdias.shape[1] - 1)),\n",
    "#               fontsize = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of over 30%\n",
    "# (azdias.isnull().mean() * 100 > 30).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of 25% to 30%\n",
    "# ((azdias.isnull().mean() * 100 > 25) & (30 >= azdias.isnull().mean() * 100)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아마도 쓰지 않을 plot\n",
    "# plot = azdias_col_missing_pct.plot(\n",
    "#     kind = 'bar', figsize=(10, 3), color='dimgray', xticks = [],\n",
    "#     title = 'azdias_col_missing_pct',\n",
    "#     xlabel = '366 columns',\n",
    "#     ylabel = '% of missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_df = customers.select_dtypes(include=['float', 'int64']).iloc[:, 1:]\n",
    "# num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pd.unique(customers.select_dtypes(include='float').values.flatten()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\n",
    "#     'display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(pd.DataFrame(attr.apply(lambda x: x.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_not_null = ~(attr.Attribute.isna())\n",
    "# attr.loc[attr_not_null, 'Description'] = attr.loc[\n",
    "#     attr_not_null, 'Description'] + ' ' + attr.loc[attr_not_null, 'desc_shift']\n",
    "\n",
    "# desc_to_null = (attr.Attribute.isna()) & ~(attr.Description.isna())\n",
    "# attr.loc[desc_to_null, 'Description'] = np.nan\n",
    "# attr = attr.drop(columns = 'desc_shift')\n",
    "# attr.loc[attr_with_value.shift(-1, fill_value = True), 'Description']\n",
    "# attr_shift = attr_null.shift\n",
    "# attr[attr_null.shift, 'Description'] = attr.loc[\n",
    "#     attr_null.shift(fill_value = False), 'Description'] + ' ' + attr[attr_null, 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# info_mg = info_mg.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "# # attr_excl = attr[attr.Attribute.isin(attr_excl)].copy()\n",
    "# attr_not_null = attr.dropna(subset = 'Attribute').copy()\n",
    "# attr_not_null = attr_not_null.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_not_null[['Attribute', 'Description']]],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_excl)][['Attribute', 'Description']].copy()\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# # info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_all(info_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 2258 rows of Attribute in attr, add 13 values exclusively in info,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# attr_mg = attr.copy()\n",
    "# info_excl = info[info.Attribute.isin(info_excl)][['Attribute', 'Description']].copy()\n",
    "# info_excl['Value'] = 'form info'\n",
    "# info_excl['Meaning'] = 'form info'\n",
    "\n",
    "# attr_mg = pd.concat(\n",
    "#     [attr_mg, info_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# # info_mg = info_mg.drop_duplicates()\n",
    "\n",
    "# # info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(attr_mg.info())\n",
    "# attr_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_mg.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# there are 93 and 51 exclusive values in data and information files\n",
    "# '''\n",
    "\n",
    "# datafile_Attr = azdias_Attr.union(customers_Attr)\n",
    "# infofile_Attr = info_Attr.union(attr_Attr)\n",
    "\n",
    "# datafile_excl = datafile_Attr - infofile_Attr\n",
    "# infofile_excl = infofile_Attr - datafile_Attr\n",
    "\n",
    "# print (len(datafile_excl), 'Attribute value(s) exclusively in data files:',\n",
    "#        '\\n', datafile_excl)\n",
    "# print (len(infofile_excl), 'Attribute value(s) exclusively in information files:',\n",
    "#        '\\n', infofile_excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# info['Information level'] = info['Information level'].fillna(method = 'ffill')\n",
    "\n",
    "# info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# attr[['Attribute', 'Description']] = attr[\n",
    "#     ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "# attr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# To enhance readability of data set - over 100 features are without description -   \n",
    "# I added 14 Description values from attr, and made a column of translation   \n",
    "# (ger_to_eng) to df_feature using code below.\n",
    "# However as this code-running takes somewhat long time, I saved the dataframe processed\n",
    "# as df_feature.xlsx in root folder\n",
    "# '''\n",
    "\n",
    "# # For values of Attribute without Description, add 14 Description values from attr\n",
    "# df_feature.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# df_feature.update(attr_excl)\n",
    "\n",
    "# df_feature.reset_index(inplace = True)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# df_feature['ger_to_eng'] = np.where(\n",
    "#     df_feature.Description.isnull(),\n",
    "#     df_feature.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# df_feature['Desc'] = df_feature.Description.fillna('') + df_feature.ger_to_eng.fillna('')\n",
    "\n",
    "# # # sort by Attribute and Information level\n",
    "# # df_feature.sort_values(by = ['Attribute', 'Information level'], inplace= True)\n",
    "\n",
    "# # df_feature = pd.read_excel('df_feature.xlsx', index_col = [0])\n",
    "# # df_feature.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# df_feature.Description = df_feature.Description.mask(\n",
    "#     df_feature.Attribute == attr_excl.Attribute,\n",
    "#     attr_excl.Description\n",
    "#     )\n",
    "# print ('Attributes missing Description:', df_feature[df_feature.Description.isna()].shape[0])\n",
    "# print (df_feature.shape)\n",
    "# df_feature.head()\n",
    "\n",
    "# ValueError: Can only compare identically-labeled Series objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_desc.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_excl = attr[\n",
    "#     attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# feature_desc.update(attr_excl)\n",
    "\n",
    "# feature_desc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between data files: \n",
    "# df customers has 3 more exclusive columns\n",
    "# '''\n",
    "\n",
    "# azdias_Attr = set(azdias.columns)\n",
    "# customers_Attr = set(customers.columns)\n",
    "\n",
    "# print(azdias_Attr - customers_Attr)\n",
    "# print(customers_Attr - azdias_Attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between information files\n",
    "# '''\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "\n",
    "# # info_excl = info_Attr - attr_Attr\n",
    "# # attr_excl = attr_Attr - info_Attr\n",
    "\n",
    "# print (len(info_Attr - attr_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - info_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - info_Attr)\n",
    "# '''\n",
    "# alignment of features between customers and information files\n",
    "# '''\n",
    "# print ('Attribute between customers and info')\n",
    "# print (len(customers_Attr - info_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - info_Attr)\n",
    "# print (len(info_Attr - customers_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - customers_Attr)\n",
    "# print ('In', len(info_Attr), 'features of info,', \n",
    "#        len(info_Attr) - len(info_Attr - customers_Attr), 'features are in Attribute of customers', '\\n')\n",
    "\n",
    "# print ('Attribute between customers and attr')\n",
    "# print (len(customers_Attr - attr_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - customers_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - customers_Attr)\n",
    "# print ('In', len(attr_Attr), 'features of attr,',\n",
    "#        len(attr_Attr) - len(attr_Attr - customers_Attr), 'features are in Attribute of customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(feature_dict.keys())[0]\n",
    "# feature_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = np.array([1, 2, 3, 4])\n",
    "\n",
    "# subtractions = np.subtract.outer(values, values)[np.triu_indices(len(values), k=1)]\n",
    "\n",
    "# for result in subtractions:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (k, v) in enumerate(feature_dict.items()):\n",
    "#     for j in range(i + 1, 4):\n",
    "#             result = values[i] - values[j]\n",
    "#             print(f\"{values[i]} - {values[j]} = {result}\")\n",
    "#     print (i, k, v)\n",
    "    \n",
    "# for i, (k, v) in enumerate(zip(list(feature_dict.keys()), list(feature_dict.values()))):\n",
    "#     print (i, (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# customers에는 없고 attr에만 있는 42개 Attribute는 Desc가 없으으로\n",
    "# (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# print ('customers에는 없고 attr에만 있는 42개 Attribute 수:', \n",
    "#        attr_num[attr_num.Desc.isna() == True].Attribute.nunique(),\n",
    "#        '\\n')\n",
    "\n",
    "# attr_num = attr_num[attr_num.Desc.isna() == False]\n",
    "# print (attr_num.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# str data of column Value\n",
    "# '''\n",
    "# # attr_num with only numeric values in Value\n",
    "# attr_str = attr[attr['Value_dtype'] == str].copy()\n",
    "# print (attr_str.shape) \n",
    "\n",
    "# # add Desc and Information level\n",
    "# attr_str = vlookup(attr_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "# print (attr_str.info())\n",
    "# attr_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 3개 null attribute는 정수와 문자를 모두 값을로 갖는 것들로 42개는 유효하고\n",
    "# 이 42개는 다음 section에서 볼 예정이므로 (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# # attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나...\n",
    "\n",
    "# attr_str = attr_str[attr_str.Desc.isna() == False]\n",
    "# print (attr_str.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attr_str_to_check = pv_attr_str[pv_attr_str.Meaning.str.contains('numeric value')].Attribute\n",
    "# attr_str_to_check = pv_attr_str[\n",
    "#     pv_attr_str.index.get_level_values(2).str.contains('numeric value')].index.get_level_values(0)\n",
    "# # 먼저 추후 체크할 것들을 뽑아 놓고\n",
    "# attr_str_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pv_attr_str = pd.DataFrame(pv_attr_str[\n",
    "#     ~(pv_attr_str.index.get_level_values(0).isin(attr_str_to_check))\n",
    "#     &~(pv_attr_str.index.get_level_values(2) == 'unknown')\n",
    "#     ].to_records())\n",
    "\n",
    "# pv_attr_str = pd.pivot_table(\n",
    "#     pv_attr_str,\n",
    "#     index = ['Attribute', 'Desc'],\n",
    "#     values = 'Value',\n",
    "#     aggfunc = lambda x: list(x)\n",
    "#     )\n",
    "\n",
    "# view_all(pv_attr_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(not_in_attr_str.values.T.shape)\n",
    "# not_in_attr_str.values.T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + bin_interval, bin_interval)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 5.0\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 5.0 * iqr\n",
    "#     upper_threshold = q3 + 5.0 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(set([i for i in col_val if i < lower_threshold or i > upper_threshold]), reverse = True)\n",
    "#     for j in outliers:\n",
    "#         print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "#     print ('\\n', '==========' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     # bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + 10, 10)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 1.5\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 1.5 * iqr\n",
    "#     upper_threshold = q3 + 1.5 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(\n",
    "#         set([i for i in col_val if i < lower_threshold or i > upper_threshold]),\n",
    "#         reverse = True)\n",
    "#     # for j in outliers:\n",
    "#     #     print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "        \n",
    "#     outlier_list = [str(int(j)) + ': ' + '{:.1%}'.format((cc_num[i] == j).sum() / count_val)\n",
    "#                     for j in outliers]\n",
    "    \n",
    "#     print('Outliers (Value: %)')\n",
    "#     for j in range(0, len(outlier_list), 10):\n",
    "#         print (', '.join(outlier_list[j: j+10]))\n",
    "#     print ('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
