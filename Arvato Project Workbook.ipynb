{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import missingno as msno\n",
    "from googletrans import Translator\n",
    "\n",
    "# import sys\n",
    "# from IPython.display import display\n",
    "# import pprint\n",
    "# import itertools\n",
    "# import math\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "'''\n",
    "custom modules\n",
    "'''\n",
    "# function similar to Excel's vlookup\n",
    "from vlookup import vlookup\n",
    "from vlookuporg import vlookup_org\n",
    "# function to view all contents of a dataframe\n",
    "from view_all import view_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install googletrans==4.0.0-rc1\n",
    "# pip install missingno\n",
    "# custom modules for convenience are in root folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msno_overview (data, chunk_size = 100):\n",
    "    '''\n",
    "    function to display missing value using missingno library\n",
    "    data: dataframe\n",
    "    chunk_size: int, size of column chunk, 100 as default  \n",
    "    '''\n",
    "    # split data columns into chunks\n",
    "    chunk_size = chunk_size\n",
    "    column_chunks = [data.iloc[:, i : i + chunk_size] for i in range(0, data.shape[1], chunk_size)]\n",
    "\n",
    "    # generate and display missingno plots for each chunk\n",
    "    for i, j in enumerate(column_chunks):\n",
    "        msno.matrix(j, figsize = (10, 3), fontsize = 8, labels = False, sparkline = False)\n",
    "        plt.title(\n",
    "            f'{data.name}: missing value overview - column {i * 100} to {min (i * chunk_size + chunk_size - 1, data.shape[1] - 1)}',\n",
    "            fontsize = 10);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 azdias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file overview\n",
    "# '''\n",
    "\n",
    "# azdias = pd.read_csv('../csv_pickle/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "# azdias.name = 'azdias'\n",
    "#\n",
    "# print (azdias.info())\n",
    "# azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891221 entries, 0 to 891220\n",
      "Columns: 366 entries, LNR to ALTERSKATEGORIE_GROB\n",
      "dtypes: float64(267), int64(93), object(6)\n",
      "memory usage: 2.4+ GB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNR</th>\n",
       "      <th>AGER_TYP</th>\n",
       "      <th>AKT_DAT_KL</th>\n",
       "      <th>ALTER_HH</th>\n",
       "      <th>ALTER_KIND1</th>\n",
       "      <th>ALTER_KIND2</th>\n",
       "      <th>ALTER_KIND3</th>\n",
       "      <th>ALTER_KIND4</th>\n",
       "      <th>ALTERSKATEGORIE_FEIN</th>\n",
       "      <th>ANZ_HAUSHALTE_AKTIV</th>\n",
       "      <th>...</th>\n",
       "      <th>VHN</th>\n",
       "      <th>VK_DHT4A</th>\n",
       "      <th>VK_DISTANZ</th>\n",
       "      <th>VK_ZG11</th>\n",
       "      <th>W_KEIT_KIND_HH</th>\n",
       "      <th>WOHNDAUER_2008</th>\n",
       "      <th>WOHNLAGE</th>\n",
       "      <th>ZABEOTYP</th>\n",
       "      <th>ANREDE_KZ</th>\n",
       "      <th>ALTERSKATEGORIE_GROB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910215</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>910220</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>910225</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910226</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910241</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 366 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LNR  AGER_TYP  AKT_DAT_KL  ALTER_HH  ALTER_KIND1  ALTER_KIND2  \\\n",
       "0  910215        -1         NaN       NaN          NaN          NaN   \n",
       "1  910220        -1         9.0       0.0          NaN          NaN   \n",
       "2  910225        -1         9.0      17.0          NaN          NaN   \n",
       "3  910226         2         1.0      13.0          NaN          NaN   \n",
       "4  910241        -1         1.0      20.0          NaN          NaN   \n",
       "\n",
       "   ALTER_KIND3  ALTER_KIND4  ALTERSKATEGORIE_FEIN  ANZ_HAUSHALTE_AKTIV  ...  \\\n",
       "0          NaN          NaN                   NaN                  NaN  ...   \n",
       "1          NaN          NaN                  21.0                 11.0  ...   \n",
       "2          NaN          NaN                  17.0                 10.0  ...   \n",
       "3          NaN          NaN                  13.0                  1.0  ...   \n",
       "4          NaN          NaN                  14.0                  3.0  ...   \n",
       "\n",
       "   VHN  VK_DHT4A  VK_DISTANZ  VK_ZG11  W_KEIT_KIND_HH  WOHNDAUER_2008  \\\n",
       "0  NaN       NaN         NaN      NaN             NaN             NaN   \n",
       "1  4.0       8.0        11.0     10.0             3.0             9.0   \n",
       "2  2.0       9.0         9.0      6.0             3.0             9.0   \n",
       "3  0.0       7.0        10.0     11.0             NaN             9.0   \n",
       "4  2.0       3.0         5.0      4.0             2.0             9.0   \n",
       "\n",
       "   WOHNLAGE ZABEOTYP ANREDE_KZ ALTERSKATEGORIE_GROB  \n",
       "0       NaN        3         1                    2  \n",
       "1       4.0        5         2                    1  \n",
       "2       2.0        5         2                    3  \n",
       "3       7.0        3         2                    4  \n",
       "4       3.0        4         1                    3  \n",
       "\n",
       "[5 rows x 366 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azdias = pd.read_pickle('../csv_pickle/azdias.pickle')\n",
    "\n",
    "print (azdias.info())\n",
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# azdias.name = 'azdias'\n",
    "# msno_overview(azdias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file overview\n",
    "# '''\n",
    "\n",
    "# customers = pd.read_csv('../csv_pickle/Udacity_CUSTOMERS_052018.csv', sep=';')\n",
    "# print (customers.info())\n",
    "# customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 191652 entries, 0 to 191651\n",
      "Columns: 369 entries, LNR to ALTERSKATEGORIE_GROB\n",
      "dtypes: float64(267), int64(94), object(8)\n",
      "memory usage: 539.5+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNR</th>\n",
       "      <th>AGER_TYP</th>\n",
       "      <th>AKT_DAT_KL</th>\n",
       "      <th>ALTER_HH</th>\n",
       "      <th>ALTER_KIND1</th>\n",
       "      <th>ALTER_KIND2</th>\n",
       "      <th>ALTER_KIND3</th>\n",
       "      <th>ALTER_KIND4</th>\n",
       "      <th>ALTERSKATEGORIE_FEIN</th>\n",
       "      <th>ANZ_HAUSHALTE_AKTIV</th>\n",
       "      <th>...</th>\n",
       "      <th>VK_ZG11</th>\n",
       "      <th>W_KEIT_KIND_HH</th>\n",
       "      <th>WOHNDAUER_2008</th>\n",
       "      <th>WOHNLAGE</th>\n",
       "      <th>ZABEOTYP</th>\n",
       "      <th>PRODUCT_GROUP</th>\n",
       "      <th>CUSTOMER_GROUP</th>\n",
       "      <th>ONLINE_PURCHASE</th>\n",
       "      <th>ANREDE_KZ</th>\n",
       "      <th>ALTERSKATEGORIE_GROB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9626</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>COSMETIC_AND_FOOD</td>\n",
       "      <td>MULTI_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9628</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>SINGLE_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143872</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>COSMETIC_AND_FOOD</td>\n",
       "      <td>MULTI_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143873</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>COSMETIC</td>\n",
       "      <td>MULTI_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143874</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>MULTI_BUYER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LNR  AGER_TYP  AKT_DAT_KL  ALTER_HH  ALTER_KIND1  ALTER_KIND2  \\\n",
       "0    9626         2         1.0      10.0          NaN          NaN   \n",
       "1    9628        -1         9.0      11.0          NaN          NaN   \n",
       "2  143872        -1         1.0       6.0          NaN          NaN   \n",
       "3  143873         1         1.0       8.0          NaN          NaN   \n",
       "4  143874        -1         1.0      20.0          NaN          NaN   \n",
       "\n",
       "   ALTER_KIND3  ALTER_KIND4  ALTERSKATEGORIE_FEIN  ANZ_HAUSHALTE_AKTIV  ...  \\\n",
       "0          NaN          NaN                  10.0                  1.0  ...   \n",
       "1          NaN          NaN                   NaN                  NaN  ...   \n",
       "2          NaN          NaN                   0.0                  1.0  ...   \n",
       "3          NaN          NaN                   8.0                  0.0  ...   \n",
       "4          NaN          NaN                  14.0                  7.0  ...   \n",
       "\n",
       "   VK_ZG11  W_KEIT_KIND_HH  WOHNDAUER_2008  WOHNLAGE  ZABEOTYP  \\\n",
       "0      2.0             6.0             9.0       7.0         3   \n",
       "1      3.0             0.0             9.0       NaN         3   \n",
       "2     11.0             6.0             9.0       2.0         3   \n",
       "3      2.0             NaN             9.0       7.0         1   \n",
       "4      4.0             2.0             9.0       3.0         1   \n",
       "\n",
       "       PRODUCT_GROUP  CUSTOMER_GROUP ONLINE_PURCHASE ANREDE_KZ  \\\n",
       "0  COSMETIC_AND_FOOD     MULTI_BUYER               0         1   \n",
       "1               FOOD    SINGLE_BUYER               0         1   \n",
       "2  COSMETIC_AND_FOOD     MULTI_BUYER               0         2   \n",
       "3           COSMETIC     MULTI_BUYER               0         1   \n",
       "4               FOOD     MULTI_BUYER               0         1   \n",
       "\n",
       "  ALTERSKATEGORIE_GROB  \n",
       "0                    4  \n",
       "1                    4  \n",
       "2                    4  \n",
       "3                    4  \n",
       "4                    3  \n",
       "\n",
       "[5 rows x 369 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers = pd.read_pickle('../csv_pickle/customers.pickle')\n",
    "\n",
    "print (customers.info())\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# customers.name = 'customers'\n",
    "# msno_overview(customers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 313 entries, 0 to 312\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Information level  312 non-null    object\n",
      " 1   Attribute          313 non-null    object\n",
      " 2   Description        313 non-null    object\n",
      " 3   Additional notes   20 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 9.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Information level</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Additional notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>in cooperation with Kantar TNS; the informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Person</td>\n",
       "      <td>ALTERSKATEGORIE_GROB</td>\n",
       "      <td>age through prename analysis</td>\n",
       "      <td>modelled on millions of first name-age-referen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Person</td>\n",
       "      <td>ANREDE_KZ</td>\n",
       "      <td>gender</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Person</td>\n",
       "      <td>CJT_GESAMTTYP</td>\n",
       "      <td>Customer-Journey-Typology relating to the pref...</td>\n",
       "      <td>relating to the preferred information, marketi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Person</td>\n",
       "      <td>FINANZ_MINIMALIST</td>\n",
       "      <td>financial typology: low financial interest</td>\n",
       "      <td>Gfk-Typology based on a representative househo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Information level             Attribute  \\\n",
       "0               NaN              AGER_TYP   \n",
       "1            Person  ALTERSKATEGORIE_GROB   \n",
       "2            Person             ANREDE_KZ   \n",
       "3            Person         CJT_GESAMTTYP   \n",
       "4            Person     FINANZ_MINIMALIST   \n",
       "\n",
       "                                         Description  \\\n",
       "0                                 best-ager typology   \n",
       "1                      age through prename analysis    \n",
       "2                                             gender   \n",
       "3  Customer-Journey-Typology relating to the pref...   \n",
       "4         financial typology: low financial interest   \n",
       "\n",
       "                                    Additional notes  \n",
       "0  in cooperation with Kantar TNS; the informatio...  \n",
       "1  modelled on millions of first name-age-referen...  \n",
       "2                                                NaN  \n",
       "3  relating to the preferred information, marketi...  \n",
       "4  Gfk-Typology based on a representative househo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "info = pd.read_excel(\n",
    "    'DIAS Information Levels - Attributes 2017.xlsx', header=1).iloc[:, 1:]\n",
    "\n",
    "# fillna of Information level as values are only in the 1st rows\n",
    "info['Information level'] = info['Information level'].fillna(method = 'ffill')\n",
    "\n",
    "print (info.info())\n",
    "info.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2258 entries, 0 to 2257\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Attribute    314 non-null    object\n",
      " 1   Description  351 non-null    object\n",
      " 2   Score        2258 non-null   object\n",
      " 3   Meaning      2247 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 70.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9D</td>\n",
       "      <td>Mini-Jobber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9E</td>\n",
       "      <td>Socking Away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>CAMEO_DEUINTL_2015</td>\n",
       "      <td>CAMEO classification 2015 - international typo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(each German CAMEO code belongs to one interna...</td>\n",
       "      <td>11</td>\n",
       "      <td>Wealthy Households-Pre-Family Couples &amp; Singles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>Wealthy Households-Young Couples With Children</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Attribute                                        Description  \\\n",
       "103                 NaN                                                NaN   \n",
       "104                 NaN                                                NaN   \n",
       "105  CAMEO_DEUINTL_2015  CAMEO classification 2015 - international typo...   \n",
       "106                 NaN  (each German CAMEO code belongs to one interna...   \n",
       "107                 NaN                                                NaN   \n",
       "\n",
       "    Score                                          Meaning  \n",
       "103    9D                                      Mini-Jobber  \n",
       "104    9E                                     Socking Away  \n",
       "105    -1                                          unknown  \n",
       "106    11  Wealthy Households-Pre-Family Couples & Singles  \n",
       "107    12   Wealthy Households-Young Couples With Children  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "attr = pd.read_excel(\n",
    "    'DIAS Attributes - Values 2017.xlsx', header=1).iloc[:, 1:]\n",
    "# rename column Value to Score for easier documentation\n",
    "attr = attr.rename(columns = {'Value' : 'Score'})\n",
    "\n",
    "print (attr.info())\n",
    "attr[103:108]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Baseline understanding\n",
    "\n",
    "**2자리 넘버링으로 정리 되도록 함**\n",
    "\n",
    "There are 4 files given:\n",
    "* 2 dataset files - azdias & customers\n",
    "* 2 reference files - info & attr:  \n",
    "  - info is for information on dataset features which are in unreadable German acronyms  \n",
    "  - attr is for information on dataset values which are in numbers and acronyms,  \n",
    "  and has corresponding meanings  \n",
    "\n",
    "To establish a baseline, this section aims to identify key factors in the reference files   \n",
    "that can help in understanding the contents of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_num (number):\n",
    "    '''\n",
    "    function to change dtype of numbers in string form\n",
    "    '''\n",
    "    if isinstance(number, str) and (\n",
    "        number.isdigit() or (number.startswith('-') and number[1:].isdigit())):\n",
    "        return int(number)\n",
    "    else:\n",
    "        return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ger_to_eng (ger_text):\n",
    "    '''\n",
    "    function to translate German text\n",
    "    '''\n",
    "    translator = Translator(service_urls=['translate.google.com'])    \n",
    "    try:\n",
    "        translation = translator.translate(ger_text, src='de', dest='en')\n",
    "        return translation.text        \n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pv_min_max(data, pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes'], display = True):\n",
    "    '''\n",
    "    function to display min/max values of Score after adding information from feature_desc\n",
    "    \n",
    "    data : dataframe to examine\n",
    "    pv_idx: list of pivot_table index\n",
    "    display: boolean, selection of result display\n",
    "    '''\n",
    "    data = vlookup(data, feature_desc, 'Attribute', ['Desc', 'Additional notes'], fill_na = 'no_info')\n",
    "    pv = pd.pivot_table(\n",
    "        data,\n",
    "        index = pv_idx,\n",
    "        values = 'Score',\n",
    "        aggfunc = [min, max]\n",
    "        )\n",
    "        \n",
    "    pv['min_max_cat'] = pv['min'].astype(int).astype(str) + ' to ' + pv['max'].astype(int).astype(str)\n",
    "    pv = pv.sort_values(by = 'min_max_cat')\n",
    "    \n",
    "    pv = pd.DataFrame(pv.to_records())\n",
    "    pv.columns = list(pv.columns[:-3]) + list(eval(i)[0] for i in pv.columns[-3:])\n",
    "    print (f'number of Attribute(s): {data.Attribute.nunique()}')\n",
    "\n",
    "    if display:\n",
    "        print (pv.min_max_cat.unique())\n",
    "    \n",
    "    return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pv_meaning_score(\n",
    "    data, \n",
    "    pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes'],\n",
    "    pv_val = ['Meaning', 'Score'],\n",
    "    display = True\n",
    "    ):\n",
    "    '''\n",
    "    function to display the summary of Meaning and Score values\n",
    "    after adding information from feature_desc\n",
    "    \n",
    "    data : dataframe to examine\n",
    "    pv_idx: list of pivot_table index\n",
    "    pv_val: list of pivot_table value\n",
    "    display: boolean, selection of result display\n",
    "    '''\n",
    "    data = vlookup(data, feature_desc, 'Attribute', ['Desc', 'Additional notes'], fill_na = 'no_info')\n",
    "    pv = pd.pivot_table(\n",
    "        data,\n",
    "        index = pv_idx,\n",
    "        values = pv_val,\n",
    "        aggfunc = lambda x: list(x))\n",
    "    \n",
    "    pv = pd.DataFrame(pv.to_records())\n",
    "    print (f'number of Attribute(s): {data.Attribute.nunique()}')\n",
    "    \n",
    "    if display:\n",
    "        view_all(pv.iloc[:5])\n",
    "    \n",
    "    return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pv_verify_null(\n",
    "    data,\n",
    "    null_list,\n",
    "    pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes'], \n",
    "    pv_val = ['Meaning', 'Score'],    \n",
    "    display = True\n",
    "    ):\n",
    "    '''\n",
    "    function to display the summary of Meaning and Score values\n",
    "    when Meaning values are in the list containing possibly null values \n",
    "    \n",
    "    data: dataframe to examine\n",
    "    null_list: list of possibly null values\n",
    "    pv_idx: pivot_table index\n",
    "    pv_val: list of pivot_table value\n",
    "    display: boolean, selection of result display\n",
    "    '''\n",
    "    null_check_Attribute = data.query('Meaning in @null_list').Attribute.to_list()\n",
    "    null_check = data.query('Attribute in @null_check_Attribute')\n",
    "    \n",
    "    pv = pv_meaning_score(null_check, pv_idx, pv_val, display)\n",
    "    \n",
    "    return pv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 reference features from info\n",
    "\n",
    "As datasets do not have information on what each column feature exactly means,  \n",
    "values of reference files have to be mapped to the features.\n",
    "  \n",
    "feature_desc below is for this needs, and formed with 369 column features from customers. \n",
    "\n",
    "After merging information columns, there are 264 features from both customers and info,  \n",
    "along with 105 exclusive features of customers lacking Description values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 369 entries, 0 to 368\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   category_large     369 non-null    object\n",
      " 1   category_small     369 non-null    object\n",
      " 2   Attribute          369 non-null    object\n",
      " 3   Information level  263 non-null    object\n",
      " 4   Additional notes   17 non-null     object\n",
      " 5   Description        276 non-null    object\n",
      " 6   Desc               367 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 23.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_large</th>\n",
       "      <th>category_small</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Information level</th>\n",
       "      <th>Additional notes</th>\n",
       "      <th>Description</th>\n",
       "      <th>Desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_category</td>\n",
       "      <td>LNR</td>\n",
       "      <td>LNR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LNR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGER</td>\n",
       "      <td>AGER</td>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in cooperation with Kantar TNS; the informatio...</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>best-ager typology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AKT</td>\n",
       "      <td>AKT</td>\n",
       "      <td>AKT_DAT_KL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Act Dat KL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALTER</td>\n",
       "      <td>ALTER</td>\n",
       "      <td>ALTER_HH</td>\n",
       "      <td>Household</td>\n",
       "      <td>NaN</td>\n",
       "      <td>main age within the household</td>\n",
       "      <td>main age within the household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALTER</td>\n",
       "      <td>ALTER</td>\n",
       "      <td>ALTER_KIND1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Old child1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category_large category_small    Attribute Information level  \\\n",
       "0    no_category            LNR          LNR               NaN   \n",
       "1           AGER           AGER     AGER_TYP               NaN   \n",
       "2            AKT            AKT   AKT_DAT_KL               NaN   \n",
       "3          ALTER          ALTER     ALTER_HH         Household   \n",
       "4          ALTER          ALTER  ALTER_KIND1               NaN   \n",
       "\n",
       "                                    Additional notes  \\\n",
       "0                                                NaN   \n",
       "1  in cooperation with Kantar TNS; the informatio...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                     Description                           Desc  \n",
       "0                            NaN                            LNR  \n",
       "1             best-ager typology             best-ager typology  \n",
       "2                            NaN                     Act Dat KL  \n",
       "3  main age within the household  main age within the household  \n",
       "4                            NaN                     Old child1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "feature_desc with column features of customers and added information from info \n",
    "'''\n",
    "\n",
    "feature_desc = pd.DataFrame(customers.columns, columns=['Attribute'])\n",
    "feature_desc = vlookup(feature_desc, info, 'Attribute')\n",
    "\n",
    "'''\n",
    "To fill missing Descriptions, add some values from attr and create translation using googletrans.\n",
    "The result is saved in the root folder due to the time-consuming code execution.\n",
    "'''\n",
    "# # for Attribute without Description, add 12 Description values from attr\n",
    "# feature_desc = vlookup(feature_desc, attr, 'Attribute', 'Description', intsec = 'update')\n",
    "\n",
    "# # add category of Attribute\n",
    "# feature_desc['category_large'] = feature_desc['Attribute'].apply(lambda x: x.split('_')[0] if '_' in x else 'no_category')\n",
    "# feature_desc['category_small'] = np.where(feature_desc['category_large'] == 'no_category',\n",
    "#                                           feature_desc['Attribute'],\n",
    "#                                           feature_desc['category_large'])\n",
    "\n",
    "# # add translated information to Desc\n",
    "# feature_desc['eng_Desc'] = np.where(\n",
    "#     feature_desc.Description.isna(),\n",
    "#     feature_desc.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# feature_desc['Desc'] = feature_desc.Description.fillna('') + feature_desc.eng_Desc.fillna('')\n",
    "\n",
    "# # column order\n",
    "# feature_desc = feature_desc[[\n",
    "#     'category_large', 'category_small', 'Attribute', 'Information level', 'Additional notes', 'Description', 'Desc']]\n",
    "\n",
    "feature_desc = pd.read_excel('feature_desc.xlsx', index_col = [0])\n",
    "print(feature_desc.info())\n",
    "feature_desc.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 feature alignment\n",
    "\n",
    "The improved readability of feature_desc has enhanced the understanding  \n",
    "of the overall structure of the datasets and the meanings of the features,  \n",
    "although some translations still remain unclear.  \n",
    "\n",
    "Since there are a few features that have similar contents, it is necessary  \n",
    "to perform imputation steps to resolve collinearity and reduce dimensionality  \n",
    "before proceeding with the modeling process.\n",
    "\n",
    "The following cells provide an overview of the number of features in each file  \n",
    "and illustrates the feature intersection and difference between the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features by file\n",
      "azdias : 366\n",
      "customers : 369\n",
      "info : 313\n",
      "attr : 314\n",
      "feature_desc : 369\n"
     ]
    }
   ],
   "source": [
    "feature_dict = {\n",
    "    'azdias' : set(azdias.columns.unique()),\n",
    "    'customers' : set(customers.columns.unique()),\n",
    "    'info' : set(info.Attribute.dropna().unique()),\n",
    "    'attr' : set(attr.Attribute.dropna().unique()),\n",
    "    'feature_desc' : set(feature_desc.Attribute.dropna().unique())\n",
    "    }\n",
    "\n",
    "print ('Number of features by file')\n",
    "for k, v in feature_dict.items():\n",
    "    print (k, ':', len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature intersection & difference between files\n",
      "azdias & customers intersection:       366  /  in azdias & not-in customers:       0\n",
      "azdias & info intersection:            264  /  in azdias & not-in info:          102\n",
      "azdias & attr intersection:            272  /  in azdias & not-in attr:           94\n",
      "azdias & feature_desc intersection:    366  /  in azdias & not-in feature_desc:    0\n",
      "customers & azdias intersection:       366  /  in customers & not-in azdias:       3\n",
      "customers & info intersection:         264  /  in customers & not-in info:       105\n",
      "customers & attr intersection:         272  /  in customers & not-in attr:        97\n",
      "customers & feature_desc intersection: 369  /  in customers & not-in feature_desc: 0\n",
      "info & azdias intersection:            264  /  in info & not-in azdias:           49\n",
      "info & customers intersection:         264  /  in info & not-in customers:        49\n",
      "info & attr intersection:              300  /  in info & not-in attr:             13\n",
      "info & feature_desc intersection:      264  /  in info & not-in feature_desc:     49\n",
      "attr & azdias intersection:            272  /  in attr & not-in azdias:           42\n",
      "attr & customers intersection:         272  /  in attr & not-in customers:        42\n",
      "attr & info intersection:              300  /  in attr & not-in info:             14\n",
      "attr & feature_desc intersection:      272  /  in attr & not-in feature_desc:     42\n",
      "feature_desc & azdias intersection:    366  /  in feature_desc & not-in azdias:    3\n",
      "feature_desc & customers intersection: 369  /  in feature_desc & not-in customers: 0\n",
      "feature_desc & info intersection:      264  /  in feature_desc & not-in info:    105\n",
      "feature_desc & attr intersection:      272  /  in feature_desc & not-in attr:     97\n"
     ]
    }
   ],
   "source": [
    "print ('Feature intersection & difference between files')\n",
    "for i in range(0, 5):\n",
    "    for j in range (0, 5):\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        key_1 = list(feature_dict.keys())[i]\n",
    "        key_2 = list(feature_dict.keys())[j]\n",
    "        set_1 = list(feature_dict.values())[i]\n",
    "        set_2 = list(feature_dict.values())[j]\n",
    "        \n",
    "        intsec = set_1.intersection(set_2)\n",
    "        ft_diff = set_1 - set_2\n",
    "        feature_dict[f'{key_1}_intsec_{key_2}'] = intsec\n",
    "        feature_dict[f'in_{key_1}_notin_{key_2}'] = ft_diff\n",
    "        \n",
    "        intsec_print = f'{key_1} & {key_2} intersection:'\n",
    "        ft_diff_print = f' /  in {key_1} & not-in {key_2}:'        \n",
    "        print (intsec_print, str(len(intsec)).rjust(41 - len(intsec_print)),\n",
    "               ft_diff_print, str(len(ft_diff)).rjust(40 - len(ft_diff_print)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 reference values from attr\n",
    "\n",
    "After identifying the dataset structure in parts 2.1 and 2.2 above,  \n",
    "part 2.3 focuses on examining the dataset contents.  \n",
    "In this part, various aspects such as the ranges of feature values,  \n",
    "values actually meaning null, data types, and more are to be verified,  \n",
    "and preliminary notes will be recorded on pre-processing requirements.\n",
    "\n",
    "<!-- ##### 2.3.0 attr modification -->\n",
    "\n",
    "Considering that info contains information about dataset features, attr contains  \n",
    "information about dataset values and the values within the datasets can be understood  \n",
    "by mapping them to the corresponding Meaning values in attr.\n",
    "\n",
    "However, it is necessary to initially modify attr since it is not in a neat form:\n",
    "* 2 numbers in 1 str value like '-1, 0' or number(s) in str like '-1'\n",
    "* add-on information cell below Description value cell\n",
    "* only 1st rows of Attribute & Description with values, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* clean attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Attribute: 272\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>0</td>\n",
       "      <td>no classification possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>1</td>\n",
       "      <td>passive elderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>2</td>\n",
       "      <td>cultural elderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>3</td>\n",
       "      <td>experience-driven elderly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Attribute         Description Score                     Meaning\n",
       "0  AGER_TYP  best-ager typology    -1                     unknown\n",
       "1  AGER_TYP  best-ager typology     0  no classification possible\n",
       "2  AGER_TYP  best-ager typology     1             passive elderly\n",
       "3  AGER_TYP  best-ager typology     2            cultural elderly\n",
       "4  AGER_TYP  best-ager typology     3   experience-driven elderly"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep attr original\n",
    "attr_org = attr.copy()\n",
    "\n",
    "'''\n",
    "change numbers in str form\n",
    "'''\n",
    "# 2 numbers in 1 str value\n",
    "attr.Score = attr.Score.apply(\n",
    "    lambda x: [int(i) for i in x.split(',')] if isinstance(x, str) and ',' in x else x)\n",
    "attr = attr.explode('Score')\n",
    "# '-1' to -1\n",
    "attr.Score = attr.Score.apply(str_to_num)\n",
    "\n",
    "'''\n",
    "merge add-on information in some cells below Description\n",
    "'''\n",
    "# create attribute and description_shift with shift(-1)\n",
    "attr['attribute_shift'] = attr.Attribute.shift(-1).fillna('')\n",
    "attr['description_shift'] = attr.Description.shift(-1).fillna('')\n",
    "\n",
    "# merge Description and add-on\n",
    "attr.Description = attr.Description.mask(\n",
    "    (~(attr['Attribute'].isna()) & (attr['attribute_shift'].isna())),\n",
    "    attr.Description + ' ' + attr.description_shift)\n",
    "\n",
    "# replace add-on with np.nan\n",
    "attr.Description = attr.Description.mask(\n",
    "    (attr['Attribute'].isna()) & ~(attr['Description'].isna()),\n",
    "    np.nan)\n",
    "# drop description shift\n",
    "attr = attr.drop(['attribute_shift', 'description_shift'], axis = 1)\n",
    "\n",
    "'''\n",
    "fill blank cells with corresponding values as only 1st lines\n",
    "of Attribute & Description have values\n",
    "'''\n",
    "\n",
    "attr[['Attribute', 'Description']] = attr[\n",
    "    ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "'''\n",
    "keep 272 intersection features between attr and datasets,\n",
    "and drop exclusive features of datasets that are not useful at this step\n",
    "'''\n",
    "\n",
    "attr = attr[attr.Attribute.isin(feature_dict['attr_intsec_customers'])] # query not working\n",
    "\n",
    "print (f'number of Attribute: {attr.Attribute.nunique()}')\n",
    "attr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP_FAMILIE_GROB    6\n",
      "LP_STATUS_GROB     5\n",
      "Name: Attribute, dtype: int64\n",
      "            Attribute          Description Score                       Meaning\n",
      "1900  LP_FAMILIE_FEIN       familytyp fine     9                   shared flat\n",
      "1901  LP_FAMILIE_FEIN       familytyp fine    10    two-generational household\n",
      "1902  LP_FAMILIE_FEIN       familytyp fine    11  multi-generational household\n",
      "1911  LP_FAMILIE_GROB      familytyp rough     9         multiperson household\n",
      "1912  LP_FAMILIE_GROB      familytyp rough    10                           NaN\n",
      "1913  LP_FAMILIE_GROB      familytyp rough    11                           NaN\n",
      "1974   LP_STATUS_FEIN   social status fine     9                   houseowners\n",
      "1975   LP_STATUS_FEIN   social status fine    10                  top earners \n",
      "1984   LP_STATUS_GROB  social status rough     9                           NaN\n",
      "1985   LP_STATUS_GROB  social status rough    10                  top earners \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A small issue detected here is that there are 11 lines without value of Meaning.  \n",
    "From the examination below, 2 features involved with these lines can be dropped,  \n",
    "as they have other features with similar but more detailed Meaning values.\n",
    "\n",
    "2 features are without Meaning values\n",
    "'''\n",
    "\n",
    "print (attr.query('Meaning.isnull()').Attribute.value_counts())\n",
    "\n",
    "'''\n",
    "these features can be dropped, as they have similar features\n",
    "with more detailed Meaning values\n",
    "'''\n",
    "\n",
    "attribute_without_meaning = attr.query('Attribute.str.contains(\"LP_FAMILIE|LP_STATUS\")')\n",
    "print (attribute_without_meaning.query('Score > 8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE dict p_process to note points to be pre-processed\n",
    "'''\n",
    "\n",
    "p_process = {'2.3-0' : 'drop / LP_FAMILIE_GROB', \n",
    "             '2.3-1' : 'drop / LP_STATUS_GROB'}\n",
    "\n",
    "# remove 2 features from attr\n",
    "attr = attr.query('Attribute not in [\"LP_FAMILIE_GROB\", \"LP_STATUS_GROB\"]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create pre-study dataframes by value form of Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'int'>    1930\n",
       "<class 'str'>      53\n",
       "Name: Score_form, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "add delimiter column Score_form\n",
    "'''\n",
    "\n",
    "attr = attr.copy() # to avoid SettingWithCopyWarning\n",
    "attr['Score_form'] = attr.Score.map(type)\n",
    "attr.Score_form.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Attribute(s): 262\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "attr_num containing Scores denoted in number\n",
    "'''\n",
    "\n",
    "attr_num = attr[attr.Score_form == int].copy()\n",
    "print (f'number of Attribute(s): {attr_num.Attribute.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Attribute(s): 9\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "attr_str containing Scores denoted in string\n",
    "'''\n",
    "\n",
    "attr_str = attr[attr.Score_form == str].copy()\n",
    "print (f'number of Attribute(s): {attr_str.Attribute.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OST_WEST_KZ'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "there is 1 feature that Scores are in both numeric and string forms\n",
    "'''\n",
    "\n",
    "set(attr_num.Attribute.unique()).intersection(set(attr_str.Attribute.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 attr Scores denoted in number\n",
    "\n",
    "먼저 attr의 contents를 보면 대부분의 데이터는 categorical 하며, 좀 더 효과적으로 데이터의 Score를 살펴보기 위해 각 Attribute의 min/max 값을 카테고리화 해 본 결과 아래와 같이 나왔다. \n",
    "\n",
    "< 아래 내용은 나중에 생사 >  \n",
    "From min/max categories 를 만들었으며, to be further checked are:   \n",
    "* If Score -1 and 0 contain data equal to null or -inf  \n",
    "* If max Score values contain data equal to null or inf\n",
    "* If features with wider min/max gap are discrete or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Attribute(s): 262\n",
      "['-1 to -1' '-1 to 15' '-1 to 2' '-1 to 3' '-1 to 4' '-1 to 5' '-1 to 6'\n",
      " '-1 to 7' '-1 to 8' '-1 to 9' '0 to 1' '0 to 10' '0 to 21' '0 to 5'\n",
      " '0 to 6' '1 to 10' '1 to 11' '1 to 12' '1 to 40' '1 to 5' '1 to 6'\n",
      " '1 to 7' '1 to 9']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Desc</th>\n",
       "      <th>Additional notes</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>min_max_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OST_WEST_KZ</td>\n",
       "      <td>flag indicating the former GDR/FRG</td>\n",
       "      <td>flag indicating the former GDR/FRG</td>\n",
       "      <td>no_info</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1 to -1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRAEGENDE_JUGENDJAHRE</td>\n",
       "      <td>dominating movement in the person's youth (ava...</td>\n",
       "      <td>dominating movement in the person's youth (ava...</td>\n",
       "      <td>own typology modelled on different AZ DIAS data</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>-1 to 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VERS_TYP</td>\n",
       "      <td>insurance typology</td>\n",
       "      <td>insurance typology</td>\n",
       "      <td>in cooperation with TNS Infratest</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1 to 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANREDE_KZ</td>\n",
       "      <td>gender</td>\n",
       "      <td>gender</td>\n",
       "      <td>no_info</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1 to 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KBA05_ANTG4</td>\n",
       "      <td>number of &gt;10 family houses in the cell</td>\n",
       "      <td>number of &gt;10 family houses in the cell</td>\n",
       "      <td>no_info</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1 to 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Attribute                                        Description  \\\n",
       "0            OST_WEST_KZ                 flag indicating the former GDR/FRG   \n",
       "1  PRAEGENDE_JUGENDJAHRE  dominating movement in the person's youth (ava...   \n",
       "2               VERS_TYP                                insurance typology    \n",
       "3              ANREDE_KZ                                             gender   \n",
       "4            KBA05_ANTG4            number of >10 family houses in the cell   \n",
       "\n",
       "                                                Desc  \\\n",
       "0                 flag indicating the former GDR/FRG   \n",
       "1  dominating movement in the person's youth (ava...   \n",
       "2                                insurance typology    \n",
       "3                                             gender   \n",
       "4            number of >10 family houses in the cell   \n",
       "\n",
       "                                   Additional notes  min  max min_max_cat  \n",
       "0                                           no_info   -1   -1    -1 to -1  \n",
       "1  own typology modelled on different AZ DIAS data    -1   15    -1 to 15  \n",
       "2                 in cooperation with TNS Infratest   -1    2     -1 to 2  \n",
       "3                                           no_info   -1    2     -1 to 2  \n",
       "4                                           no_info   -1    2     -1 to 2  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "summary of attr_num with min/max value observed\n",
    "'''\n",
    "\n",
    "attr_num_score = pv_min_max(attr_num)\n",
    "attr_num_score.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최소 값 -1의 경우  \n",
    "  대부분의 Attr은 최소 값이 -1이며, 262개 숫자로 score 가 제시된 피쳐 중 85%(222개)를 차지한다. -1은 아래와 같은 언노운을 의미하므로 모두 널처리 한다.\n",
    "\n",
    "<아래 내용 나중 생사>  \n",
    "* **cases of Score -1 and 0**\n",
    "     \n",
    "  i) All Score value -1s in datasets should be replaced to null,  \n",
    "    as the reference of attr shows that it always means unknown.\n",
    "\n",
    "  ii) Few Score value 0s can be replaced to null if the corresponding  \n",
    "    value of Meaning is in list to_null.\n",
    "\n",
    "  iii) All other Score value 0s should be remained, as the reference  \n",
    "    of attr shows value 0 is not meaning null in nearly all cases.  \n",
    "\n",
    "  Analyses for this conclusion are in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown    222\n",
       "Name: Meaning, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "when Score value is -1, the meaning is 100% unknown in reference file\n",
    "'''\n",
    "\n",
    "attr_num.query('Score < 0').Meaning.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE pre-processing items\n",
    "'''\n",
    "\n",
    "p_process['2.3.1-0'] = 'replace / All / {-1: np.nan}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최소 값이 0이나 1인 경우도 40개가 존재 하는데, 동 최소값에는 다양한 값이 존재한다.  \n",
    "  2 리스트 처리 하였고, 0은 null로 갈 내용이 있으나 최소값 1에는 전혀 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown                                                                             312\n",
       "highest activity within the last 12 months                                           12\n",
       "no transactions known                                                                10\n",
       "no Online-transactions within the last 12 months                                      3\n",
       "single                                                                                1\n",
       "none                                                                                  1\n",
       "very high mobility                                                                    1\n",
       "typical low-income earners                                                            1\n",
       "single low-income- and average earners of younger age                                 1\n",
       "single low-income earners of younger age                                              1\n",
       "doesn't belong to the green avantgarde                                                1\n",
       "building is located in a 125 x 125m-grid cell (RA1), which is a consumption cell      1\n",
       "unknown / no main age detectable                                                      1\n",
       "Event travelers                                                                       1\n",
       "business cell                                                                         1\n",
       "Universal                                                                             1\n",
       "mainly 1-2 family homes                                                               1\n",
       "Name: Meaning, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Meaning values corresponding to max Scores\n",
    "'''\n",
    "# row indexes corresponding to max Scores of Attributes  \n",
    "attr_num['Score'] = attr_num.Score.astype(float)\n",
    "temp_min_idx = attr_num.groupby('Attribute')['Score'].idxmin()\n",
    "\n",
    "attr_num.loc[temp_min_idx]\n",
    "\n",
    "attr_num.loc[temp_min_idx].Meaning.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Score_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALTERSKATEGORIE_GROB</td>\n",
       "      <td>age classification through prename analysis</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALTERSKATEGORIE_GROB</td>\n",
       "      <td>age classification through prename analysis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ALTER_HH</td>\n",
       "      <td>main age within the household</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unknown / no main age detectable</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANREDE_KZ</td>\n",
       "      <td>gender</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>WOHNLAGE</td>\n",
       "      <td>residential-area</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>W_KEIT_KIND_HH</td>\n",
       "      <td>likelihood of a child present in this household</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>W_KEIT_KIND_HH</td>\n",
       "      <td>likelihood of a child present in this household</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>ZABEOTYP</td>\n",
       "      <td>typification of energy consumers</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>ZABEOTYP</td>\n",
       "      <td>typification of energy consumers</td>\n",
       "      <td>9.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;class 'int'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Attribute                                      Description  \\\n",
       "0                 AGER_TYP                               best-ager typology   \n",
       "5     ALTERSKATEGORIE_GROB     age classification through prename analysis    \n",
       "5     ALTERSKATEGORIE_GROB     age classification through prename analysis    \n",
       "11                ALTER_HH                    main age within the household   \n",
       "33               ANREDE_KZ                                           gender   \n",
       "...                    ...                                              ...   \n",
       "2229              WOHNLAGE                                residential-area    \n",
       "2244        W_KEIT_KIND_HH  likelihood of a child present in this household   \n",
       "2244        W_KEIT_KIND_HH  likelihood of a child present in this household   \n",
       "2251              ZABEOTYP                 typification of energy consumers   \n",
       "2251              ZABEOTYP                 typification of energy consumers   \n",
       "\n",
       "      Score                           Meaning     Score_form  \n",
       "0      -1.0                           unknown  <class 'int'>  \n",
       "5      -1.0                           unknown  <class 'int'>  \n",
       "5       0.0                           unknown  <class 'int'>  \n",
       "11      0.0  unknown / no main age detectable  <class 'int'>  \n",
       "33     -1.0                           unknown  <class 'int'>  \n",
       "...     ...                               ...            ...  \n",
       "2229   -1.0                           unknown  <class 'int'>  \n",
       "2244   -1.0                           unknown  <class 'int'>  \n",
       "2244    0.0                           unknown  <class 'int'>  \n",
       "2251   -1.0                           unknown  <class 'int'>  \n",
       "2251    9.0                           unknown  <class 'int'>  \n",
       "\n",
       "[350 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_num.loc[temp_min_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**중복이 생김. 에러일 수 있음 6월 7일** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attribute\n",
       "AGER_TYP                   0\n",
       "ALTERSKATEGORIE_GROB       5\n",
       "ALTER_HH                  11\n",
       "ANREDE_KZ                 33\n",
       "BALLRAUM                  40\n",
       "                        ... \n",
       "VERS_TYP                2216\n",
       "WOHNDAUER_2008          2219\n",
       "WOHNLAGE                2229\n",
       "W_KEIT_KIND_HH          2244\n",
       "ZABEOTYP                2251\n",
       "Name: Score, Length: 262, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_num.groupby('Attribute')['Score'].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.SeriesGroupBy object at 0x7fab49a832e0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_meaning_score(\n",
    "    data, \n",
    "    pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes'],\n",
    "    pv_val = ['Meaning', 'Score'],\n",
    "    display = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "최소값 0일 경우\n",
    "\n",
    "아래 내용은 나중에 생사\n",
    "when Score value is 0, it means none in most cases,\n",
    "and there are also some corresponding Meaning values\n",
    "that seem to be definitely equal to null\n",
    "'''\n",
    "\n",
    "# 먼저 최소값 -1 제외한 후 최소값 0 Meaning 점검\n",
    "temp_min_0 = attr_num_score[attr_num_score['min'] == 0].Attribute\n",
    "attr_num.query('Attribute in (@temp_min_0) and Score == 0').Meaning.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "먼저 확실하게 null을 의미하는 2개 Score는 to_null 리스트에 넣어 later, pre-processing에 사용하고, \n",
    "context에 따라 정확한 의미 파악이 필요한 나머지 값들은 maybe_null에 넣어 상세 점검을 함\n",
    "'''\n",
    "to_null = ['unknown', 'unknown / no main age detectable']\n",
    "maybe_null = ['no transactions known', 'none']\n",
    "\n",
    "'''\n",
    "2 메이비널 값을 컨텍스트 속에서 살펴보면\n",
    "- 'no transactions known'은 score 0일 때는 cover하는 의미가 있지만 (12달 간 there was no transaction)\n",
    "max일 때는 cover하는 의미가 없어 null이 더 타당해 보임\n",
    "- none은 null이 아니며 0을 의미함\n",
    "'''\n",
    "for i in maybe_null:\n",
    "    print (i)\n",
    "    verify_maybe_null = pv_verify_null(attr_num, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE pre-processing items\n",
    "'''\n",
    "\n",
    "p_process['2.3.1-1'] = 'mask / All / replace Score to null if corresponding to Meaning is in to_null'\n",
    "p_process['2.3.1-2'] = 'mask / All / replace max Score to null if corresponding to Meaning is \"no transactions known\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "최소값 0일 null로 바꿀 미닝 없음\n",
    "'''\n",
    "temp_min_1 = attr_num_score[attr_num_score['min'] == 1].Attribute\n",
    "attr_num.query('Attribute in (@temp_min_1) and Score == 1').Meaning.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위 query 에서 보면 작은 값이 highest category를 의미하는 경우도 있으므로 max 값에 null 여부도 점검해야 함 \n",
    "\n",
    "<생사 나중에>\n",
    "* **max Score values**  \n",
    "  \n",
    "  some Meaning values linked to max Scores seem to possibly be null.  \n",
    "  However most of them are not verified as null or already in list to_null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe_null = ['none', 'no transactions known', 'no classification possible',\n",
    "#               'classification not possible', 'no score calculated']\n",
    "\n",
    "# '''\n",
    "# check if none is also equal to null and should be added to to_nul:\n",
    "# none does not mean null in any case (see examples below).\n",
    "# '''\n",
    "# for i in maybe_null:\n",
    "#     print(i)\n",
    "#     verify_maybe_null = pv_verify_null(attr_num, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Meaning values corresponding to max Scores\n",
    "'''\n",
    "# row indexes corresponding to max Scores of Attributes  \n",
    "attr_num['Score'] = attr_num.Score.astype(float)\n",
    "max_idx = attr_num.groupby('Attribute')['Score'].idxmax()\n",
    "\n",
    "attr_num.loc[max_idx].Meaning.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "add possibly null_values from Meanings of max score\n",
    "'''\n",
    "maybe_null = ['Inactive', 'unremarkable', 'other']\n",
    "\n",
    "# # filter values already in to_null\n",
    "# maybe_null = [i for i in maybe_null if i not in to_null]\n",
    "# maybe_null\n",
    "\n",
    "'''\n",
    "check if Meaning values of maybe_null should be added to to_nul\n",
    "3 값 모두 명확하지 않으나 Inactive는 null로 보는 것이 의미나 outlying score 상 더 타당해 보임\n",
    "'''\n",
    "\n",
    "for i in maybe_null:\n",
    "    print (i)\n",
    "    verify_maybe_null = pv_verify_null(attr_num, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "form list maybe_null add only 'inactive' to to_null,\n",
    "as other values seems to have their own meanings\n",
    "'''\n",
    "\n",
    "to_null.append('Inactive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **data type of features with wider min/max Score gap**\n",
    "  \n",
    "  All features that have min/max Score gap over 10 are found to be discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check features that have min/max Score gap over 10\n",
    "'''\n",
    "# from attr_num_score above, extract Attribute values having min/max Score gap over 10\n",
    "gap_over10_attribute = attr_num_score[\n",
    "    (attr_num_score['max'] - attr_num_score['min']) > 10].Attribute.unique()\n",
    "\n",
    "gap_over10 = pv_meaning_score(\n",
    "    attr_num.query('Attribute in @gap_over10_attribute'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 attr Scores denoted in string\n",
    "\n",
    "///// 98 features in str type are analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "summary of attr_str with Meaning and Score values\n",
    "'''\n",
    "\n",
    "attr_str_score = pv_meaning_score(attr_str, display = False)\n",
    "view_all(attr_str_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **7 features are found as continuous.**  \n",
    "  These features will checked in detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list containing continuous features \n",
    "attr_continuous = list(\n",
    "    attr_str_score[attr_str_score.Score.apply(lambda x: x == ['…'])].Attribute.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset exploration\n",
    "\n",
    "Based on baseline understanding and frame works from reference files in section_2,  \n",
    "section_3 is dealing with actual datasets.  \n",
    "In this section, steps taken in the previous section will be repeated to fine-tune  \n",
    "and finalize pro-processing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create a concatenated dataframe of 2 datasets for temporary use\n",
    "'''\n",
    "\n",
    "concat_data = pd.concat([customers, azdias], axis=0)\n",
    "concat_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_summary (data,  score_form, display = True):\n",
    "    '''\n",
    "    function to create the summary table of given dataframe,\n",
    "    by applying describe and transpose methods\n",
    "    \n",
    "    data: dataframe\n",
    "    score_form : str / list, dtypes of columns to be selected\n",
    "    display: boolean, selection of result display\n",
    "    '''\n",
    "    # filter data    \n",
    "    data_copy = data.select_dtypes(include = score_form)\n",
    "    \n",
    "    # extract summary of data by applying describe and transpose\n",
    "    data_summary = data_copy.describe().T.reset_index()\n",
    "\n",
    "    # merge Desc (information on Attribute) from feature_desc and add min_max_cat\n",
    "    data_summary = data_summary.rename(columns = {'index' : 'Attribute'})\n",
    "    data_summary = vlookup(data_summary, feature_desc, 'Attribute', 'Desc')\n",
    "    # data_summary['null_portion'] = (1 - data_summary['count'] / data.shape[0]).map('{:.1%}'.format)    \n",
    "    data_summary['null_portion'] = 1 - data_summary['count'] / data.shape[0]\n",
    "    \n",
    "    # add min_max_cat incase score_form in numeric\n",
    "    if any(x in ['int', 'float'] for x in score_form):\n",
    "        min_max_display = True\n",
    "        data_summary['min_max_cat'] = data_summary[\n",
    "            'min'].apply(lambda x: '{:_.0f}'.format(x)).astype(str) + ' to ' + data_summary[\n",
    "            'max'].apply(lambda x: '{:_.0f}'.format(x)).astype(str)\n",
    "    else:\n",
    "        min_max_display = False\n",
    "    \n",
    "    if display:\n",
    "        print (f'number of Attribute(s): {data_summary.Attribute.nunique()}')\n",
    "        view_all(data_summary.iloc[:5])\n",
    "        \n",
    "        if min_max_display:\n",
    "            print ('min_max Scores:', '\\n', data_summary.min_max_cat.unique())\n",
    "    \n",
    "    return data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_feature (data, feature, view_all = True, view_0_10 = True):\n",
    "    '''\n",
    "    function to view and check continuous numeric data\n",
    "    feature : str, feature name\n",
    "    view_0_10 : boolean for histogram display of value 0 to 10, default as True\n",
    "    '''\n",
    "    # create data_stat using \n",
    "    data_stat = dataset_summary(data, ['int', 'float'], display = False)\n",
    "\n",
    "    min_val = data[feature].min()\n",
    "    max_val = data[feature].max()\n",
    "    bin_edges = np.arange(min_val, max_val + 10, 10)\n",
    "    desc_val = data_stat[data_stat['Attribute'] == feature]['Desc'].values[0]\n",
    "\n",
    "    if view_all:\n",
    "        ax = data[feature].plot(\n",
    "            kind = 'hist',\n",
    "            figsize=(10, 1.5),\n",
    "            color='gray',\n",
    "            bins = bin_edges,\n",
    "            align = 'mid',\n",
    "            title = (f'histogram - {desc_val} - {feature}')\n",
    "            );\n",
    "        ax.set_xlabel('Scores - Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "        plt.show()\n",
    "\n",
    "    if view_0_10:\n",
    "        ax = data[feature].plot(\n",
    "            kind = 'hist',\n",
    "            figsize=(10, 1.5),\n",
    "            color='gray',\n",
    "            bins = np.arange(-0.5, 11.5, 1),\n",
    "            align = 'mid',\n",
    "            title = (f'histogram - {desc_val} - {feature} - Score 0 to 10'),\n",
    "            );\n",
    "        ax.set_xlabel('Scores - Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "        plt.show()\n",
    "    \n",
    "    # define the outlier thresholds by applying multiplier 1.5\n",
    "    q1 = data_stat[data_stat['Attribute'] == feature]['25%'].values[0]\n",
    "    q3 = data_stat[data_stat['Attribute'] == feature]['75%'].values[0]\n",
    "    iqr = q3 - q1\n",
    "    lower_threshold = q1 - 1.5 * iqr\n",
    "    upper_threshold = q3 + 1.5 * iqr\n",
    "\n",
    "    # identify outliers\n",
    "    col_val = data[feature].values\n",
    "    outliers = sorted(\n",
    "        set([feature for feature in col_val if feature < lower_threshold or feature > upper_threshold]),\n",
    "        reverse = True)\n",
    "\n",
    "    # print outliers\n",
    "    count_val = data_stat[data_stat['Attribute'] == feature].fillna(0)['count'].values[0]       \n",
    "    outlier_list = [\n",
    "        str(int(j)) + ': ' + '{:.1%}'.format((data[feature] == j).sum() / count_val)\n",
    "        for j in outliers\n",
    "        ]\n",
    "    \n",
    "    print('10 outliers (Score: %)', '\\n',\n",
    "          ', '.join(outlier_list[:5]), '…', ', '.join(outlier_list[-5:]))\n",
    "\n",
    "    # for j in range(0, len(outlier_list), 10):\n",
    "    #     print (', '.join(outlier_list[j : j+10]))\n",
    "    \n",
    "    print ('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 intersection features in both of datasets and reference files\n",
    "///// For ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "concat_intsec containing intersection features in both of datasets and attr\n",
    "'''\n",
    "\n",
    "concat_intsec = concat_data[\n",
    "    list(feature_dict['customers_intsec_attr'])].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 numeric dataset Scores of intersection features\n",
    "\n",
    "///// For ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "intsec_num_score containing Scores denoted in number\n",
    "'''\n",
    "\n",
    "intsec_num_score = dataset_summary(concat_intsec, ['int', 'float'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check if Scores in actual datasets are within range given by attr.\n",
    "'''\n",
    "\n",
    "# merge min/max values by Attribute in attr_num_score in 2.3.1\n",
    "num_range_check = vlookup(\n",
    "    intsec_num_score, attr_num_score, 'Attribute', ['min', 'max', 'min_max_cat'], intsec = 'copy')\n",
    "\n",
    "num_range_check['check'] = np.where(\n",
    "    num_range_check['min_max_cat_'].isnull(), \n",
    "    'not in attr_num',\n",
    "    np.where(\n",
    "        (num_range_check['min'] < num_range_check['min_']),\n",
    "         'min lower',\n",
    "         np.where((num_range_check['max'] > num_range_check['max_']),\n",
    "         'max higher',\n",
    "         'within min/max range')))\n",
    "\n",
    "num_range_check.check.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* features out of reference Score range:  \n",
    "  뭐뭐뭐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3 features are showing lower min value in dataset \n",
    "'''\n",
    "num_range_check.query('check == \"min lower\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "as Meaning values of these 3 features in attr do not cover null value,\n",
    "value 0 should be replaced as null (see example below)\n",
    "'''\n",
    "\n",
    "attr.query('Attribute == \"LP_LEBENSPHASE_GROB\"').Meaning.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE pre-processing items\n",
    "'''\n",
    "\n",
    "for i in num_range_check.query('check == \"min lower\"').Attribute.unique():\n",
    "    p_process['3.1.1-'+ i] = 'replace / ' + i + ' / {0: np.nan}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "9 features are expected to be continuous,\n",
    "as Meaning values of continuous features have string form in attr. \n",
    "'''\n",
    "not_in_attr_num = num_range_check.query('check == \"not in attr_num\"')\n",
    "not_in_attr_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "after removing 2 features to be dropped (see 2.3)\n",
    "remaining 7 features are same as continuous features denoted in 2.3.2\n",
    "'''\n",
    "\n",
    "intsec_continuous = not_in_attr_num.query('Attribute != \"LP_FAMILIE_GROB\" and Attribute != \"LP_STATUS_GROB\"')\n",
    "set(intsec_continuous.Attribute.unique()) == set(attr_continuous)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* continuous features:   \n",
    "  뭐뭐뭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_PERSONEN  \n",
    "- Value 0 might be strange or extreme but is possible\n",
    "- outliers: household with over 10 adult persons is highly extreme or data error,\n",
    "  that Value over 10 should be replaced to NaN\n",
    "'''\n",
    "\n",
    "view_feature (concat_intsec, 'ANZ_PERSONEN', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.1.1-ANZ_PERSONEN'] = 'mask / ANZ_PERSONEN / Score > 10 to null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_HH_TITEL  \n",
    "- No pre-processing needed: max values might be strange or extreme but are possible\n",
    "'''\n",
    "\n",
    "view_feature (concat_intsec, 'ANZ_HH_TITEL', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "KBA13_ANZAHL_PKW\n",
    "- in this feature values over 1250 is grouped by 100. \n",
    "  Log scaling is needed due to high skewness.\n",
    "'''\n",
    "\n",
    "view_feature (concat_intsec, 'KBA13_ANZAHL_PKW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.1.1-KBA13_ANZAHL_PKW'] = 'log_scaling / KBA13_ANZAHL_PKW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_TITEL\n",
    "- it seems that there is no pre-processing needed.\n",
    "  However, this feature is linked to ANZ_PERSONEN above\n",
    "  that Score should be replaced to null if corresponding ANZ_PERSONEN is null.\n",
    "'''\n",
    "\n",
    "view_feature (concat_intsec, 'ANZ_TITEL', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.1.1-ANZ_TITEL'] = 'mask / ANZ_TITEL / Score to null if ANZ_PERSONEN is null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN_GEBAEUDEJAHR   \n",
    "- No pre-processing needed\n",
    "'''\n",
    "\n",
    "view_feature (concat_intsec, 'MIN_GEBAEUDEJAHR', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GEBURTSJAHR  \n",
    "- outliers: birth year can not be 0, that Value < 1900 should be replaced to null\n",
    "'''\n",
    "\n",
    "view_feature (concat_intsec, 'GEBURTSJAHR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.1.1-GEBURTSJAHR'] = 'mask / GEBURTSJAHR / Score < 1900 to null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_HAUSHALTE_AKTIV  \n",
    "- No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "- Log scaling is needed due to high skewness\n",
    "'''\n",
    "\n",
    "view_feature (concat_intsec, 'ANZ_HAUSHALTE_AKTIV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.1.1-ANZ_HAUSHALTE_AKTIV'] = 'log_scaling / HAUSHALTE_AKTIV'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 string dataset Scores of intersection features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 뭐뭐뭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_obj_score = dataset_summary(concat_intsec, ['object'], False)\n",
    "intsec_obj_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check if Scores in actual datasets are within range given by attr.\n",
    "'''\n",
    "\n",
    "# merge min/max values by Attribute in attr_num_score in 2.3.1\n",
    "for i in intsec_obj_score.Attribute.unique():\n",
    "    score_in_dataset = set(concat_intsec[i].unique())\n",
    "    score_in_attr = set(attr.query('Attribute == \"{}\"'.format(i)).Score.values)\n",
    "    \n",
    "    print (i)\n",
    "    print (f'a) scores in datasets: {score_in_dataset}')\n",
    "    print (f'b) scores in reference: {score_in_attr}')\n",
    "    print (f'a) b) intersection: {score_in_dataset.intersection(score_in_attr)}')\n",
    "    print (f'a) - b): {score_in_dataset - score_in_attr}')\n",
    "    print (f'b) - a): {score_in_attr - score_in_dataset}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scores in datasets 1 feature has numeric values\n",
    "intsec_discrete_str = intsec_obj_score.query('Attribute != \"CAMEO_DEUG_2015\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.1.2-CAMEO_DEU_2015'] = 'replace / CAMEO_DEU_2015 / {\"XX\": np.nan}'\n",
    "p_process['3.1.2-CAMEO_DEUG_2015'] = 'replace / CAMEO_DEUG_2015 / {\"X\": np.nan}'\n",
    "p_process['3.1.2'] = 'astype / All_numeric / change dtypes of numbers in string form'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 exclusive features NOT in reference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "concat_excl containing exclusive features of datasets\n",
    "'''\n",
    "\n",
    "concat_excl = concat_data[\n",
    "    list(feature_dict['in_customers_notin_attr'])].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 numeric dataset Scores of exclusive features\n",
    "\n",
    "뭐뭐뭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "excl_num_score containing Scores denoted in number\n",
    "'''\n",
    "\n",
    "excl_num_score = dataset_summary(concat_excl, ['int', 'float'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**왜 attr에 없는가... 가깝하나 없다는 것이 이 피쳐들을 설명하는 단초가 될 수 있음**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **data type of features with wider min/max Score gap**\n",
    "  \n",
    "min/max Score gap is not big. 아무 reference가 없으니 section_2에서 정리한 일반 원칙 적용하여 pre-processing\n",
    "\n",
    "All features that have min/max Score gap over 10 and null portion below 50% are to be checked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check features that have min/max Score gap 10 or below\n",
    "'''\n",
    "# from pv_min_max above, extract Attribute values\n",
    "# with min/max Score gap > 10 and null_portion < 0.5\n",
    "\n",
    "excl_num_score[\n",
    "    (excl_num_score['max'] - excl_num_score['min']) <= 10].Attribute.unique()\n",
    "\n",
    "# gap_10below_attribute = excl_num_score[\n",
    "#     (excl_num_score['max'] - excl_num_score['min']) <= 10].Attribute.unique()\n",
    "\n",
    "# gap_10below = excl_num_score.query(\n",
    "#     'Attribute.isin(@gap_10below_attribute) and null_portion < 0.5')\n",
    "# view_all(gap_10below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check features that have min/max Score gap over 10\n",
    "'''\n",
    "# from pv_min_max above, extract Attribute values\n",
    "# with min/max Score gap > 10 and null_portion < 0.5\n",
    "gap_over10_attribute = excl_num_score[\n",
    "    (excl_num_score['max'] - excl_num_score['min']) > 10].Attribute.unique()\n",
    "\n",
    "gap_over10 = excl_num_score.query(\n",
    "    'Attribute.isin(@gap_over10_attribute) and null_portion < 0.5')\n",
    "view_all(gap_over10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Desc, 2 features are categorical \n",
    "excl_continuous = gap_over10.query(\n",
    "    'Attribute != \"ALTERSKATEGORIE_FEIN\" and Attribute != \"GEMEINDETYP\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_STATISTISCHE_HAUSHALTE   \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "  - Log scaling is needed due to high skewness\n",
    "'''\n",
    "\n",
    "view_feature (concat_excl, 'ANZ_STATISTISCHE_HAUSHALTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.2.1-ANZ_HAUSHALTE_AKTIV'] = 'log_scaling / ANZ_HAUSHALTE_AKTIV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VK_DISTANZ  \n",
    "- No pre-processing needed\n",
    "'''\n",
    "\n",
    "view_feature (concat_excl, 'VK_DISTANZ', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "LNR is the serial index of dataset\n",
    "'''\n",
    "\n",
    "p_process['3.2.1-LNR'] = 'drop / LNR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ALTERSKATEGORIE_FEIN  \n",
    "- 0을 null로\n",
    "'''\n",
    "\n",
    "view_feature (concat_excl, 'ALTERSKATEGORIE_FEIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.2.1-ALTERSKATEGORIE_FEIN'] = 'replace / ALTERSKATEGORIE_FEIN / {0: np.nan}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GEMEINDETYP   \n",
    "- No pre-processing needed, 정확한 의미는 모르겠으나 극단적 특이성은 없음\n",
    "'''\n",
    "\n",
    "view_feature (concat_excl, 'GEMEINDETYP', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANZ_KINDER  \n",
    "  - 0을 null로 보면 대부분 값이 없으므로 Drop\n",
    "'''\n",
    "\n",
    "view_feature (concat_excl, 'ANZ_KINDER', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2.1-ANZ_KINDER'] = 'drop / ANZ_KINDER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "EINGEZOGENAM_HH_JAHR \n",
    "- outliers: Eng translation is not completely understandable, but 3 outliers can be replaced to null\n",
    "'''\n",
    "\n",
    "view_feature (concat_excl, 'EINGEZOGENAM_HH_JAHR', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2.1-EINGEZOGENAM_HH_JAHR'] = 'mask / EINGEZOGENAM_HH_JAHR / Value < 1980 to null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERDICHTUNGSRAUM   \n",
    "- No pre-processing needed although Eng translation not completely understandable\n",
    "'''\n",
    "\n",
    "view_feature (concat_excl, 'VERDICHTUNGSRAUM')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 string dataset Scores of exclusive features\n",
    "\n",
    "///// For ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "excl_obj_score containing Scores denoted in obj\n",
    "'''\n",
    "\n",
    "excl_obj_score = dataset_summary(concat_excl, ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check if abnormal Scores exist in actual datasets.\n",
    "'''\n",
    "# exclude high null_portion rows and EINGEFUEGT_AM\n",
    "excl_obj_not_date = excl_obj_score.query('Attribute != \"EINGEFUEGT_AM\"')\n",
    "\n",
    "# excl_obj_val = dict()\n",
    "for i in excl_obj_not_date.Attribute.unique():\n",
    "    \n",
    "    print (i)\n",
    "    print (f'scores in datasets: {set(concat_excl[i].unique())}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scores in datasets 1 feature has numeric values\n",
    "excl_discrete_str = excl_obj_score.query('Attribute != \"CAMEO_INTL_2015\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE\n",
    "'''\n",
    "\n",
    "p_process['3.2.2-CAMEO_INTL_2015'] = 'replace / CAMEO_INTL_2015 / {\"X\": np.nan}'\n",
    "p_process['3.2.2-EINGEFUEGT_AM'] = 'astype / EINGEFUEGT_AM / change dtype to datetime only with year values as there are too many unique values'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function] miss_val_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_val_summary(df, axis_val, x_bin = 2, bar_chart = True):\n",
    "    '''\n",
    "    function to display summary, bar-chart (optional) and histogram\n",
    "    of missing value by column or raw\n",
    "    df: dataframe\n",
    "    axis_val: str, one of 'column' or 'row'\n",
    "    x_bin: size of x bin, 10 as default\n",
    "    bar_chart: option of bar chart display \n",
    "    '''\n",
    "    # index of axis\n",
    "    axis_idx = 0 if axis_val == 'column' else 1\n",
    "    \n",
    "    # % of missing values\n",
    "    missing_pct = df.isnull().mean(axis = axis_idx) * 100\n",
    "    df_desc = missing_pct.describe()\n",
    "\n",
    "    # summary of missing value\n",
    "    print (\n",
    "        '% of missing value in ' + str(int(df_desc[0])) + ' ' + axis_val + 's of ' + df.name)\n",
    "    print (df_desc[1:].to_string())\n",
    "    \n",
    "    # bar-chart of missing value\n",
    "    if bar_chart:\n",
    "        missing_pct.plot(\n",
    "            kind = 'bar', figsize=(10, 3), color='gray',\n",
    "            \n",
    "            title = ('bar chart - ' + df.name + ': missing value by ' + axis_val),\n",
    "            ylabel = '% of missing value',\n",
    "            xlabel = (str(int(df_desc[0])) + ' columns'),\n",
    "            xticks = [],\n",
    "            );\n",
    "        plt.show()\n",
    "    \n",
    "    # hist of missing value\n",
    "    x_range = ((df_desc[-1] + 10) // 10) * 10 + x_bin\n",
    "    ax = missing_pct.plot(\n",
    "        kind = 'hist', figsize=(10, 3), color='gray',\n",
    "        \n",
    "        bins = np.arange(0, x_range, x_bin),\n",
    "        title = ('histogram - ' + df.name + ': missing value by ' + axis_val)\n",
    "        )\n",
    "    ax.set_xlabel('% of missing value');\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorize pre-processing items and create reference objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "categorize pre-processing items\n",
    "'''\n",
    "# transform p_process from dictionary to dataframe \n",
    "p_process = {'pre_processing_itmes' : p_process}\n",
    "p_process_items = pd.DataFrame(p_process).reset_index()\n",
    "p_process_items [['method', 'feature_dict']] = p_process_items[\n",
    "    'pre_processing_itmes'].str.split(' / ', n=1, expand=True)\n",
    "\n",
    "# categorize p_process_items\n",
    "p_process_all = p_process_items.query('feature_dict.str.contains(\"All\")')\n",
    "p_process_items = p_process_items.query('not feature_dict.str.contains(\"All\")')\n",
    "\n",
    "p_process_list = ['p_process_all']\n",
    "for i in p_process_items.method.unique():\n",
    "    globals()['p_process_' + i] = p_process_items.query('method == \"{}\"'.format(i))\n",
    "    globals()['p_process_' + i].name = 'p_process_' + i\n",
    "    p_process_list.append('p_process_' + i)\n",
    "\n",
    "p_process_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reference object(s) for p_process_all\n",
    "'''\n",
    "\n",
    "view_all(p_process_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature lists\n",
    "\n",
    "# continuous_features form 3.1.1 and 3.2.1\n",
    "continuous_features = list(intsec_continuous.Attribute.unique()) + list(excl_continuous.Attribute.unique())\n",
    "# continuous_features form 3.1.2 and 3.2.2\n",
    "discrete_str_features = list(intsec_discrete_str.Attribute.unique()) + list(excl_discrete_str.Attribute.unique())\n",
    "discrete_num_features_all = [\n",
    "    x for x in concat_data.columns if x not in (continuous_features + discrete_str_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mask\n",
    "'''\n",
    "\n",
    "to_null_mask = attr.query('Meaning in (@to_null)')[['Attribute', 'Score']].set_index('Attribute')\n",
    "to_null_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reference object(s) for p_process_drop\n",
    "'''\n",
    "\n",
    "view_all(p_process_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "missing value by column\n",
    "'''\n",
    "concat_data.name = 'concat_data'\n",
    "miss_val_summary(concat_data, 'column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (azdias.columns[azdias.isna().sum()/azdias.shape[0] > 0.3])\n",
    "print (customers.columns[customers.isna().sum()/customers.shape[0] > 0.3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-1이 없을 때 0이 unknown인 경우**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all(p_process_astype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "missing value by column\n",
    "'''\n",
    "customers.name = 'cistomers'\n",
    "miss_val_summary(customers, 'column')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work / Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def col_val (df):\n",
    "#     '''\n",
    "#     function to check values of dataframe columns\n",
    "#     df : dataframe\n",
    "#     '''\n",
    "#     # for i in df.columns:\n",
    "#     #     print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#     #         df[i].value_counts(), '\\n', '*     *     *')\n",
    "#     for i in df.columns:\n",
    "#         print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#         list(df[i].unique()), '\\n', '*     *     *')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from googletrans import Translator\n",
    "\n",
    "# # Create an instance of the Translator\n",
    "# translator = Translator(service_urls=['translate.google.com'])\n",
    "\n",
    "# # Text to be translated\n",
    "# text = \"AGER_TYP\"\n",
    "\n",
    "# # Translate the text from German to English\n",
    "# translation = translator.translate(text, src='de', dest='en')\n",
    "\n",
    "# # Print the translated text\n",
    "# print(\"Original text (German):\", text)\n",
    "# print(\"Translated text (English):\", translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def miss_val_hist(df, axis_val, x_bin = 10):\n",
    "#     '''\n",
    "#     function to display missing value histogram by column or raw\n",
    "#     df: dataframe\n",
    "#     axis_val: str, one of 'column' or 'row'\n",
    "#     x_bin: size of xtick bin, 10 as default \n",
    "#     '''\n",
    "#     # axis value\n",
    "#     axis_num = 0 if axis_val == 'column' else 1\n",
    "    \n",
    "#     # % of missing values\n",
    "#     missing_pct = df.isnull().mean(axis = axis_num) * 100\n",
    "\n",
    "#     # max % of missing values by column\n",
    "#     missing_pct_max = missing_pct.max()\n",
    "#     print ('max % of missing values by ' + axis_val + ': ', missing_pct_max)\n",
    "\n",
    "#     # plot missing values by column\n",
    "    \n",
    "#     print (missing_pct.describe())\n",
    "    \n",
    "#     x_range = ((missing_pct_max + x_bin * 2) // x_bin) * x_bin\n",
    "\n",
    "#     ax = missing_pct.plot(\n",
    "#         kind = 'hist', figsize=(10, 3), color='gray',\n",
    "#         bins = np.arange(0, x_range, 10),\n",
    "#         title = (df.name + ': missing value by ' + axis_val)\n",
    "#         )\n",
    "#     ax.set_xlabel('% of missing value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이써닉 하지 못한 코드\n",
    "# # missing value overview\n",
    "# for i in range(0, ((azdias.shape[1] + 100) // 100) * 100, 100):\n",
    "#     msno.matrix(azdias.iloc[:, i : i + 99],\n",
    "#                 figsize=(10, 3), fontsize = 12, labels = False, sparkline = False)\n",
    "#     plt.title('missing value overview: col ' + str (i) + ' to ' + str (min(i + 99, azdias.shape[1] - 1)),\n",
    "#               fontsize = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of over 30%\n",
    "# (azdias.isnull().mean() * 100 > 30).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of 25% to 30%\n",
    "# ((azdias.isnull().mean() * 100 > 25) & (30 >= azdias.isnull().mean() * 100)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아마도 쓰지 않을 plot\n",
    "# plot = azdias_col_missing_pct.plot(\n",
    "#     kind = 'bar', figsize=(10, 3), color='dimgray', xticks = [],\n",
    "#     title = 'azdias_col_missing_pct',\n",
    "#     xlabel = '366 columns',\n",
    "#     ylabel = '% of missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_df = customers.select_dtypes(include=['float', 'int64']).iloc[:, 1:]\n",
    "# num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pd.unique(customers.select_dtypes(include='float').values.flatten()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\n",
    "#     'display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(pd.DataFrame(attr.apply(lambda x: x.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_not_null = ~(attr.Attribute.isna())\n",
    "# attr.loc[attr_not_null, 'Description'] = attr.loc[\n",
    "#     attr_not_null, 'Description'] + ' ' + attr.loc[attr_not_null, 'desc_shift']\n",
    "\n",
    "# desc_to_null = (attr.Attribute.isna()) & ~(attr.Description.isna())\n",
    "# attr.loc[desc_to_null, 'Description'] = np.nan\n",
    "# attr = attr.drop(columns = 'desc_shift')\n",
    "# attr.loc[attr_with_value.shift(-1, fill_value = True), 'Description']\n",
    "# attr_shift = attr_null.shift\n",
    "# attr[attr_null.shift, 'Description'] = attr.loc[\n",
    "#     attr_null.shift(fill_value = False), 'Description'] + ' ' + attr[attr_null, 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# info_mg = info_mg.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "# # attr_excl = attr[attr.Attribute.isin(attr_excl)].copy()\n",
    "# attr_not_null = attr.dropna(subset = 'Attribute').copy()\n",
    "# attr_not_null = attr_not_null.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_not_null[['Attribute', 'Description']]],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_excl)][['Attribute', 'Description']].copy()\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# # info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_all(info_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 2258 rows of Attribute in attr, add 13 values exclusively in info,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# attr_mg = attr.copy()\n",
    "# info_excl = info[info.Attribute.isin(info_excl)][['Attribute', 'Description']].copy()\n",
    "# info_excl['Value'] = 'form info'\n",
    "# info_excl['Meaning'] = 'form info'\n",
    "\n",
    "# attr_mg = pd.concat(\n",
    "#     [attr_mg, info_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# # info_mg = info_mg.drop_duplicates()\n",
    "\n",
    "# # info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(attr_mg.info())\n",
    "# attr_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_mg.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# there are 93 and 51 exclusive values in data and information files\n",
    "# '''\n",
    "\n",
    "# datafile_Attr = azdias_Attr.union(customers_Attr)\n",
    "# infofile_Attr = info_Attr.union(attr_Attr)\n",
    "\n",
    "# datafile_excl = datafile_Attr - infofile_Attr\n",
    "# infofile_excl = infofile_Attr - datafile_Attr\n",
    "\n",
    "# print (len(datafile_excl), 'Attribute value(s) exclusively in data files:',\n",
    "#        '\\n', datafile_excl)\n",
    "# print (len(infofile_excl), 'Attribute value(s) exclusively in information files:',\n",
    "#        '\\n', infofile_excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# info['Information level'] = info['Information level'].fillna(method = 'ffill')\n",
    "\n",
    "# info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# attr[['Attribute', 'Description']] = attr[\n",
    "#     ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "# attr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# To enhance readability of data set - over 100 features are without description -   \n",
    "# I added 14 Description values from attr, and made a column of translation   \n",
    "# (ger_to_eng) to df_feature using code below.\n",
    "# However as this code-running takes somewhat long time, I saved the dataframe processed\n",
    "# as df_feature.xlsx in root folder\n",
    "# '''\n",
    "\n",
    "# # For values of Attribute without Description, add 14 Description values from attr\n",
    "# df_feature.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# df_feature.update(attr_excl)\n",
    "\n",
    "# df_feature.reset_index(inplace = True)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# df_feature['ger_to_eng'] = np.where(\n",
    "#     df_feature.Description.isnull(),\n",
    "#     df_feature.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# df_feature['Desc'] = df_feature.Description.fillna('') + df_feature.ger_to_eng.fillna('')\n",
    "\n",
    "# # # sort by Attribute and Information level\n",
    "# # df_feature.sort_values(by = ['Attribute', 'Information level'], inplace= True)\n",
    "\n",
    "# # df_feature = pd.read_excel('df_feature.xlsx', index_col = [0])\n",
    "# # df_feature.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# df_feature.Description = df_feature.Description.mask(\n",
    "#     df_feature.Attribute == attr_excl.Attribute,\n",
    "#     attr_excl.Description\n",
    "#     )\n",
    "# print ('Attributes missing Description:', df_feature[df_feature.Description.isna()].shape[0])\n",
    "# print (df_feature.shape)\n",
    "# df_feature.head()\n",
    "\n",
    "# ValueError: Can only compare identically-labeled Series objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_desc.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_excl = attr[\n",
    "#     attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# feature_desc.update(attr_excl)\n",
    "\n",
    "# feature_desc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between data files: \n",
    "# df customers has 3 more exclusive columns\n",
    "# '''\n",
    "\n",
    "# azdias_Attr = set(azdias.columns)\n",
    "# customers_Attr = set(customers.columns)\n",
    "\n",
    "# print(azdias_Attr - customers_Attr)\n",
    "# print(customers_Attr - azdias_Attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between information files\n",
    "# '''\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "\n",
    "# # info_excl = info_Attr - attr_Attr\n",
    "# # attr_excl = attr_Attr - info_Attr\n",
    "\n",
    "# print (len(info_Attr - attr_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - info_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - info_Attr)\n",
    "# '''\n",
    "# alignment of features between customers and information files\n",
    "# '''\n",
    "# print ('Attribute between customers and info')\n",
    "# print (len(customers_Attr - info_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - info_Attr)\n",
    "# print (len(info_Attr - customers_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - customers_Attr)\n",
    "# print ('In', len(info_Attr), 'features of info,', \n",
    "#        len(info_Attr) - len(info_Attr - customers_Attr), 'features are in Attribute of customers', '\\n')\n",
    "\n",
    "# print ('Attribute between customers and attr')\n",
    "# print (len(customers_Attr - attr_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - customers_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - customers_Attr)\n",
    "# print ('In', len(attr_Attr), 'features of attr,',\n",
    "#        len(attr_Attr) - len(attr_Attr - customers_Attr), 'features are in Attribute of customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(feature_dict.keys())[0]\n",
    "# feature_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = np.array([1, 2, 3, 4])\n",
    "\n",
    "# subtractions = np.subtract.outer(values, values)[np.triu_indices(len(values), k=1)]\n",
    "\n",
    "# for result in subtractions:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (k, v) in enumerate(feature_dict.items()):\n",
    "#     for j in range(i + 1, 4):\n",
    "#             result = values[i] - values[j]\n",
    "#             print(f\"{values[i]} - {values[j]} = {result}\")\n",
    "#     print (i, k, v)\n",
    "    \n",
    "# for i, (k, v) in enumerate(zip(list(feature_dict.keys()), list(feature_dict.values()))):\n",
    "#     print (i, (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# customers에는 없고 attr에만 있는 42개 Attribute는 Desc가 없으으로\n",
    "# (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# print ('customers에는 없고 attr에만 있는 42개 Attribute 수:', \n",
    "#        attr_num[attr_num.Desc.isna() == True].Attribute.nunique(),\n",
    "#        '\\n')\n",
    "\n",
    "# attr_num = attr_num[attr_num.Desc.isna() == False]\n",
    "# print (attr_num.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# str data of column Value\n",
    "# '''\n",
    "# # attr_num with only numeric values in Value\n",
    "# attr_str = attr[attr['Value_dtype'] == str].copy()\n",
    "# print (attr_str.shape) \n",
    "\n",
    "# # add Desc and Information level\n",
    "# attr_str = vlookup(attr_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "# print (attr_str.info())\n",
    "# attr_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 3개 null attribute는 정수와 문자를 모두 값을로 갖는 것들로 42개는 유효하고\n",
    "# 이 42개는 다음 section에서 볼 예정이므로 (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# # attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나...\n",
    "\n",
    "# attr_str = attr_str[attr_str.Desc.isna() == False]\n",
    "# print (attr_str.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attr_str_to_check = pv_attr_str[pv_attr_str.Meaning.str.contains('numeric value')].Attribute\n",
    "# attr_str_to_check = pv_attr_str[\n",
    "#     pv_attr_str.index.get_level_values(2).str.contains('numeric value')].index.get_level_values(0)\n",
    "# # 먼저 추후 체크할 것들을 뽑아 놓고\n",
    "# attr_str_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pv_attr_str = pd.DataFrame(pv_attr_str[\n",
    "#     ~(pv_attr_str.index.get_level_values(0).isin(attr_str_to_check))\n",
    "#     &~(pv_attr_str.index.get_level_values(2) == 'unknown')\n",
    "#     ].to_records())\n",
    "\n",
    "# pv_attr_str = pd.pivot_table(\n",
    "#     pv_attr_str,\n",
    "#     index = ['Attribute', 'Desc'],\n",
    "#     values = 'Value',\n",
    "#     aggfunc = lambda x: list(x)\n",
    "#     )\n",
    "\n",
    "# view_all(pv_attr_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(not_in_attr_str.values.T.shape)\n",
    "# not_in_attr_str.values.T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + bin_interval, bin_interval)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 5.0\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 5.0 * iqr\n",
    "#     upper_threshold = q3 + 5.0 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(set([i for i in col_val if i < lower_threshold or i > upper_threshold]), reverse = True)\n",
    "#     for j in outliers:\n",
    "#         print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "#     print ('\\n', '==========' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     # bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + 10, 10)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 1.5\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 1.5 * iqr\n",
    "#     upper_threshold = q3 + 1.5 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(\n",
    "#         set([i for i in col_val if i < lower_threshold or i > upper_threshold]),\n",
    "#         reverse = True)\n",
    "#     # for j in outliers:\n",
    "#     #     print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "        \n",
    "#     outlier_list = [str(int(j)) + ': ' + '{:.1%}'.format((cc_num[i] == j).sum() / count_val)\n",
    "#                     for j in outliers]\n",
    "    \n",
    "#     print('Outliers (Value: %)')\n",
    "#     for j in range(0, len(outlier_list), 10):\n",
    "#         print (', '.join(outlier_list[j: j+10]))\n",
    "#     print ('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** score_meaning_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_meaning_val(data, pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes']):\n",
    "#     '''\n",
    "#     function to check unique values of Score and Meaning by Attribute\n",
    "#     data : dataframe to examine\n",
    "#     pv_idx: list of pivot_table index\n",
    "#     '''\n",
    "#     data = vlookup(data, feature_desc, 'Attribute', ['Desc', 'Additional notes'], nan_val = 'no_info')\n",
    "#     pv = pd.pivot_table(\n",
    "#         data,\n",
    "#         index = pv_idx,\n",
    "#         values = ['Meaning', 'Score'],\n",
    "#         aggfunc = lambda x: list(x))\n",
    "    \n",
    "#     return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def verify_null(data, null_list, pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes']):\n",
    "#     '''\n",
    "#     function to display the summary of Score and Meaning values,\n",
    "#     when Meaning values are in the list containing possibly null values.   \n",
    "    \n",
    "#     data: dataframe to examine\n",
    "#     null_list: list of possibly null values\n",
    "#     pv_idx: pivot_table index\n",
    "#     '''\n",
    "#     null_check_Attribute = data[data['Meaning'].isin(null_list)]['Attribute'].to_list()\n",
    "#     null_check = data[data['Attribute'].isin(null_check_Attribute)]\n",
    "    \n",
    "#     null_check = vlookup(null_check, feature_desc, 'Attribute', ['Desc', 'Additional notes'], fill_na = 'no_info')\n",
    "#     pv = pd.pivot_table(\n",
    "#         null_check,\n",
    "#         index = pv_idx,\n",
    "#         values = ['Score', 'Meaning'],\n",
    "#         aggfunc = lambda x: list(x))\n",
    "    \n",
    "#     return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For values of Attribute without Description, add 12 Description values from attr\n",
    "# attr_to_add = attr[['Attribute', 'Description']].copy()\n",
    "# attr_to_add.dropna(inplace = True)\n",
    "# attr_to_add = attr_to_add.rename(columns = {'Description': 'Description_to_add'})\n",
    "\n",
    "# feature_desc = vlookup(feature_desc, attr_to_add, 'Attribute')\n",
    "# feature_desc.Description = np.where(\n",
    "#     ((feature_desc.Description.isnull() == True) & (feature_desc.Description_to_add.isnull() == False)),\n",
    "#     feature_desc.Description_to_add,\n",
    "#     feature_desc.Description)\n",
    "# feature_desc = feature_desc.drop('Description_to_add', axis=1)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# feature_desc['ger_to_eng'] = np.where(\n",
    "#     feature_desc.Description.isnull(),\n",
    "#     feature_desc.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# feature_desc['Desc'] = feature_desc.Description.fillna('') + feature_desc.ger_to_eng.fillna('')\n",
    "\n",
    "# feature_desc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pv_min_max 2.3.1 변경 for merge\n",
    "# min_max_copy = pd.DataFrame(pv_min_max.to_records())\n",
    "# min_max_copy.columns = list(\n",
    "#     min_max_copy.columns[:2]) + list(eval(i)[0] for i in min_max_copy.columns[2:])\n",
    "# min_max_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from IPython.display import display\n",
    "# import sys\n",
    "# from contextlib import ExitStack\n",
    "\n",
    "# def view_all (data):\n",
    "#     '''\n",
    "#     function to display all contents of data\n",
    "#     '''\n",
    "#     # with ExitStack() as stack:\n",
    "#     #     stack.enter_context(pd.option_context(\n",
    "#     #     'display.max_rows', None,\n",
    "#     #     'display.max_colwidth', None,\n",
    "#     #     'display.max_seq_items', None\n",
    "#     #     ))\n",
    "#     #     stack.enter_context(np.printoptions(threshold=np.inf))\n",
    "    \n",
    "#     # np.set_printoptions(threshold = sys.maxsize)\n",
    "    \n",
    "#     with pd.option_context(\n",
    "#     'display.max_rows', None, \n",
    "#     'display.max_colwidth', None,\n",
    "#     'display.max_seq_items', None,\n",
    "#     ):\n",
    "#         display(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
