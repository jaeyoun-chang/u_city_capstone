{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import missingno as msno\n",
    "from googletrans import Translator\n",
    "\n",
    "# import sys\n",
    "# from IPython.display import display\n",
    "# import pprint\n",
    "# import itertools\n",
    "# import math\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "'''\n",
    "custom modules\n",
    "'''\n",
    "# function similar to Excel's vlookup\n",
    "from vlookup import vlookup\n",
    "# function to view all contents of a dataframe\n",
    "from view_all import view_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install googletrans==4.0.0-rc1\n",
    "# pip install missingno\n",
    "# custom modules for convenience are in root folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 azdias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file overview\n",
    "# '''\n",
    "\n",
    "# azdias = pd.read_csv('../csv_pickle/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "# azdias.name = 'azdias'\n",
    "# print (azdias.info())\n",
    "# azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891221 entries, 0 to 891220\n",
      "Columns: 366 entries, LNR to ALTERSKATEGORIE_GROB\n",
      "dtypes: float64(267), int64(93), object(6)\n",
      "memory usage: 2.4+ GB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNR</th>\n",
       "      <th>AGER_TYP</th>\n",
       "      <th>AKT_DAT_KL</th>\n",
       "      <th>ALTER_HH</th>\n",
       "      <th>ALTER_KIND1</th>\n",
       "      <th>ALTER_KIND2</th>\n",
       "      <th>ALTER_KIND3</th>\n",
       "      <th>ALTER_KIND4</th>\n",
       "      <th>ALTERSKATEGORIE_FEIN</th>\n",
       "      <th>ANZ_HAUSHALTE_AKTIV</th>\n",
       "      <th>...</th>\n",
       "      <th>VHN</th>\n",
       "      <th>VK_DHT4A</th>\n",
       "      <th>VK_DISTANZ</th>\n",
       "      <th>VK_ZG11</th>\n",
       "      <th>W_KEIT_KIND_HH</th>\n",
       "      <th>WOHNDAUER_2008</th>\n",
       "      <th>WOHNLAGE</th>\n",
       "      <th>ZABEOTYP</th>\n",
       "      <th>ANREDE_KZ</th>\n",
       "      <th>ALTERSKATEGORIE_GROB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910215</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>910220</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>910225</td>\n",
       "      <td>-1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910226</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910241</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 366 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LNR  AGER_TYP  AKT_DAT_KL  ALTER_HH  ALTER_KIND1  ALTER_KIND2  \\\n",
       "0  910215        -1         NaN       NaN          NaN          NaN   \n",
       "1  910220        -1         9.0       0.0          NaN          NaN   \n",
       "2  910225        -1         9.0      17.0          NaN          NaN   \n",
       "3  910226         2         1.0      13.0          NaN          NaN   \n",
       "4  910241        -1         1.0      20.0          NaN          NaN   \n",
       "\n",
       "   ALTER_KIND3  ALTER_KIND4  ALTERSKATEGORIE_FEIN  ANZ_HAUSHALTE_AKTIV  ...  \\\n",
       "0          NaN          NaN                   NaN                  NaN  ...   \n",
       "1          NaN          NaN                  21.0                 11.0  ...   \n",
       "2          NaN          NaN                  17.0                 10.0  ...   \n",
       "3          NaN          NaN                  13.0                  1.0  ...   \n",
       "4          NaN          NaN                  14.0                  3.0  ...   \n",
       "\n",
       "   VHN  VK_DHT4A  VK_DISTANZ  VK_ZG11  W_KEIT_KIND_HH  WOHNDAUER_2008  \\\n",
       "0  NaN       NaN         NaN      NaN             NaN             NaN   \n",
       "1  4.0       8.0        11.0     10.0             3.0             9.0   \n",
       "2  2.0       9.0         9.0      6.0             3.0             9.0   \n",
       "3  0.0       7.0        10.0     11.0             NaN             9.0   \n",
       "4  2.0       3.0         5.0      4.0             2.0             9.0   \n",
       "\n",
       "   WOHNLAGE ZABEOTYP ANREDE_KZ ALTERSKATEGORIE_GROB  \n",
       "0       NaN        3         1                    2  \n",
       "1       4.0        5         2                    1  \n",
       "2       2.0        5         2                    3  \n",
       "3       7.0        3         2                    4  \n",
       "4       3.0        4         1                    3  \n",
       "\n",
       "[5 rows x 366 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azdias = pd.read_pickle('../csv_pickle/azdias.pickle')\n",
    "\n",
    "print (azdias.info())\n",
    "azdias.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** miss_val_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_val_overview (df, chunk_size = 100):\n",
    "    '''\n",
    "    function to display missing value using missingno library\n",
    "    df: dataframe\n",
    "    chunk_size: int, size of column chunk, 100 as default  \n",
    "    '''\n",
    "    # split df columns into chunks\n",
    "    chunk_size = chunk_size\n",
    "    column_chunks = [df.iloc[:, i : i + chunk_size] for i in range(0, df.shape[1], chunk_size)]\n",
    "\n",
    "    # generate and display missingno plots for each chunk\n",
    "    for i, j in enumerate(column_chunks):\n",
    "        msno.matrix(j, figsize = (10, 3), fontsize = 8, labels = False, sparkline = False)\n",
    "        plt.title(\n",
    "            f'{df.name}: missing value overview - column {i * 100} to {min (i * chunk_size + chunk_size - 1, df.shape[1] - 1)}',\n",
    "            fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# azdias.name = 'azdias'\n",
    "# miss_val_overview(azdias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file overview\n",
    "# '''\n",
    "\n",
    "# customers = pd.read_csv('../csv_pickle/Udacity_CUSTOMERS_052018.csv', sep=';')\n",
    "# print (customers.info())\n",
    "# customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_pickle('../csv_pickle/customers.pickle')\n",
    "\n",
    "print (customers.info())\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# customers.name = 'customers'\n",
    "# miss_val_overview(customers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "info = pd.read_excel(\n",
    "    'DIAS Information Levels - Attributes 2017.xlsx', header=1).iloc[:, 1:]\n",
    "\n",
    "print (info.info())\n",
    "info.head(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "attr = pd.read_excel(\n",
    "    'DIAS Attributes - Values 2017.xlsx', header=1).iloc[:, 1:]\n",
    "# rename column Value to Score for easier documentation\n",
    "attr = attr.rename(columns = {'Value' : 'Score'})\n",
    "\n",
    "print (attr.info())\n",
    "attr[103:108]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Baseline understanding\n",
    "\n",
    "**2자리 넘버링으로 정리 되도록 함**\n",
    "\n",
    "There are 4 files given:\n",
    "* 2 dataset files - azdias & customers\n",
    "* 2 reference files - info & attr:  \n",
    "  - info is for information on dataset features which are in unreadable German acronyms  \n",
    "  - attr is for information on dataset values which are in numbers and acronyms,  \n",
    "  and has corresponding meanings  \n",
    "\n",
    "To establish a baseline, this section aims to identify key factors in the reference files   \n",
    "that can help in understanding the contents of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 feature reference\n",
    "\n",
    "As datasets do not have information on what each column feature exactly means,  \n",
    "values of reference files have to be mapped to the features.\n",
    "  \n",
    "feature_desc below is for this needs, and formed with 369 column features from customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "feature_desc with column features of customers and added information from info \n",
    "'''\n",
    "\n",
    "feature_desc = pd.DataFrame(customers.columns, columns=['Attribute'])\n",
    "feature_desc = vlookup(feature_desc, info, 'Attribute')\n",
    "# custom function vlookup is similar to Excel's vlookup\n",
    "\n",
    "feature_desc.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging information columns, there are 264 features from both customers and info,  \n",
    "along with 105 exclusive features of customers lacking Description values.  \n",
    "\n",
    "To fill the 105 missing Descriptions, 12 values are added from the reference file attr,  \n",
    "and translation values are created using the googletrans module.  \n",
    "The resulting dataframe is saved in the root folder due to the time-consuming code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For values of Attribute without Description, add 12 Description values from attr\n",
    "# attr_to_add = attr[['Attribute', 'Description']].copy()\n",
    "# attr_to_add.dropna(inplace = True)\n",
    "# attr_to_add = attr_to_add.rename(columns = {'Description': 'Description_to_add'})\n",
    "\n",
    "# feature_desc = vlookup(feature_desc, attr_to_add, 'Attribute')\n",
    "# feature_desc.Description = np.where(\n",
    "#     ((feature_desc.Description.isnull() == True) & (feature_desc.Description_to_add.isnull() == False)),\n",
    "#     feature_desc.Description_to_add,\n",
    "#     feature_desc.Description)\n",
    "# feature_desc = feature_desc.drop('Description_to_add', axis=1)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# feature_desc['ger_to_eng'] = np.where(\n",
    "#     feature_desc.Description.isnull(),\n",
    "#     feature_desc.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# feature_desc['Desc'] = feature_desc.Description.fillna('') + feature_desc.ger_to_eng.fillna('')\n",
    "\n",
    "feature_desc = pd.read_excel('feature_desc.xlsx', index_col = [0])\n",
    "feature_desc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "understand all features one by one using custom function view_all\n",
    "\n",
    "not to run this cell, to save space in the final submission \n",
    "'''\n",
    "\n",
    "# view_all(feature_desc)\n",
    "\n",
    "feature_desc.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improved readability of feature_desc has enhanced the understanding  \n",
    "of the overall structure of the datasets and the meanings of the features,  \n",
    "although some translations still remain unclear.  \n",
    "\n",
    "Since there are a few features that have similar contents, it is necessary  \n",
    "to perform imputation steps to resolve collinearity and reduce dimensionality  \n",
    "before proceeding with the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_process below is a dict formed to record points to be pre-processed in the following sections.\n",
    "\n",
    "# '''\n",
    "# p_process dict to record points to be pre_processed\n",
    "# '''\n",
    "\n",
    "# p_process = {'2.1' : 'features of similar contents: collinearity resolution & dimensionality reduction'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 feature alignment\n",
    "\n",
    "The following cells provide an overview of the number of features in each file  \n",
    "and illustrates the feature intersection and difference between the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    'azdias' : set(azdias.columns.unique()),\n",
    "    'customers' : set(customers.columns.unique()),\n",
    "    'info' : set(info.Attribute.dropna().unique()),\n",
    "    'attr' : set(attr.Attribute.dropna().unique()),\n",
    "    'feature_desc' : set(feature_desc.Attribute.dropna().unique())\n",
    "    }\n",
    "\n",
    "print ('Number of features by file')\n",
    "for k, v in feature_dict.items():\n",
    "    print (k, ':', len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Feature intersection & difference between files')\n",
    "for i in range(0, 5):\n",
    "    for j in range (0, 5):\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        key_1 = list(feature_dict.keys())[i]\n",
    "        key_2 = list(feature_dict.keys())[j]\n",
    "        set_1 = list(feature_dict.values())[i]\n",
    "        set_2 = list(feature_dict.values())[j]\n",
    "        \n",
    "        intsec = set_1.intersection(set_2)\n",
    "        ft_diff = set_1 - set_2\n",
    "        feature_dict[f'{key_1}_intsec_{key_2}'] = intsec\n",
    "        feature_dict[f'in_{key_1}_notin_{key_2}'] = ft_diff\n",
    "        \n",
    "        intsec_print = f'{key_1} & {key_2} intersection:'\n",
    "        ft_diff_print = f' /  in {key_1} & not-in {key_2}:'        \n",
    "        print (intsec_print, str(len(intsec)).rjust(41 - len(intsec_print)),\n",
    "               ft_diff_print, str(len(ft_diff)).rjust(40 - len(ft_diff_print)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 dataset values\n",
    "\n",
    "After identifying the dataset structure in parts 2.1 and 2.2 above,  \n",
    "part 2.3 focuses on examining the dataset contents.  \n",
    "In this part, various aspects such as the ranges of feature values,  \n",
    "values actually meaning null, data types, and more are to be verified,  \n",
    "and preliminary notes will be recorded on pre-processing requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.0 attr modification\n",
    "\n",
    "Considering that info contains information about dataset features, attr contains  \n",
    "information about dataset values and the values within the datasets can be understood  \n",
    "by mapping them to the corresponding Meaning values in attr.\n",
    "\n",
    "However, it is necessary to initially modify attr since it is not in a neat form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "some Description cells have minor add-on information below\n",
    "which should be merged\n",
    "'''\n",
    " # keep original attr file\n",
    "attr_org = attr.copy()\n",
    "\n",
    "# merge minor information cells\n",
    "attr['description_shift'] = attr.Description.shift(-1).fillna('')\n",
    "attr.Description = attr.Description.mask(\n",
    "    ~(attr['Attribute'].isna()),\n",
    "    attr.Description + ' ' + attr.description_shift)\n",
    "attr.Description = attr.Description.mask(\n",
    "    (attr['Attribute'].isna()) & ~(attr['Description'].isna()),\n",
    "    np.nan)\n",
    "attr = attr.drop(columns = 'description_shift')\n",
    "\n",
    "'''\n",
    "fill null as only 1st lines of Attribute & Description have values\n",
    "'''\n",
    "attr[['Attribute', 'Description']] = attr[\n",
    "    ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "print (attr.info())\n",
    "attr.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modifying the attribute file to have well-organized data values,  \n",
    "it is filtered to retain only 272 intersection features with customers.  \n",
    "This filtering process involves dropping the exclusive features of attr  \n",
    "that are not useful for analyzing the datasets of customers and azdias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "extract 272 intersection features\n",
    "'''\n",
    "intsec_ft = feature_dict['attr_intsec_customers']\n",
    "attr = attr[attr.Attribute.isin(intsec_ft)]\n",
    "\n",
    "print (attr.Attribute.nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of modifying attr, column Score_form is added  \n",
    "to classify 272 features into 2 categories based on value forms in Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr['Score_form'] = attr.Score.map(type)\n",
    "attr.Score_form.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 features with numeric Score values\n",
    "\n",
    "Out of 272 features, 264 features are extracted with the condition  \n",
    "that values within column Score are of numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "attr_num containing Score values in numeric form\n",
    "'''\n",
    "\n",
    "attr_num = attr[attr.Score_form == int]\n",
    "attr_num.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Score values lacking Meaning values**\n",
    "    \n",
    "  A small issue detected here is that there are 11 lines without value of Meaning.  \n",
    "  From the examination below, 2 features involved with these lines can be dropped,  \n",
    "  as they have other features with similar but more detailed Meaning values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2 features involved with lines without Meaning values\n",
    "'''\n",
    "attr_num[attr_num.Meaning.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "these 2 features can be dropped, as they have other features\n",
    "with similar but more detailed Meaning value\n",
    "'''\n",
    "lp_gros_fine = attr[attr.Attribute.str.contains('LP_FAMILIE') | attr.Attribute.str.contains('LP_STATUS')]\n",
    "lp_gros_fine[lp_gros_fine['Score'] > 8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Note]** p_process below is a dict formed to note points to be pre-processed in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process = {'2.3.1' : 'drop features LP_FAMILIE_GROB / LP_STATUS_GROB'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping 2 features, remaining features are further organized into groups  \n",
    "based on min/max values observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "drop 2 features from attr_num\n",
    "'''\n",
    "attr_num = attr_num[(~attr_num['Attribute'].isin(['LP_FAMILIE_GROB', 'LP_STATUS_GROB']))]\n",
    "\n",
    "'''\n",
    "pivot table summary of attr_num with min/max value categories\n",
    "'''\n",
    "print (f'{attr_num.Attribute.nunique()} features grouped by min/max values')\n",
    "pv_min_max = pd.pivot_table(\n",
    "    attr_num,\n",
    "    index = ['Attribute', 'Description'],\n",
    "    values = 'Score',\n",
    "    aggfunc = [min, max]\n",
    "    )\n",
    "pv_min_max['min_max_cat'] = pv_min_max['min'].astype(int).astype(str) + ' to ' + pv_min_max['max'].astype(int).astype(str)\n",
    "pv_min_max = pv_min_max.sort_values(by = 'min_max_cat')\n",
    "\n",
    "pv_min_max.min_max_cat.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From min/max categories above, to be further checked are:   \n",
    "* If Score -1 and 0 contain data equal to null or -inf  \n",
    "* If max Score values contain data equal to null or inf\n",
    "* If features with wider min/max gap are discrete or continuous"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Cases of Score -1 and 0**\n",
    "     \n",
    "  i) All Score value -1s in datasets should be replaced to null,  \n",
    "    as the reference of attr shows that it always means unknown.\n",
    "\n",
    "  ii) Few Score value 0s can be replaced to null if the corresponding  \n",
    "    value of Meaning is in list to_null.\n",
    "\n",
    "  iii) All other Score value 0s should be remained, as the reference  \n",
    "    of attr shows value 0 is not meaning null in nearly all cases.  \n",
    "\n",
    "  Analyses for this conclusion are in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "when Score value is -1, the meaning is 100% unknown in reference file\n",
    "'''\n",
    "\n",
    "attr_num_mn1 = attr_num[attr_num.Score == -1]\n",
    "attr_num_mn1.Meaning.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "when Score value is 0, it means none in most occasion,\n",
    "and there are few cases observed that the corresponding value\n",
    "of Meaning is definitely equal to null\n",
    "'''\n",
    "\n",
    "attr_num_0 = attr_num[attr_num.Score == 0]\n",
    "attr_num_0.Meaning.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** verify_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_null(data, null_list, pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes']):\n",
    "    '''\n",
    "    function to display the summary of Score and Meaning values,\n",
    "    when Meaning values are in the list containing possibly null values.   \n",
    "    \n",
    "    data: dataframe to examine\n",
    "    null_list: list of possibly null values\n",
    "    pv_idx: pivot_table index\n",
    "    '''\n",
    "    null_check_Attribute = data[data['Meaning'].isin(null_list)]['Attribute'].to_list()\n",
    "    null_check = data[data['Attribute'].isin(null_check_Attribute)]\n",
    "    \n",
    "    null_check = vlookup(null_check, feature_desc, 'Attribute', ['Desc', 'Additional notes'], nan_val = 'no_info')\n",
    "    pv = pd.pivot_table(\n",
    "        null_check,\n",
    "        index = pv_idx,\n",
    "        values = ['Score', 'Meaning'],\n",
    "        aggfunc = lambda x: list(x))\n",
    "    \n",
    "    return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "select Meaning values which seems to be definitely null,\n",
    "and verify them\n",
    "'''\n",
    "to_null = ['unknown','no transactions known', 'no classification possible',\n",
    "           'unknown / no main age detectable', 'classification not possible',\n",
    "           'no score calculated'\n",
    "            ]\n",
    "\n",
    "verify_to_null = verify_null(attr_num, to_null)\n",
    "view_all(verify_to_null.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_null = ['none']\n",
    "\n",
    "'''\n",
    "check if none is also equal to null and should be added to to_nul:\n",
    "none does not mean null in any case (see examples below).\n",
    "'''\n",
    "\n",
    "verify_maybe_null = verify_null(attr_num, maybe_null)\n",
    "view_all(verify_maybe_null.iloc[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Note]** p_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['2.3.1-value_-1'] = 'All value -1s in datasets to be null'\n",
    "p_process['2.3.1-value_0'] = 'Value 0s to be null if corresponding Meaning value is in to_null'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **max Score values**  \n",
    "  \n",
    "  some Meaning values linked to max Scores seem to possibly be null.  \n",
    "  However most of them are not verified as null or already in list to_null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "extract Meaning values corresponding to max Score values\n",
    "'''\n",
    "attr_num['Score'] = attr_num['Score'].astype(float)\n",
    "max_idx = attr_num.groupby('Attribute')['Score'].idxmax()\n",
    "\n",
    "attr_num_max = attr_num.loc[max_idx]\n",
    "attr_num_max.Meaning.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "select Meaning values to be checked\n",
    "'''\n",
    "maybe_null = ['uniformly distributed', 'no transactions known', 'Inactive', 'unremarkable',\n",
    "              'unknown', 'other', 'indifferent']\n",
    "\n",
    "maybe_null = [i for i in maybe_null if i not in to_null]\n",
    "maybe_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check if Meaning values of maybe_null should be added to to_nul\n",
    "'''\n",
    "\n",
    "verify_maybe_null = verify_null(attr_num, maybe_null)\n",
    "view_all(verify_maybe_null.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "form list maybe_null only inactive can be regarded as null\n",
    "'''\n",
    "\n",
    "to_null.append('Inactive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data type of features with wider min/max Score gap**\n",
    "  \n",
    "  All features that have min/max Score gap over 7 are found to be discrete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** score_meaning_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meaning_score_val(data, pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes']):\n",
    "    '''\n",
    "    function to display the summary of Score and Meaning values\n",
    "    data : dataframe to examine\n",
    "    pv_idx: list of pivot_table index\n",
    "    '''\n",
    "    data = vlookup(data, feature_desc, 'Attribute', ['Desc', 'Additional notes'], nan_val = 'no_info')\n",
    "    pv = pd.pivot_table(\n",
    "        data,\n",
    "        index = pv_idx,\n",
    "        values = ['Meaning', 'Score'],\n",
    "        aggfunc = lambda x: list(x))\n",
    "    \n",
    "    return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All features that have min/max Score gap over 7 are found to be discrete\n",
    "'''\n",
    "# from pv_min_max above, extract Attribute values having Score values over 7\n",
    "max_over_7_Attribute = pv_min_max[pv_min_max[('max', 'Score')] > 7].index.get_level_values(0)\n",
    "max_over_7 = attr_num[attr_num['Attribute'].isin(max_over_7_Attribute)]\n",
    "\n",
    "pv_max_over_7 = meaning_score_val(max_over_7)\n",
    "view_all(pv_max_over_7.iloc[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 str values\n",
    "\n",
    "///// 98 features in str type are analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_str = attr[attr.Score_form == str]\n",
    "\n",
    "print ('number of features in str type:', attr_str.Attribute.nunique())\n",
    "print ('number of intersection features in int & str type:',\n",
    "       len(set(attr_str.Attribute.unique()).intersection(set(attr_num.Attribute.unique()))), '\\n')\n",
    "\n",
    "pv_attr_str = meaning_score_val(attr_str)\n",
    "view_all(pv_attr_str.iloc[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As there are cases that 2 numbers paired are meaning 'unknown',  \n",
    "the summary table is remade after deleting these lines.\n",
    "'''\n",
    "\n",
    "attr_str = attr_str[attr_str.Meaning != 'unknown']\n",
    "\n",
    "pv_attr_str = meaning_score_val(attr_str)\n",
    "view_all(pv_attr_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observation above:\n",
    "* 2 features are discrete - CAMEO_DEU_2015 and OST_WEST_KZ\n",
    "* 7 features continuous that skewness of these features should be checked for scaling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Note]** p_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_feature = attr_str[attr_str['Meaning'].str.contains('numeric value')]\n",
    "cont_ft = list(cont_feature['Attribute'].unique())\n",
    "\n",
    "p_process['2.3.2'] = 'check skewness for scaling of ' + ', '.join(cont_ft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset exploration\n",
    "\n",
    "Based on baseline understanding and frame works from reference files in section_2,  \n",
    "section_3 is dealing with actual datasets.  \n",
    "In this section, steps taken in the previous section will be repeated to fine-tune  \n",
    "and finalize pro-processing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create a concatenated dataframe of 2 datasets for temporary use\n",
    "'''\n",
    "\n",
    "concat_data = pd.concat([customers, azdias], axis=0)\n",
    "concat_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 intsec features\n",
    "\n",
    "///// For ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_col = list(feature_dict['customers_intsec_attr'])\n",
    "\n",
    "concat_intsec = concat_data[intsec_col]\n",
    "print (concat_intsec.info())\n",
    "concat_intsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_summary (data,  score_form, include = True):\n",
    "    '''\n",
    "    aaa\n",
    "    '''\n",
    "    # filter and copy data\n",
    "    if include:\n",
    "        data_copy = data.select_dtypes(include = score_form).copy()\n",
    "    else:\n",
    "        data_copy = data.select_dtypes(exclude = score_form).copy()\n",
    "    \n",
    "    # extract summary of data by applying describe and transpose\n",
    "    data_summary = data_copy.describe().T.reset_index()\n",
    "\n",
    "    # merge Desc (information on Attribute) from feature_desc and add min_max_cat\n",
    "    data_summary = data_summary.rename(columns = {'index' : 'Attribute'})\n",
    "    data_summary = vlookup(data_summary, feature_desc, 'Attribute', 'Desc')\n",
    "    \n",
    "    # print(data_summary.shape)\n",
    "    # print(data_summary.head())\n",
    "    \n",
    "    return data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_num = df_summary(concat_intsec, ['int', 'float'])\n",
    "intsec_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_num['min_max_cat'] = intsec_num[\n",
    "    'min'].apply(lambda x: '{:_.0f}'.format(x)).astype(str) + ' to ' + intsec_num[\n",
    "    'max'].apply(lambda x: '{:_.0f}'.format(x)).astype(str)\n",
    "\n",
    "intsec_num.min_max_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pv_min_max 2.3.1 변경 for merge\n",
    "min_max_copy = pd.DataFrame(pv_min_max.to_records())\n",
    "min_max_copy.columns = list(\n",
    "    min_max_copy.columns[:2]) + list(eval(i)[0] for i in min_max_copy.columns[2:])\n",
    "min_max_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_num_check = intsec_num.merge(\n",
    "    min_max_copy.drop('Description', axis = 1),\n",
    "    on = 'Attribute',\n",
    "    how = 'left',\n",
    "    suffixes = ('_attr', '_customer')\n",
    ")\n",
    "intsec_num_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_num_check['check'] = np.where(\n",
    "    intsec_num_check['min_max_cat_customer'].isnull(), \n",
    "    'no min/max',\n",
    "    np.where(\n",
    "        (intsec_num_check['min_customer'] < intsec_num_check['min_attr']),\n",
    "         'min lower',\n",
    "         np.where((intsec_num_check['max_customer'] > intsec_num_check['max_attr']),\n",
    "         'max_higher',\n",
    "         'within min/max range')))\n",
    "\n",
    "intsec_num_check['check'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "within min/max range는 원래 아는데로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "min_lower\n",
    "'''\n",
    "intsec_num_check[intsec_num_check['check'] == 'min_lower']['min_customer'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 아는데로 -1을 null 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "no min/max\n",
    "'''\n",
    "intsec_no_min_max = intsec_num_check[intsec_num_check['check'] == 'no min/max']\n",
    "intsec_no_min_max\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2개는 지우기로 한 것이고,\n",
    "나머지는 str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_no_min_max = intsec_no_min_max.query('Attribute != \"LP_FAMILIE_GROB\" and Attribute != \"LP_STATUS_GROB\"')\n",
    "set(intsec_no_min_max.Attribute.unique()) - set(attr_str.Attribute.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동 features는 따로 살펴봄:   \n",
    "여기에 아래 그래프 삽입"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///// From min/max values\n",
    "* min Value -1 should be replaced to NaN - noted in 2.3.1\n",
    "* Most features has max values not exceeding 40 and can be regarded as discrete - checked in 2.3.3  \n",
    "  Features with max values over 40 will be further analyzed below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_stat.min_max_cat.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.2 detailed feature check: numeric features with max over 40 & continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract features with max values over 40 from concat_stat\n",
    "# concat_high_max = concat_stat[concat_stat['max'] > 40][['Attribute', 'Desc']]\n",
    "# ft_to_check = list(concat_high_max.Attribute.unique())\n",
    "\n",
    "# # compare list of features of high max values with list of continuos features\n",
    "# # in section_2 - cont_ft in 2.3.2 \n",
    "# set (cont_ft) - set(ft_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_to_check.extend(['ANZ_HH_TITEL', 'ANZ_TITEL'])\n",
    "\n",
    "# view_all(concat_stat[concat_stat['Attribute'].isin(ft_to_check)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** view_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_feature (data, feature, view_all = True, view_0_10 = True):\n",
    "    '''\n",
    "    function to view and check continuous numeric data\n",
    "    feature : str, feature name\n",
    "    view_0_10 : boolean for histogram display of value 0 to 10, default as True\n",
    "    '''\n",
    "    # create data_stat using \n",
    "    data_stat = df_summary(data, ['int', 'float'])\n",
    "\n",
    "    min_val = data[feature].min()\n",
    "    max_val = data[feature].max()\n",
    "    bin_edges = np.arange(min_val, max_val + 10, 10)\n",
    "    desc_val = data_stat[data_stat['Attribute'] == feature]['Desc'].values[0]\n",
    "\n",
    "    if view_all:\n",
    "        ax = data[feature].plot(\n",
    "            kind = 'hist',\n",
    "            figsize=(10, 1.5),\n",
    "            color='gray',\n",
    "            bins = bin_edges,\n",
    "            align = 'mid',\n",
    "            title = ('histogram - ' + desc_val + ' - ' + feature)\n",
    "            );\n",
    "        ax.set_xlabel('Values - Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "        plt.show()\n",
    "\n",
    "    if view_0_10:    \n",
    "        ax = data[feature].plot(\n",
    "            kind = 'hist',\n",
    "            figsize=(10, 1.5),\n",
    "            color='gray',\n",
    "            bins = np.arange(-0.5, 11.5, 1),\n",
    "            align = 'mid',\n",
    "            title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "            );\n",
    "        ax.set_xlabel('Values - Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "        plt.show()\n",
    "    \n",
    "    # define the outlier thresholds by applying multiplier 1.5\n",
    "    q1 = data_stat[data_stat['Attribute'] == feature]['25%'].values[0]\n",
    "    q3 = data_stat[data_stat['Attribute'] == feature]['75%'].values[0]\n",
    "    iqr = q3 - q1\n",
    "    lower_threshold = q1 - 1.5 * iqr\n",
    "    upper_threshold = q3 + 1.5 * iqr\n",
    "\n",
    "    # identify outliers\n",
    "    col_val = data[feature].values\n",
    "    outliers = sorted(\n",
    "        set([feature for feature in col_val if feature < lower_threshold or feature > upper_threshold]),\n",
    "        reverse = True)\n",
    "\n",
    "    # print outliers\n",
    "    count_val = data_stat[data_stat['Attribute'] == feature].fillna(0)['count'].values[0]       \n",
    "    outlier_list = [\n",
    "        str(int(j)) + ': ' + '{:.1%}'.format((data[feature] == j).sum() / count_val)\n",
    "        for j in outliers\n",
    "        ]\n",
    "    \n",
    "    print('Outliers (Value: %)')\n",
    "    for j in range(0, len(outlier_list), 10):\n",
    "        print (', '.join(outlier_list[j : j+10]))\n",
    "    print ('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining features in ft_to_check are analyzed one by one using function view_feature above:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_HH_TITEL  \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_intsec, 'ANZ_HH_TITEL', False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GEBURTSJAHR  \n",
    "  - outliers: birth year can not be 0, that Value < 1900 should be replaced to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_intsec, 'GEBURTSJAHR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-GEBURTSJAHR'] = 'replace Value < 1900 to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_PERSONEN  \n",
    "  - Value 0: a household can not have 0 person, that Value 0 should be replaced to NaN\n",
    "  - outliers: household with over 10 persons is highly extreme or data error, that Value over 10 should be replaced to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_intsec, 'ANZ_PERSONEN', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_PERSONEN'] = 'replace Value 0 to NaN / Value > 10 to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MIN_GEBAEUDEJAHR   \n",
    "  - No pre-processing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_intsec, 'MIN_GEBAEUDEJAHR', True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_HAUSHALTE_AKTIV  \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "  - Log scale is needed due to high skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_intsec, 'ANZ_HAUSHALTE_AKTIV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_HAUSHALTE_AKTIV'] = '[Log scale]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KBA13_ANZAHL_PKW   \n",
    "  - Log scale is needed due to high skewness as values over 1250 is grouped by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_intsec, 'KBA13_ANZAHL_PKW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-KBA13_ANZAHL_PKW'] = '[Log scale]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_TITEL\n",
    "  Value 0 and max value might be strange or extreme but are possible, but this feature is linked to ANZ_PERSONEN above\n",
    "  - replace value to NaN if corresponding ANZ_PERSONEN is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_intsec, 'ANZ_TITEL', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_TITEL'] = 'replace value to NaN if ANZ_PERSONEN is NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_obj = df_summary(concat_intsec, ['int', 'float'], False)\n",
    "intsec_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(intsec_obj.Attribute.unique()) - set(attr_str.Attribute.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMEO_DEUG_2015는 numeric이므로 identical 하지 않음을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "일단 원래 데이터를 불러와서\n",
    "'''\n",
    "\n",
    "intsec_obj_data = concat_intsec[intsec_obj.Attribute.unique()]\n",
    "intsec_obj_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(intsec_obj_data.CAMEO_DEUG_2015.unique()).difference(set(attr[attr.Attribute == 'CAMEO_DEUG_2015']['Score'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_obj_data.CAMEO_DEUG_2015.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[attr.Attribute == 'CAMEO_DEUG_2015']['Score'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(intsec_obj_data.OST_WEST_KZ.unique()).difference(set(attr[attr.Attribute == 'OST_WEST_KZ']['Score'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(intsec_obj_data.CAMEO_DEU_2015.unique()).difference(set(attr[attr.Attribute == 'CAMEO_DEU_2015']['Score'].unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가이드 받은 내용과 다르며 XX, X는 null 처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/3 오전"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 excl features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_col = list(feature_dict['in_customers_notin_attr'])\n",
    "\n",
    "concat_excl = concat_data[excl_col]\n",
    "concat_excl.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_num = df_summary(concat_excl, ['int', 'float'])\n",
    "excl_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_num['min_max_cat'] = excl_num[\n",
    "    'min'].apply(lambda x: '{:_.0f}'.format(x)).astype(str) + ' to ' + excl_num[\n",
    "    'max'].apply(lambda x: '{:_.0f}'.format(x)).astype(str)\n",
    "\n",
    "excl_num.min_max_cat.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* max 30 이하는 위에서 살펴본 데로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max 30 이상 검토\n",
    "'''\n",
    "excl_num[excl_num['max'] > 30]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**아래 항목은 다음 절로 이동**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GEMEINDETYP   \n",
    "  - No pre-processing needed, 정확한 의미는 모르겠으나 극단적 특이성은 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_excl, 'GEMEINDETYP', True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EINGEZOGENAM_HH_JAHR \n",
    "  - outliers: Eng translation is not completely understandable, but 3 outliers can be replaced to NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_excl, 'EINGEZOGENAM_HH_JAHR', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_numeric[concat_numeric['EINGEZOGENAM_HH_JAHR'] < 1980]['EINGEZOGENAM_HH_JAHR'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-AEINGEZOGENAM_HH_JAHR'] = 'replace Value < 1980 to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LNR with the same count of concat_numeric seems to be the serial index of dataset, that it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2'] = 'drop LNR'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EXTSEL992   \n",
    "  - No pre-processing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_excl, 'EXTSEL992')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_STATISTISCHE_HAUSHALTE   \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "  - Log scale is needed due to high skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_excl, 'ANZ_STATISTISCHE_HAUSHALTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_STATISTISCHE_HAUSHALTE'] = '[Log scale]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VERDICHTUNGSRAUM   \n",
    "  - No pre-processing needed with Eng translation not completely understandable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature (concat_excl, 'VERDICHTUNGSRAUM')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위 항목은 다음 절로 이동**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_obj = df_summary(concat_excl, ['int', 'float'], False)\n",
    "excl_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(excl_obj.Attribute.unique()):\n",
    "    print (i, concat_excl[i].nunique(), concat_excl[i].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EINGEFUEGT_AM: date 이나 연단위로 ... 너무 많아서\n",
    "* CAMEO_INTL_2015: xx nan으로"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**전처리 준비작업 여기까지**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pv_min_max 2.3.1 변경 for merge\n",
    "min_max_copy = pd.DataFrame(pv_min_max.to_records())\n",
    "min_max_copy.columns = list(\n",
    "    min_max_copy.columns[:2]) + list(eval(i)[0] for i in min_max_copy.columns[2:])\n",
    "min_max_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_num_check = intsec_num.merge(\n",
    "    min_max_copy.drop('Description', axis = 1),\n",
    "    on = 'Attribute',\n",
    "    how = 'left',\n",
    "    suffixes = ('_attr', '_customer')\n",
    ")\n",
    "intsec_num_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_num_check['check'] = np.where(\n",
    "    intsec_num_check['min_max_cat_customer'].isnull(), \n",
    "    'no min/max',\n",
    "    np.where(\n",
    "        (intsec_num_check['min_customer'] < intsec_num_check['min_attr']),\n",
    "         'min lower',\n",
    "         np.where((intsec_num_check['max_customer'] > intsec_num_check['max_attr']),\n",
    "         'max_higher',\n",
    "         'within min/max range')))\n",
    "\n",
    "intsec_num_check['check'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "within min/max range는 원래 아는데로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "min_lower\n",
    "'''\n",
    "intsec_num_check[intsec_num_check['check'] == 'min_lower']['min_customer'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 아는데로 -1을 null 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "no min/max\n",
    "'''\n",
    "intsec_no_min_max = intsec_num_check[intsec_num_check['check'] == 'no min/max']\n",
    "intsec_no_min_max\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2개는 지우기로 한 것이고,\n",
    "나머지는 str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsec_no_min_max = intsec_no_min_max.query('Attribute != \"LP_FAMILIE_GROB\" and Attribute != \"LP_STATUS_GROB\"')\n",
    "set(intsec_no_min_max.Attribute.unique()) - set(attr_str.Attribute.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동 features는 따로 살펴봄:   \n",
    "여기에 아래 그래프 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset excl\n",
    "'''\n",
    "\n",
    "excl_col = list(feature_dict['in_customers_notin_attr'])\n",
    "\n",
    "concat_excl = concat_data[excl_col]\n",
    "concat_excl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_num = df_summary(concat_excl, ['int', 'float'])\n",
    "print (excl_num.shape)\n",
    "excl_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_num['min_max_cat'] = excl_num[\n",
    "    'min'].apply(lambda x: '{:_.0f}'.format(x)).astype(str) + ' to ' + excl_num[\n",
    "    'max'].apply(lambda x: '{:_.0f}'.format(x)).astype(str)\n",
    "\n",
    "excl_num.min_max_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_num[excl_num['max'] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 features with numeric Score form\n",
    "\n",
    "///// For numeric features, concat_numeric including int and float type data is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_num = concat_data.select_dtypes(include = ['int', 'float']).copy()\n",
    "concat_num.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_summary (data, score_form):\n",
    "    '''\n",
    "    aaa\n",
    "    '''\n",
    "    # filter and copy data\n",
    "    data_copy = data.select_dtypes(include = score_form).copy()\n",
    "    \n",
    "    # extract summary of data by applying describe and transpose\n",
    "    data_summary = data_copy.describe().T.reset_index()\n",
    "\n",
    "    # merge Desc (information on Attribute) from feature_desc and add min_max_cat\n",
    "    data_summary = data_summary.rename(columns = {'index' : 'Attribute'})\n",
    "    data_summary = vlookup(data_summary, feature_desc, 'Attribute', 'Desc')\n",
    "    \n",
    "    return data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset excl\n",
    "'''\n",
    "\n",
    "excl_col = list(feature_dict['in_customers_notin_attr'])\n",
    "\n",
    "concat_excl = concat_data[excl_col]\n",
    "concat_excl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_num = df_summary(concat_excl, ['int', 'float'])\n",
    "print (excl_num.shape)\n",
    "excl_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_num['min_max_cat'] = excl_num[\n",
    "    'min'].apply(lambda x: '{:_.0f}'.format(x)).astype(str) + ' to ' + excl_num[\n",
    "    'max'].apply(lambda x: '{:_.0f}'.format(x)).astype(str)\n",
    "\n",
    "excl_num.min_max_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_num[excl_num['max'] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 features with numeric Score form\n",
    "\n",
    "///// For numeric features, concat_numeric including int and float type data is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_numeric = concat_data.select_dtypes(include = ['int', 'float']).copy()\n",
    "concat_num = concat_numeric.copy()\n",
    "concat_num.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///// And concat_num with the summary statistics is formed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 str values\n",
    "\n",
    "///// extract 8 features in str type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_string = concat_data.select_dtypes(exclude = ['int', 'float'])\n",
    "concat_str = concat_string.copy()\n",
    "concat_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract summary of concat_string by applying describe and transpose\n",
    "concat_summary = concat_str.describe().T.reset_index()\n",
    "\n",
    "# merge Desc (information on Attribute) from feature_desc\n",
    "concat_summary = concat_summary.rename(columns = {'index' : 'Attribute'})\n",
    "concat_summary = vlookup(concat_summary, feature_desc, 'Attribute', 'Desc')\n",
    "concat_summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///// type EINGEFUEGT_AM should be changed to datetime with only year value to avoid excessively huge size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_str = concat_str.drop('EINGEFUEGT_AM', axis = 1)\n",
    "p_process['3.3.2'] = 'change type of EINGEFUEGT_AM to datetime of year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract values of Attribute in list\n",
    "concat_str_values = pd.DataFrame(\n",
    "    {'Attribute': concat_str.columns,\n",
    "     'Score_val': concat_str.values.T.tolist()})\n",
    "concat_str_values['Score_val'] = concat_str_values['Score_val'].apply(\n",
    "    lambda x: list(pd.Series(x).drop_duplicates().dropna()))\n",
    "\n",
    "# merge Desc (information on Attribute) from feature_desc and add min_max_cat\n",
    "concat_str_values = vlookup(concat_str_values, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "\n",
    "view_all(concat_str_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그냥 여기서 보고 처리하는 것이 나을 듯**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_attr_str_compare = pd.DataFrame(pv_attr_str.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_attr_str_compare = pv_attr_str\n",
    "concat_str_compare = vlookup(concat_str_values, pv_attr_str_compare, 'Attribute', 'Score', 'not_in_attr')\n",
    "concat_str_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all(concat_str_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_num[attr_num['Attribute'] == 'CUSTOMER_GROUP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_str_compare[concat_str_compare['Attribute'] == 'CAMEO_DEU_2015']['Score_val'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**23.05.31**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_str_compare['Score_compare'] = [list(a - b) for a, b in zip(\n",
    "    concat_str_compare['Score_val'], concat_str_compare['Score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_numeric = concat_data.select_dtypes(include = ['int', 'float']).copy()\n",
    "concat_num = concat_numeric.copy()\n",
    "concat_num.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///// And concat_num with the summary statistics is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract summary statistics of concat_numeric by applying describe and transpose\n",
    "concat_stat = concat_num.describe().T.reset_index()\n",
    "\n",
    "# merge Desc (information on Attribute) from feature_desc and add min_max_cat\n",
    "concat_stat = concat_stat.rename(columns = {'index' : 'Attribute'})\n",
    "concat_stat = vlookup(concat_stat, feature_desc, 'Attribute', 'Desc')\n",
    "concat_stat['min_max_cat'] = concat_stat[\n",
    "    'min'].apply(lambda x: '{:_.0f}'.format(x)).astype(str) + ' to ' + concat_stat[\n",
    "    'max'].apply(lambda x: '{:_.0f}'.format(x)).astype(str)\n",
    "\n",
    "concat_stat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.3.2-XX'] = 'replace Value X, XX to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process = {'pre_processing_itmes' : p_process}\n",
    "p_process_items = pd.DataFrame(p_process).reset_index()\n",
    "p_process_items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그 다음에 Preprocessing을 하고 나면 (필요시 극히 비슷한 컬럼 제외),   \n",
    "Imputing, Scaling 하면 PCA, Clustering, 앙상블로 나갈 수 있음** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function] miss_val_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_val_summary(df, axis_val, x_bin = 2, bar_chart = True):\n",
    "    '''\n",
    "    function to display summary, bar-chart (optional) and histogram\n",
    "    of missing value by column or raw\n",
    "    df: dataframe\n",
    "    axis_val: str, one of 'column' or 'row'\n",
    "    x_bin: size of x bin, 10 as default\n",
    "    bar_chart: option of bar chart display \n",
    "    '''\n",
    "    # index of axis\n",
    "    axis_idx = 0 if axis_val == 'column' else 1\n",
    "    \n",
    "    # % of missing values\n",
    "    missing_pct = df.isnull().mean(axis = axis_idx) * 100\n",
    "    df_desc = missing_pct.describe()\n",
    "\n",
    "    # summary of missing value\n",
    "    print (\n",
    "        '% of missing value in ' + str(int(df_desc[0])) + ' ' + axis_val + 's of ' + df.name)\n",
    "    print (df_desc[1:].to_string())\n",
    "    \n",
    "    # bar-chart of missing value\n",
    "    if bar_chart:\n",
    "        missing_pct.plot(\n",
    "            kind = 'bar', figsize=(10, 3), color='gray',\n",
    "            \n",
    "            title = ('bar chart - ' + df.name + ': missing value by ' + axis_val),\n",
    "            ylabel = '% of missing value',\n",
    "            xlabel = (str(int(df_desc[0])) + ' columns'),\n",
    "            xticks = [],\n",
    "            );\n",
    "        plt.show()\n",
    "    \n",
    "    # hist of missing value\n",
    "    x_range = ((df_desc[-1] + 10) // 10) * 10 + x_bin\n",
    "    ax = missing_pct.plot(\n",
    "        kind = 'hist', figsize=(10, 3), color='gray',\n",
    "        \n",
    "        bins = np.arange(0, x_range, x_bin),\n",
    "        title = ('histogram - ' + df.name + ': missing value by ' + axis_val)\n",
    "        )\n",
    "    ax.set_xlabel('% of missing value');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "missing value by columns of azdias\n",
    "'''\n",
    "\n",
    "miss_val_summary(azdias, 'column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "missing value by column\n",
    "'''\n",
    "\n",
    "miss_val_summary(customers, 'column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 alignment of features   \n",
    "as values of data files (azdias & customers) can be readable by explanations of information files (info & attr),   \n",
    "check alignment in column features of data files and equivalent values of column Attribute of information files at first   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 합쳐서 데이터 분석시 참조\n",
    "> 93 데이터에만 있는 Attr은 어떻게 할 것인가?\n",
    "> 데이터 파일에 없고 정보 파일에만 있는 51 Attr은 제외하여 simplify 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 feature description\n",
    "As the data sets do not have information on what each feature (column name) exactly means,   \n",
    "values of information files (info & attr) should be mapped to the features at first,   \n",
    "to see how the data sets are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to know what features datasets have, add information to column names of customers\n",
    "'''\n",
    "\n",
    "feature_desc = pd.DataFrame(customers.columns, columns=['Attribute'])\n",
    "feature_desc = vlookup(feature_desc, info, 'Attribute')\n",
    "\n",
    "'''\n",
    "df_feature has 369 unique Attribute values:\n",
    "105 exclusive values of customers and 264 shared values with customers\n",
    "(see 1.5 alignment of features)\n",
    "''' \n",
    "print ('Attributes missing Description:', feature_desc[feature_desc.Description.isna()].shape[0])\n",
    "print (feature_desc.shape)\n",
    "feature_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To enhance readability of data set - over 100 features are without description -   \n",
    "I added 14 Description values from attr, and made a column of translation   \n",
    "(ger_to_eng) to df_feature using code below.\n",
    "However as this code-running takes somewhat long time, I saved the dataframe processed\n",
    "as df_feature.xlsx in root folder\n",
    "'''\n",
    "\n",
    "# # For values of Attribute without Description, add 14 Description values from attr\n",
    "# df_feature.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# df_feature.update(attr_excl)\n",
    "\n",
    "# df_feature.reset_index(inplace = True)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# df_feature['ger_to_eng'] = np.where(\n",
    "#     df_feature.Description.isnull(),\n",
    "#     df_feature.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# df_feature['Desc'] = df_feature.Description.fillna('') + df_feature.ger_to_eng.fillna('')\n",
    "\n",
    "feature_desc = pd.read_excel('feature_desc.xlsx', index_col = [0])\n",
    "feature_desc.head(10)\n",
    "\n",
    "print ('Attributes missing Desc:', feature_desc[feature_desc.Desc.isna()].shape[0])\n",
    "print (feature_desc.shape)\n",
    "feature_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find all features one by one\n",
    "'''\n",
    "\n",
    "# # not to run\n",
    "\n",
    "# view_all(feature_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(attr.Attribute.unique()) - set(feature_desc.Attribute.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (list(set(attr.Attribute.unique()) - set(feature_desc.Attribute.unique())))\n",
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df = attr.iloc[:, :2].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df[short_df.Attribute.isin(diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[attr.iloc[:, :2].drop_duplicates().Attribute.isin(diff)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 살펴 봤음. 모든 Attribute에 대해서 Description을 completely 이해할 수는 없었으나   \n",
    "> 대체적인 내용 구성을 이해할 수는 있었음   \n",
    "> 비슷한 내용을 나타내는 중복열, 유사열이 많아 공선성 해소, 차원 축소가 필요함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 data values\n",
    "The values of the data sets can also be readable by mapping values of Value and Meaning of attr.   \n",
    "In this section, 데이터 종류 (연속/이산), 이상치, 사실상의 null value 등 데이터 전처리를 위한 데이터의 내용적 측면을 점검하겠음.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check types of values of attr Value, which contains information on data values\n",
    "'''\n",
    "\n",
    "attr['Value_dtype'] = attr.Value.map(type)\n",
    "print (attr.Value_dtype.value_counts())\n",
    "\n",
    "# attr['Meaning_dtype'] = attr.Meaning.map(type)\n",
    "# print (attr.Meaning_dtype.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 attr의 int 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5에서 살펴본 바와 같이 272개 피쳐는 attr을 통해 설명이 가능함 (물론 이것도 데이터를 따로 파악해 봐야 하나)    \n",
    "데이터 셋을 직접 살펴 보는 것은 뒤에 별도록 진행하고 우선 attr을 분석하여 데이터 내용이 어떻게 구성되어 있는지 Basis를 확보해야 함.   \n",
    "\n",
    "2113개의 정수 값이고, 145개는 object로 정수 값으로 정의된 value를 먼저 점검해 보겠음.\n",
    "\n",
    "* numeric data of column Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "numeric data of column Value\n",
    "'''\n",
    "# attr_num with only numeric values in Value\n",
    "attr_num = attr[attr['Value_dtype'] == int].copy()\n",
    "print (attr_num.shape) \n",
    "\n",
    "# add Desc and Information level\n",
    "attr_num = vlookup(attr_num, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "print (attr_num.info())\n",
    "attr_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "customers에는 없고 attr에만 있는 42개 Attribute는 Desc가 없으으로\n",
    "(1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "'''\n",
    "\n",
    "print ('customers에는 없고 attr에만 있는 42개 Attribute 수:', \n",
    "       attr_num[attr_num.Desc.isna() == True].Attribute.nunique(),\n",
    "       '\\n')\n",
    "\n",
    "attr_num = attr_num[attr_num.Desc.isna() == False]\n",
    "print (attr_num.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "numeric data of column Value where 1774 rows have data in int type 으로\n",
    "pivot을 돌려 int type Value가 어떤 min max 값을 가지는 지 정리함.\n",
    "이를 통해 각 feature의 data type을 추정하고, 효과적으로 비정상 값을 찾아내려 함\n",
    "'''\n",
    "\n",
    "# summary of numeric data of Value\n",
    "pv_attr_num = pd.pivot_table(\n",
    "    attr_num,\n",
    "    index = ['Attribute', 'Desc'],\n",
    "    values = 'Value',\n",
    "    aggfunc = [min, max]\n",
    "    )\n",
    "\n",
    "pv_attr_num['min_max_cat'] = pv_attr_num['min'].astype(str) + ' to ' + pv_attr_num['max'].astype(str)\n",
    "pv_attr_num = pv_attr_num.sort_values(by = 'min_max_cat')\n",
    "\n",
    "print (\n",
    "    'min_max category of numeric data in column Value', '\\n',\n",
    "    pv_attr_num.min_max_cat.value_counts())\n",
    "pv_attr_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find all numeric data of column Value one by one\n",
    "'''\n",
    "\n",
    "# # not to run\n",
    "\n",
    "# with pd.option_context(\n",
    "#     'display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(pv_attr_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. value -1, 0의 경우 null 또는 -inf에 상응하는 데이터가 있는지  \n",
    "> 2. max가 7까지는 descrete, 8 이상은 continue 인지 봐야하고 Max의 null 또는 -inf에 상응하는 데이터가 있는지\n",
    "> 3. binary cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. value -1, 0의 경우 null 또는 -inf에 상응하는 데이터가 있는지 \n",
    "'''\n",
    "attr_below_1 = attr_num[attr_num.Value < 1]\n",
    "print(attr_below_1.shape)\n",
    "attr_below_1.Meaning.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "위 값 중 null 가능성이 있는 값만 추려 본결과\n",
    "'''\n",
    "\n",
    "maybe_null = ['unknown',\n",
    "              'no classification possible',\n",
    "              'unknown / no main age detectable',\n",
    "              'no transactions known', \n",
    "              'no transaction known', \n",
    "              'classification not possible',\n",
    "              'none',\n",
    "              'no score calculated'\n",
    "              ]\n",
    "\n",
    "attr_below_1 = attr_below_1[attr_below_1.Meaning.isin(maybe_null)].sort_values(by = 'Meaning')\n",
    "print(attr_below_1.shape)\n",
    "attr_below_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not to run\n",
    "\n",
    "# view_all(attr_below_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Meaning이 maybe_null 이면 모두 nan 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. max가 7까지는 descrete, 8 이상은 continue 인지 봐야하고 Max의 null 또는 -inf에 상응하는 데이터가 있는지\n",
    "'''\n",
    "\n",
    "# pv_attr_num에서 max가 7보다 큰 Attr을 추출함\n",
    "max_over_7_Attr = pv_attr_num[pv_attr_num[('max', 'Value')] > 7].index.get_level_values(0)\n",
    "# view_all(attr_num[attr_num.Attribute.isin(max_over_7_Attr)])\n",
    "max_over_7 = attr_num[attr_num.Attribute.isin(max_over_7_Attr)]\n",
    "pv_max_over_7 = pd.pivot_table(\n",
    "    max_over_7,\n",
    "    index = 'Attribute',\n",
    "    values = 'Meaning',\n",
    "    aggfunc = lambda x: list(x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all(pv_max_over_7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> no continuous data detected, 그러나 최대값에 maybe null이 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_over_7['Value'] = pd.to_numeric(max_over_7['Value'], errors='coerce')\n",
    "\n",
    "max_over_7_idxmax = max_over_7.groupby('Attribute')['Value'].idxmax()\n",
    "\n",
    "max_over_7.loc[max_over_7_idxmax, 'Meaning'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> maybe_null에 'uniformly distributed', ... 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "3. binary cells\n",
    "'''\n",
    "\n",
    "# pv_attr_num에서 max가 4보다 작은 Attr을 추출함\n",
    "max_under_4_Attr = pv_attr_num[pv_attr_num[('max', 'Value')] < 4].index.get_level_values(0)\n",
    "# view_all(attr_num[attr_num.Attribute.isin(max_over_7_Attr)])\n",
    "max_under_4 = attr_num[attr_num.Attribute.isin(max_under_4_Attr)]\n",
    "pv_max_under_4 = pd.pivot_table(\n",
    "    max_under_4,\n",
    "    index = 'Attribute',\n",
    "    values = 'Meaning',\n",
    "    aggfunc = lambda x: list(x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all(pv_max_under_4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 뭐뭐뭐가 이진으로 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 attr의 str 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "str data of column Value\n",
    "'''\n",
    "# attr_num with only numeric values in Value\n",
    "attr_str = attr[attr['Value_dtype'] == str].copy()\n",
    "print (attr_str.shape) \n",
    "\n",
    "# add Desc and Information level\n",
    "attr_str = vlookup(attr_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "print (attr_str.info())\n",
    "attr_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3개 null attribute는 정수와 문자를 모두 값을로 갖는 것들로 42개는 유효하고\n",
    "이 42개는 다음 section에서 볼 예정이므로 (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "'''\n",
    "\n",
    "# attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나...\n",
    "\n",
    "attr_str = attr_str[attr_str.Desc.isna() == False]\n",
    "print (attr_str.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pivot을 돌려 attribute 별로 어떤 str 값을 가지는 지 정리함.\n",
    "이를 통해 각 feature의 data type을 추정하고, 효과적으로 비정상 값을 찾아내려 함\n",
    "'''\n",
    "\n",
    "pv_attr_str = pd.pivot_table(\n",
    "    attr_str,\n",
    "    index = ['Attribute', 'Desc', 'Meaning'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: x\n",
    "    )\n",
    "\n",
    "pv_attr_str.head(10)\n",
    "# view_all(pv_attr_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. unknown을 표현하는 경우, drop에 포함    \n",
    "> 2. 연속형 수치를 표현하는 경우 ... 이는 data set을 직접 보고 파악해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "순수하게 str인 경우만 추출\n",
    "'''\n",
    "# pv_attr_str = pd.DataFrame(pv_attr_str.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_str_to_check = pv_attr_str[pv_attr_str.Meaning.str.contains('numeric value')].Attribute\n",
    "attr_str_to_check = pv_attr_str[\n",
    "    pv_attr_str.index.get_level_values(2).str.contains('numeric value')].index.get_level_values(0)\n",
    "# 먼저 추후 체크할 것들을 뽑아 놓고\n",
    "attr_str_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_attr_str = pd.DataFrame(pv_attr_str[\n",
    "    ~(pv_attr_str.index.get_level_values(0).isin(attr_str_to_check))\n",
    "    &~(pv_attr_str.index.get_level_values(2) == 'unknown')\n",
    "    ].to_records())\n",
    "\n",
    "pv_attr_str = pd.pivot_table(\n",
    "    pv_attr_str,\n",
    "    index = ['Attribute', 'Desc'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: list(x)\n",
    "    )\n",
    "\n",
    "view_all(pv_attr_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CAMEO_DEU_2015 정상적인 카테고리 데이터... 피쳐   \n",
    "> OST_WEST_KZ은 2진"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 customers에만 있는 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**상당히 해깔리게 되어 있는데, 데이터의 컬럼과 정보 파일의 Attribute 숫자를 좀 정확하게 정리하고   \n",
    "하던데로 커스터머에만 있는 데이터를 정리하면 Wrangling이 끝남** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그 다음에 Preprocessing을 하고 나면 (필요시 극히 비슷한 컬럼 제외),   \n",
    "Imputing, Scaling 하면 PCA, Clustering, 앙상블로 나갈 수 있음** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. attributes_xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_val(attributes_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = list(attributes_xlsx.Meaning.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_words = []\n",
    "\n",
    "for synset in w_list:\n",
    "    if len(synset.lemmas()) > 1:\n",
    "        ambiguous_words.append(synset.name().split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(wordnet.all_synsets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wordnet.synsets('unknown')\n",
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "\n",
    "for synset in synsets:\n",
    "    for lemma in synset.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the target word\n",
    "target_word = 'unidentified'\n",
    "\n",
    "# Retrieve synsets for the target word\n",
    "synsets = wordnet.synsets(target_word)\n",
    "\n",
    "# # Retrieve synonyms for each synset and filter out synonyms containing the target word\n",
    "# filtered_synonyms = []\n",
    "\n",
    "# for synset in synsets:\n",
    "#     synonyms = synset.lemmas()\n",
    "#     filtered_synonyms.extend([synonym.name() for synonym in synonyms if target_word not in synonym.name()])\n",
    "\n",
    "# # Remove duplicate synonyms and sort the list\n",
    "# filtered_synonyms = sorted(set(filtered_synonyms))\n",
    "\n",
    "# print(filtered_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_synonyms = []\n",
    "\n",
    "for synset in synsets:\n",
    "    synonyms = synset.lemmas()\n",
    "    filtered_synonyms.extend([synonym.name() for synonym in synonyms if target_word not in synonym.name()])\n",
    "\n",
    "# Remove duplicate synonyms and sort the list\n",
    "filtered_synonyms = sorted(set(filtered_synonyms))\n",
    "\n",
    "print(filtered_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_word in filtered_synonyms:\n",
    "    # # Define the target word\n",
    "    # target_word = 'unidentified'\n",
    "\n",
    "    # Retrieve synsets for the target word\n",
    "    synsets = wordnet.synsets(target_word)\n",
    "\n",
    "    for synset in synsets:\n",
    "        synonyms = synset.lemmas()\n",
    "        filtered_synonyms.extend([synonym.name() for synonym in synonyms if target_word not in synonym.name()])\n",
    "\n",
    "    # Remove duplicate synonyms and sort the list\n",
    "    filtered_synonyms = sorted(set(filtered_synonyms))\n",
    "\n",
    "print(filtered_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customers\n",
    "\n",
    "print (customers.info())\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes_xlsx\n",
    "\n",
    "print (attributes_xlsx.info())\n",
    "attributes_xlsx.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify attributes_xlsx\n",
    "\n",
    "attributes_xlsx = attributes_xlsx.iloc[:, 1:] # 1st column has no info\n",
    "attributes_xlsx[['Attribute', 'Description']] = attributes_xlsx[\n",
    "    ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "print (attributes_xlsx.info())\n",
    "attributes_xlsx.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information_xlsx\n",
    "\n",
    "print (information_xlsx.info())\n",
    "information_xlsx.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work / Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def col_val (df):\n",
    "#     '''\n",
    "#     function to check values of dataframe columns\n",
    "#     df : dataframe\n",
    "#     '''\n",
    "#     # for i in df.columns:\n",
    "#     #     print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#     #         df[i].value_counts(), '\\n', '*     *     *')\n",
    "#     for i in df.columns:\n",
    "#         print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#         list(df[i].unique()), '\\n', '*     *     *')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from googletrans import Translator\n",
    "\n",
    "# # Create an instance of the Translator\n",
    "# translator = Translator(service_urls=['translate.google.com'])\n",
    "\n",
    "# # Text to be translated\n",
    "# text = \"AGER_TYP\"\n",
    "\n",
    "# # Translate the text from German to English\n",
    "# translation = translator.translate(text, src='de', dest='en')\n",
    "\n",
    "# # Print the translated text\n",
    "# print(\"Original text (German):\", text)\n",
    "# print(\"Translated text (English):\", translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def miss_val_hist(df, axis_val, x_bin = 10):\n",
    "#     '''\n",
    "#     function to display missing value histogram by column or raw\n",
    "#     df: dataframe\n",
    "#     axis_val: str, one of 'column' or 'row'\n",
    "#     x_bin: size of xtick bin, 10 as default \n",
    "#     '''\n",
    "#     # axis value\n",
    "#     axis_num = 0 if axis_val == 'column' else 1\n",
    "    \n",
    "#     # % of missing values\n",
    "#     missing_pct = df.isnull().mean(axis = axis_num) * 100\n",
    "\n",
    "#     # max % of missing values by column\n",
    "#     missing_pct_max = missing_pct.max()\n",
    "#     print ('max % of missing values by ' + axis_val + ': ', missing_pct_max)\n",
    "\n",
    "#     # plot missing values by column\n",
    "    \n",
    "#     print (missing_pct.describe())\n",
    "    \n",
    "#     x_range = ((missing_pct_max + x_bin * 2) // x_bin) * x_bin\n",
    "\n",
    "#     ax = missing_pct.plot(\n",
    "#         kind = 'hist', figsize=(10, 3), color='gray',\n",
    "#         bins = np.arange(0, x_range, 10),\n",
    "#         title = (df.name + ': missing value by ' + axis_val)\n",
    "#         )\n",
    "#     ax.set_xlabel('% of missing value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이써닉 하지 못한 코드\n",
    "# # missing value overview\n",
    "# for i in range(0, ((azdias.shape[1] + 100) // 100) * 100, 100):\n",
    "#     msno.matrix(azdias.iloc[:, i : i + 99],\n",
    "#                 figsize=(10, 3), fontsize = 12, labels = False, sparkline = False)\n",
    "#     plt.title('missing value overview: col ' + str (i) + ' to ' + str (min(i + 99, azdias.shape[1] - 1)),\n",
    "#               fontsize = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of over 30%\n",
    "# (azdias.isnull().mean() * 100 > 30).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of 25% to 30%\n",
    "# ((azdias.isnull().mean() * 100 > 25) & (30 >= azdias.isnull().mean() * 100)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아마도 쓰지 않을 plot\n",
    "# plot = azdias_col_missing_pct.plot(\n",
    "#     kind = 'bar', figsize=(10, 3), color='dimgray', xticks = [],\n",
    "#     title = 'azdias_col_missing_pct',\n",
    "#     xlabel = '366 columns',\n",
    "#     ylabel = '% of missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_df = customers.select_dtypes(include=['float', 'int64']).iloc[:, 1:]\n",
    "# num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pd.unique(customers.select_dtypes(include='float').values.flatten()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\n",
    "#     'display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(pd.DataFrame(attr.apply(lambda x: x.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_not_null = ~(attr.Attribute.isna())\n",
    "# attr.loc[attr_not_null, 'Description'] = attr.loc[\n",
    "#     attr_not_null, 'Description'] + ' ' + attr.loc[attr_not_null, 'desc_shift']\n",
    "\n",
    "# desc_to_null = (attr.Attribute.isna()) & ~(attr.Description.isna())\n",
    "# attr.loc[desc_to_null, 'Description'] = np.nan\n",
    "# attr = attr.drop(columns = 'desc_shift')\n",
    "# attr.loc[attr_with_value.shift(-1, fill_value = True), 'Description']\n",
    "# attr_shift = attr_null.shift\n",
    "# attr[attr_null.shift, 'Description'] = attr.loc[\n",
    "#     attr_null.shift(fill_value = False), 'Description'] + ' ' + attr[attr_null, 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# info_mg = info_mg.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "# # attr_excl = attr[attr.Attribute.isin(attr_excl)].copy()\n",
    "# attr_not_null = attr.dropna(subset = 'Attribute').copy()\n",
    "# attr_not_null = attr_not_null.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_not_null[['Attribute', 'Description']]],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_excl)][['Attribute', 'Description']].copy()\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# # info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_all(info_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 2258 rows of Attribute in attr, add 13 values exclusively in info,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# attr_mg = attr.copy()\n",
    "# info_excl = info[info.Attribute.isin(info_excl)][['Attribute', 'Description']].copy()\n",
    "# info_excl['Value'] = 'form info'\n",
    "# info_excl['Meaning'] = 'form info'\n",
    "\n",
    "# attr_mg = pd.concat(\n",
    "#     [attr_mg, info_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# # info_mg = info_mg.drop_duplicates()\n",
    "\n",
    "# # info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(attr_mg.info())\n",
    "# attr_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_mg.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# there are 93 and 51 exclusive values in data and information files\n",
    "# '''\n",
    "\n",
    "# datafile_Attr = azdias_Attr.union(customers_Attr)\n",
    "# infofile_Attr = info_Attr.union(attr_Attr)\n",
    "\n",
    "# datafile_excl = datafile_Attr - infofile_Attr\n",
    "# infofile_excl = infofile_Attr - datafile_Attr\n",
    "\n",
    "# print (len(datafile_excl), 'Attribute value(s) exclusively in data files:',\n",
    "#        '\\n', datafile_excl)\n",
    "# print (len(infofile_excl), 'Attribute value(s) exclusively in information files:',\n",
    "#        '\\n', infofile_excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# info['Information level'] = info['Information level'].fillna(method = 'ffill')\n",
    "\n",
    "# info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# attr[['Attribute', 'Description']] = attr[\n",
    "#     ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "# attr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# To enhance readability of data set - over 100 features are without description -   \n",
    "# I added 14 Description values from attr, and made a column of translation   \n",
    "# (ger_to_eng) to df_feature using code below.\n",
    "# However as this code-running takes somewhat long time, I saved the dataframe processed\n",
    "# as df_feature.xlsx in root folder\n",
    "# '''\n",
    "\n",
    "# # For values of Attribute without Description, add 14 Description values from attr\n",
    "# df_feature.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# df_feature.update(attr_excl)\n",
    "\n",
    "# df_feature.reset_index(inplace = True)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# df_feature['ger_to_eng'] = np.where(\n",
    "#     df_feature.Description.isnull(),\n",
    "#     df_feature.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# df_feature['Desc'] = df_feature.Description.fillna('') + df_feature.ger_to_eng.fillna('')\n",
    "\n",
    "# # # sort by Attribute and Information level\n",
    "# # df_feature.sort_values(by = ['Attribute', 'Information level'], inplace= True)\n",
    "\n",
    "# # df_feature = pd.read_excel('df_feature.xlsx', index_col = [0])\n",
    "# # df_feature.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# df_feature.Description = df_feature.Description.mask(\n",
    "#     df_feature.Attribute == attr_excl.Attribute,\n",
    "#     attr_excl.Description\n",
    "#     )\n",
    "# print ('Attributes missing Description:', df_feature[df_feature.Description.isna()].shape[0])\n",
    "# print (df_feature.shape)\n",
    "# df_feature.head()\n",
    "\n",
    "# ValueError: Can only compare identically-labeled Series objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_desc.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_excl = attr[\n",
    "#     attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# feature_desc.update(attr_excl)\n",
    "\n",
    "# feature_desc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between data files: \n",
    "# df customers has 3 more exclusive columns\n",
    "# '''\n",
    "\n",
    "# azdias_Attr = set(azdias.columns)\n",
    "# customers_Attr = set(customers.columns)\n",
    "\n",
    "# print(azdias_Attr - customers_Attr)\n",
    "# print(customers_Attr - azdias_Attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between information files\n",
    "# '''\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "\n",
    "# # info_excl = info_Attr - attr_Attr\n",
    "# # attr_excl = attr_Attr - info_Attr\n",
    "\n",
    "# print (len(info_Attr - attr_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - info_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - info_Attr)\n",
    "# '''\n",
    "# alignment of features between customers and information files\n",
    "# '''\n",
    "# print ('Attribute between customers and info')\n",
    "# print (len(customers_Attr - info_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - info_Attr)\n",
    "# print (len(info_Attr - customers_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - customers_Attr)\n",
    "# print ('In', len(info_Attr), 'features of info,', \n",
    "#        len(info_Attr) - len(info_Attr - customers_Attr), 'features are in Attribute of customers', '\\n')\n",
    "\n",
    "# print ('Attribute between customers and attr')\n",
    "# print (len(customers_Attr - attr_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - customers_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - customers_Attr)\n",
    "# print ('In', len(attr_Attr), 'features of attr,',\n",
    "#        len(attr_Attr) - len(attr_Attr - customers_Attr), 'features are in Attribute of customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(feature_dict.keys())[0]\n",
    "# feature_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = np.array([1, 2, 3, 4])\n",
    "\n",
    "# subtractions = np.subtract.outer(values, values)[np.triu_indices(len(values), k=1)]\n",
    "\n",
    "# for result in subtractions:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (k, v) in enumerate(feature_dict.items()):\n",
    "#     for j in range(i + 1, 4):\n",
    "#             result = values[i] - values[j]\n",
    "#             print(f\"{values[i]} - {values[j]} = {result}\")\n",
    "#     print (i, k, v)\n",
    "    \n",
    "# for i, (k, v) in enumerate(zip(list(feature_dict.keys()), list(feature_dict.values()))):\n",
    "#     print (i, (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# customers에는 없고 attr에만 있는 42개 Attribute는 Desc가 없으으로\n",
    "# (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# print ('customers에는 없고 attr에만 있는 42개 Attribute 수:', \n",
    "#        attr_num[attr_num.Desc.isna() == True].Attribute.nunique(),\n",
    "#        '\\n')\n",
    "\n",
    "# attr_num = attr_num[attr_num.Desc.isna() == False]\n",
    "# print (attr_num.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# str data of column Value\n",
    "# '''\n",
    "# # attr_num with only numeric values in Value\n",
    "# attr_str = attr[attr['Value_dtype'] == str].copy()\n",
    "# print (attr_str.shape) \n",
    "\n",
    "# # add Desc and Information level\n",
    "# attr_str = vlookup(attr_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "# print (attr_str.info())\n",
    "# attr_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 3개 null attribute는 정수와 문자를 모두 값을로 갖는 것들로 42개는 유효하고\n",
    "# 이 42개는 다음 section에서 볼 예정이므로 (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# # attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나...\n",
    "\n",
    "# attr_str = attr_str[attr_str.Desc.isna() == False]\n",
    "# print (attr_str.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attr_str_to_check = pv_attr_str[pv_attr_str.Meaning.str.contains('numeric value')].Attribute\n",
    "# attr_str_to_check = pv_attr_str[\n",
    "#     pv_attr_str.index.get_level_values(2).str.contains('numeric value')].index.get_level_values(0)\n",
    "# # 먼저 추후 체크할 것들을 뽑아 놓고\n",
    "# attr_str_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pv_attr_str = pd.DataFrame(pv_attr_str[\n",
    "#     ~(pv_attr_str.index.get_level_values(0).isin(attr_str_to_check))\n",
    "#     &~(pv_attr_str.index.get_level_values(2) == 'unknown')\n",
    "#     ].to_records())\n",
    "\n",
    "# pv_attr_str = pd.pivot_table(\n",
    "#     pv_attr_str,\n",
    "#     index = ['Attribute', 'Desc'],\n",
    "#     values = 'Value',\n",
    "#     aggfunc = lambda x: list(x)\n",
    "#     )\n",
    "\n",
    "# view_all(pv_attr_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(not_in_attr_str.values.T.shape)\n",
    "# not_in_attr_str.values.T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + bin_interval, bin_interval)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 5.0\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 5.0 * iqr\n",
    "#     upper_threshold = q3 + 5.0 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(set([i for i in col_val if i < lower_threshold or i > upper_threshold]), reverse = True)\n",
    "#     for j in outliers:\n",
    "#         print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "#     print ('\\n', '==========' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     # bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + 10, 10)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 1.5\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 1.5 * iqr\n",
    "#     upper_threshold = q3 + 1.5 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(\n",
    "#         set([i for i in col_val if i < lower_threshold or i > upper_threshold]),\n",
    "#         reverse = True)\n",
    "#     # for j in outliers:\n",
    "#     #     print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "        \n",
    "#     outlier_list = [str(int(j)) + ': ' + '{:.1%}'.format((cc_num[i] == j).sum() / count_val)\n",
    "#                     for j in outliers]\n",
    "    \n",
    "#     print('Outliers (Value: %)')\n",
    "#     for j in range(0, len(outlier_list), 10):\n",
    "#         print (', '.join(outlier_list[j: j+10]))\n",
    "#     print ('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[function]** score_meaning_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_meaning_val(data, pv_idx = ['Attribute', 'Description', 'Desc', 'Additional notes']):\n",
    "#     '''\n",
    "#     function to check unique values of Score and Meaning by Attribute\n",
    "#     data : dataframe to examine\n",
    "#     pv_idx: list of pivot_table index\n",
    "#     '''\n",
    "#     data = vlookup(data, feature_desc, 'Attribute', ['Desc', 'Additional notes'], nan_val = 'no_info')\n",
    "#     pv = pd.pivot_table(\n",
    "#         data,\n",
    "#         index = pv_idx,\n",
    "#         values = ['Meaning', 'Score'],\n",
    "#         aggfunc = lambda x: list(x))\n",
    "    \n",
    "#     return pv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
