{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import pprint\n",
    "\n",
    "import missingno as msno\n",
    "from googletrans import Translator\n",
    "\n",
    "import itertools\n",
    "\n",
    "'''custom modules for convenience'''\n",
    "# function similar to Excel's vlookup\n",
    "from vlookup import vlookup\n",
    "# function to view all contents of a dataframe\n",
    "from view_all import view_all\n",
    "\n",
    "# import math\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install googletrans==4.0.0-rc1\n",
    "# pip install missingno\n",
    "# custom modules for convenience are in root folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 azdias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file structure\n",
    "# '''\n",
    "\n",
    "# azdias = pd.read_csv('../csv_pickle/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "# azdias.name = 'azdias'\n",
    "# print (azdias.shape)\n",
    "# azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = pd.read_pickle('../csv_pickle/azdias.pickle')\n",
    "azdias.name = 'azdias'\n",
    "print (azdias.info())\n",
    "azdias.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function] miss_val_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_val_overview(df, chunk_size = 100):\n",
    "    '''\n",
    "    function to display missing value using missingno(msno) library\n",
    "    df: dataframe\n",
    "    chunk_size: size of column chunk, 100 as default  \n",
    "    '''\n",
    "    # split df columns into chunks\n",
    "    chunk_size = chunk_size\n",
    "    column_chunks = [df.iloc[:, i : i + chunk_size] for i in range(0, df.shape[1], chunk_size)]\n",
    "\n",
    "    # generate and display missingno plots for each chunk\n",
    "    for i, _ in enumerate(column_chunks):\n",
    "        msno.matrix(_, figsize = (10, 3), fontsize = 10, labels = False, sparkline = False)\n",
    "        plt.title(\n",
    "            f'missing value overview - column {i * 100} to {min (i * chunk_size + chunk_size - 1, df.shape[1] - 1)}',\n",
    "            fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# miss_val_overview(azdias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function] miss_val_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_val_summary(df, axis_val, x_bin = 2, bar_chart = True):\n",
    "    '''\n",
    "    function to display summary, bar-chart (optional) and histogram\n",
    "    of missing value by column or raw\n",
    "    df: dataframe\n",
    "    axis_val: str, one of 'column' or 'row'\n",
    "    x_bin: size of x bin, 10 as default\n",
    "    bar_chart: option of bar chart display \n",
    "    '''\n",
    "    # index of axis\n",
    "    axis_idx = 0 if axis_val == 'column' else 1\n",
    "    \n",
    "    # % of missing values\n",
    "    missing_pct = df.isnull().mean(axis = axis_idx) * 100\n",
    "    df_desc = missing_pct.describe()\n",
    "\n",
    "    # summary of missing value\n",
    "    print (\n",
    "        '% of missing value in ' + str(int(df_desc[0])) + ' ' + axis_val + 's of ' + df.name)\n",
    "    print (df_desc[1:].to_string())\n",
    "    \n",
    "    # bar-chart of missing value\n",
    "    if bar_chart:\n",
    "        missing_pct.plot(\n",
    "            kind = 'bar', figsize=(10, 3), color='gray',\n",
    "            \n",
    "            title = ('bar chart - ' + df.name + ': missing value by ' + axis_val),\n",
    "            ylabel = '% of missing value',\n",
    "            xlabel = (str(int(df_desc[0])) + ' columns'),\n",
    "            xticks = [],\n",
    "            );\n",
    "        plt.show()\n",
    "    \n",
    "    # hist of missing value\n",
    "    x_range = ((df_desc[-1] + 10) // 10) * 10 + x_bin\n",
    "    ax = missing_pct.plot(\n",
    "        kind = 'hist', figsize=(10, 3), color='gray',\n",
    "        \n",
    "        bins = np.arange(0, x_range, x_bin),\n",
    "        title = ('histogram - ' + df.name + ': missing value by ' + axis_val)\n",
    "        )\n",
    "    ax.set_xlabel('% of missing value');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "missing value by columns of azdias\n",
    "'''\n",
    "\n",
    "miss_val_summary(azdias, 'column')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> xxx colum, 891K row데이터 인데, 심하게 missing value 많은 컬럼이 있음  \n",
    "> info는 컬럼에 대한 정보, att는 데이터에 대한 정보를 가지고 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출전 주석 해제\n",
    "# '''\n",
    "# load and file structure\n",
    "# '''\n",
    "\n",
    "# customers = pd.read_csv('../csv_pickle/Udacity_CUSTOMERS_052018.csv', sep=';')\n",
    "# print (customers.info())\n",
    "# customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_pickle('../csv_pickle/customers.pickle')\n",
    "customers.name = 'customers'\n",
    "print (customers.info())\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# missing value overview\n",
    "# '''\n",
    "\n",
    "# miss_val_overview(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "missing value by column\n",
    "'''\n",
    "\n",
    "miss_val_summary(customers, 'column')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> xxx colum, 891K row데이터 인데, 심하게 missing value 많은 컬럼이 있음  \n",
    "> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "info = pd.read_excel(\n",
    "    'DIAS Information Levels - Attributes 2017.xlsx', header=1).iloc[:, 1:]\n",
    "info.name = 'info'\n",
    "\n",
    "print (info.info())\n",
    "info.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> info은 dataset의 Feature 즉 column names가 의미하는 바를 알려주는 파일임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load and overview\n",
    "'''\n",
    "\n",
    "attr = pd.read_excel(\n",
    "    'DIAS Attributes - Values 2017.xlsx', header=1).iloc[:, 1:]\n",
    "attr.name = 'attr'\n",
    "\n",
    "print (attr.info())\n",
    "attr[103:108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not to run\n",
    "\n",
    "# view_all(attr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> attr은 dataset 각 column의 values가 의미하는 바를 알려주는 파일임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Understanding data contents\n",
    "\n",
    "4개의 파일을 통해 살펴보면 2개 데이터 셋의 Feature 들은 simplified German acronym으로 되어 있어 readable 하지 못하며,    \n",
    "2개의 information files를 통해 데이터셋 features 및 values 의 의미를 파악할 수 있음.   \n",
    "따라서 데이터를 제대로 explore하고, 전처리하기 위해 data set이 어떤 feature들로 구성되는 지를 먼저 이해하고자 함\n",
    "\n",
    "Therefore as the first step, I will understand the contents of data sets using 2 information files,     \n",
    "and record points to be pre-processed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 dataset features\n",
    "\n",
    "info와 attr은 dataset의 Feature 즉 column names가 의미하는 바를 알려주고 있음.   \n",
    "따라서 info와 attr를 사용한 feature_desc를 정리하여 data set이 어떤 feature들로 구성되는 지를 먼저 이해하고자 함\n",
    "\n",
    "As the data sets do not have information on what each feature (column name) exactly means,   \n",
    "values of information files (info & attr) should be mapped to the features at first,   \n",
    "to see how the data sets are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "add information to column names of customers to know what features datasets have \n",
    "'''\n",
    "\n",
    "# 먼저 모든 컬럼을 포함하고 있는 customers의 컬럼 이름을 값으로 갖는 Attribute 컬럼을 포함한 data frame을 만듦\n",
    "feature_desc = pd.DataFrame(customers.columns, columns=['Attribute'])\n",
    "feature_desc.name = 'feature_desc'\n",
    "\n",
    "# info에서 Attribute의 상세내용을 mapping 시킴\n",
    "feature_desc = vlookup(feature_desc, info, 'Attribute')\n",
    "\n",
    "feature_desc.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_desc has 369 unique Attribute values as customers have 369 columns:   \n",
    "264 common features in both of customers and info,    \n",
    "and 105 features of customers not having Description values.\n",
    "\n",
    "To enhance readability of data set - over 100 features are without description -   \n",
    "I added 12 Description values from attr, and made a column of translation (ger_to_eng)   \n",
    "for 93 features of which Descriptions are not found in both info and attr.   \n",
    "However as this code-running takes somewhat long time, I saved the dataframe processed    \n",
    "as df_feature.xlsx in root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For values of Attribute without Description, add 12 Description values from attr\n",
    "# attr_to_add = attr[['Attribute', 'Description']].copy()\n",
    "# attr_to_add.dropna(inplace = True)\n",
    "# attr_to_add = attr_to_add.rename(columns = {'Description': 'Description_to_add'})\n",
    "\n",
    "# feature_desc = vlookup(feature_desc, attr_to_add, 'Attribute')\n",
    "# feature_desc.Description = np.where(\n",
    "#     ((feature_desc.Description.isnull() == True) & (feature_desc.Description_to_add.isnull() == False)),\n",
    "#     feature_desc.Description_to_add,\n",
    "#     feature_desc.Description)\n",
    "# feature_desc = feature_desc.drop('Description_to_add', axis=1)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# feature_desc['ger_to_eng'] = np.where(\n",
    "#     feature_desc.Description.isnull(),\n",
    "#     feature_desc.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# feature_desc['Desc'] = feature_desc.Description.fillna('') + feature_desc.ger_to_eng.fillna('')\n",
    "\n",
    "feature_desc = pd.read_excel('feature_desc.xlsx', index_col = [0])\n",
    "# feature_desc.name = 'feature_desc'\n",
    "feature_desc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "understand all features one by one using custom function view_all.\n",
    "\n",
    "not to run this cell, to save space in final submission \n",
    "'''\n",
    "\n",
    "# view_all(feature_desc)\n",
    "\n",
    "feature_desc.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could not completely find exact meanings of all Attributes even with translations added,    \n",
    "but could have overall understandings on features.    \n",
    "As there are a few features containing similar contents, imputation steps for collinearity resolution   \n",
    "and dimensionality reduction are needed before modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "p_process dict to record point to be pre_processed\n",
    "'''\n",
    "\n",
    "p_process = {'2.1' : 'features of similar contents: collinearity resolution & dimensionality reduction'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 feature alignment\n",
    "\n",
    "Number of features by file and feature intersection & difference between files are as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    'azdias' : set(azdias.columns.unique()),\n",
    "    'customers' : set(customers.columns.unique()),\n",
    "    'info' : set(info.Attribute.dropna().unique()),\n",
    "    'attr' : set(attr.Attribute.dropna().unique()),\n",
    "    'feature_desc' : set(feature_desc.Attribute.dropna().unique())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of features by file')\n",
    "for k, v in feature_dict.items():\n",
    "    print (k, ':', len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Feature intersection & difference between files')\n",
    "for i in range(0, 5):\n",
    "    for j in range (0, 5):\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        key_1 = list(feature_dict.keys())[i]\n",
    "        key_2 = list(feature_dict.keys())[j]\n",
    "        set_1 = list(feature_dict.values())[i]\n",
    "        set_2 = list(feature_dict.values())[j]\n",
    "        \n",
    "        intsec = set_1.intersection(set_2)\n",
    "        ft_diff = set_1 - set_2\n",
    "        feature_dict[key_1 + '_intsec_' + key_2] = intsec\n",
    "        feature_dict['in_' + key_1 + '_notin_' + key_2] = ft_diff\n",
    "        \n",
    "        intsec_print = key_1 + ' & ' + key_2 + ' intersection:'\n",
    "        ft_diff_print = ' /  in ' + key_1 + ' & not-in ' + key_2 + ':'        \n",
    "        print (intsec_print, str(len(intsec)).rjust(41 - len(intsec_print)),\n",
    "               ft_diff_print, str(len(ft_diff)).rjust(40 - len(ft_diff_print)))\n",
    "\n",
    "'''\n",
    "customer에 있는데 feature_desc에 없는 것들 관련\n",
    "info, attr에 유사한 이름으로 있을 수 있음\n",
    "나중에 attr에 없는 값을 유추할 때 생각해 볼 사항\n",
    "\n",
    "attr이 커버하지 못하는 feature를 살펴보고, 어떻게 전처리 할 지 정리함   \n",
    "--> 컬럼명과 내용의 유사도를 파악하여 동일 전처리 룰 적용 / 또는 info에는 있는데 typo일 수도\n",
    "\n",
    "Unique Point가 될 수 있음\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 data set values\n",
    "\n",
    "attr은 각 column의 values가 의미하는 바를 알려주고 있음.  \n",
    "따라서 attr을 활용하여 각 feature가 어떤 데이터를 가지고 있는지 파악하고\n",
    "어떤 값들에 대한 전처리가 필요한지를 먼저 정리함\n",
    "\n",
    "2.2에서 살펴본 바와 같이 272개 피쳐는 attr을 통해 설명이 가능하므로\n",
    "\n",
    "The values of the data sets can also be readable by mapping values of Value and Meaning of attr.   \n",
    "In this section, 데이터 종류 (연속/이산), 이상치, 사실상의 null value 등 데이터 전처리를 위한 데이터의 내용적 측면을 점검하겠음."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 attr modification\n",
    "\n",
    "attr 내용은 neat하게 정리되어 있지 않은 상태로, 일부 Description 밑에    \n",
    "추가 설명으로 보이는 문구 셀이 있고, 각 블록의 첫 행에만 Attribute 및 Description     \n",
    "값이 있어 at first proper data format으로 변형함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "merge additional information cells to Description cells\n",
    "'''\n",
    "attr_org = attr.copy() # 먼저 기존 attr을 다른 이름으로 놓아 두고\n",
    "\n",
    "attr['description_shift'] = attr.Description.shift(-1).fillna('')\n",
    "# Description 밑에 있던 설명문구를 description_shift를 사용하여 병합\n",
    "attr.Description = attr.Description.mask(\n",
    "    ~(attr.Attribute.isna()),\n",
    "    attr.Description + ' ' + attr.description_shift)\n",
    "# 기존 Description 밑의 설명 문구 삭제\n",
    "attr.Description = attr.Description.mask(\n",
    "    (attr.Attribute.isna()) & ~(attr.Description.isna()),\n",
    "    np.nan) # 기존\n",
    "attr = attr.drop(columns = 'description_shift')\n",
    "\n",
    "'''\n",
    "fill null cells as only 1st lines of information have values\n",
    "'''\n",
    "attr[['Attribute', 'Description']] = attr[\n",
    "    ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "print (attr.info())\n",
    "attr.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 data values by type\n",
    "\n",
    "After extracting 272 features in both of customers and attr, I will analyze data values by type   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "extract 272 features\n",
    "'''\n",
    "in_attr = feature_dict['feature_desc_intsec_attr']\n",
    "in_attr = attr[attr.Attribute.isin(in_attr)].copy()\n",
    "\n",
    "print (in_attr.Attribute.nunique())\n",
    "in_attr.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 272 feature extracted, 2 features have NaN in their Meaning.    \n",
    "I decided to drop them, as they have other features with similar and more detailed Meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_attr[in_attr.Meaning.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_attr[in_attr.Attribute.str.contains('LP_FAMILIE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_attr[in_attr.Attribute.str.contains('LP_STATUS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['2.3.2'] = 'drop features LP_FAMILIE_GROB / LP_STATUS_GROB'\n",
    "\n",
    "in_attr = in_attr[(in_attr.Attribute != 'LP_FAMILIE_GROB')\n",
    "                        & (in_attr.Attribute != 'LP_STATUS_GROB')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column Value_type is added to divide remaining 270 features by data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_attr['Value_type'] = in_attr.Value.map(type)\n",
    "in_attr.Value_type.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.3 int values\n",
    "\n",
    "262 features in int type are categorized by their min/max values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "in_attr_num containing int type Value\n",
    "'''\n",
    "in_attr_num = in_attr[in_attr.Value_type == int]\n",
    "print ('number of features in int type:', in_attr_num.Attribute.nunique())\n",
    "\n",
    "'''\n",
    "pv_attr: summary of in_attr_num with min/max values categorized\n",
    "'''\n",
    "pv_attr = pd.pivot_table(\n",
    "    in_attr_num,\n",
    "    index = ['Attribute', 'Description'],\n",
    "    values = 'Value',\n",
    "    aggfunc = [min, max]\n",
    "    )\n",
    "pv_attr['min_max_cat'] = pv_attr['min'].astype(str) + ' to ' + pv_attr['max'].astype(str)\n",
    "pv_attr = pv_attr.sort_values(by = 'min_max_cat')\n",
    "\n",
    "pv_attr.min_max_cat.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From min/max categories above, all 262 features seem to contain discrete values.   \n",
    "To be further checked are:   \n",
    "* if Value -1 and 0 contain data equal to null or -inf   \n",
    "* if max Values contain data equal to null or inf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In case of Value -1 and 0:   \n",
    "  - Values with Meaning of list to_null should be replaced to NaN.   \n",
    "  - For numeric features exclusively in datasets (not in attr),   \n",
    "  Value -1 should be replaced to NaN as they mean 100% NaN in attr reference,   \n",
    "  and Value 0 should be kept as 'none' in Meaning is not for replacement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_num_mn1 = in_attr_num[in_attr_num.Value == -1]\n",
    "attr_num_mn1.Meaning.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_num_0 = in_attr_num[in_attr_num.Value == 0]\n",
    "attr_num_0.Meaning.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "select Meaning values to be NaN\n",
    "'''\n",
    "to_null = ['unknown','no transactions known', 'no classification possible',\n",
    "           'unknown / no main age detectable', 'classification not possible',\n",
    "           'no score calculated'\n",
    "            ]\n",
    "'''\n",
    "select Meaning values to be checked for NaN\n",
    "'''\n",
    "maybe_null = ['none']\n",
    "\n",
    "p_process['2.3.3-to_null'] = 'replace Values with Meaning in to_null to NaN'\n",
    "p_process['2.3.3-value_-1'] = 'for features not in attr, replace Value -1 to NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check if maybe_null value can be added to to_nul\n",
    "'''\n",
    "null_check = in_attr_num[in_attr_num['Meaning'].isin(maybe_null)]['Attribute'].tolist()\n",
    "null_check = in_attr_num[in_attr_num['Attribute'].isin(null_check)]\n",
    "null_check = vlookup(null_check, feature_desc, 'Attribute', ['Desc', 'Additional notes'], nan_val = 'no_info')\n",
    "pv_attr = pd.pivot_table(\n",
    "    null_check,\n",
    "    index = ['Attribute', 'Description', 'Desc', 'Additional notes'],\n",
    "    values = ['Meaning', 'Value'],\n",
    "    aggfunc = lambda x: list(x))\n",
    "pv_attr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In case of max Values, most Values with Meanings equal to NaN are already in list to_null,   \n",
    "  except Meaning 'inactive' added to list to_null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pv_attr_num에서 max Value가 7보다 큰 Attribute를 추출함\n",
    "# max_over_7 = pv_attr_num[pv_attr_num[('max', 'Value')] > 7].index.get_level_values(0)\n",
    "\n",
    "# # max가 7보다 큰 Attribut의 Meaning을 List에 담아 데이터가 discrete / continuous 여부를 파악\n",
    "# max_over_7 = in_attr_num[in_attr_num.Attribute.isin(max_over_7)]\n",
    "# pv_max_over_7 = pd.pivot_table(\n",
    "#     max_over_7,\n",
    "#     index = 'Attribute',\n",
    "#     values = 'Meaning',\n",
    "#     aggfunc = lambda x: x)\n",
    "\n",
    "# view_all(pv_max_over_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "extract Meaning of max values using corresponding index\n",
    "'''\n",
    "in_attr_num['Value'] = in_attr_num['Value'].astype(float)\n",
    "max_idxmax = in_attr_num.groupby('Attribute')['Value'].idxmax()\n",
    "\n",
    "in_attr_num.loc[max_idxmax, 'Meaning'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "select Meaning values to be checked for NaN\n",
    "'''\n",
    "maybe_null = ['uniformly distributed', 'no transactions known', 'Inactive', 'unremarkable',\n",
    "              'unknown', 'other', 'indifferent']\n",
    "\n",
    "maybe_null = [i for i in maybe_null if i not in to_null]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "extract Attributes of which max values are Meanings of list maybe_null\n",
    "'''\n",
    "max_check = in_attr_num.loc[max_idxmax]\n",
    "max_check = max_check[max_check['Meaning'].isin(maybe_null)]['Attribute'].tolist()\n",
    "'''\n",
    "check what maybe_null values to add to to_nul\n",
    "'''\n",
    "max_check = in_attr_num[in_attr_num['Attribute'].isin(max_check)]\n",
    "max_check = vlookup(max_check, feature_desc, 'Attribute', ['Desc', 'Additional notes'], nan_val = 'no_info')\n",
    "pv_attr = pd.pivot_table(\n",
    "    max_check,\n",
    "    index = ['Attribute', 'Description', 'Desc', 'Additional notes'],\n",
    "    values = ['Meaning', 'Value'],\n",
    "    aggfunc = lambda x: list(x))\n",
    "view_all(pv_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_null.append('Inactive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* max가 7 보다 큰 경우:   \n",
    "  모두 discrete로 파악됨, Max에 상은하는 Meaning에 null에 상응하는 데이터('no transactions known')가 있으나 이미 to_null에 포함되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pv_attr_num에서 max Value가 7보다 큰 Attribute를 추출함\n",
    "max_over_7 = pv_attr_num[pv_attr_num[('max', 'Value')] > 7].index.get_level_values(0)\n",
    "\n",
    "# max가 7보다 큰 Attribut의 Meaning을 List에 담아 데이터가 discrete / continuous 여부를 파악\n",
    "max_over_7 = in_attr_num[in_attr_num.Attribute.isin(max_over_7)]\n",
    "pv_max_over_7 = pd.pivot_table(\n",
    "    max_over_7,\n",
    "    index = 'Attribute',\n",
    "    values = 'Meaning',\n",
    "    aggfunc = lambda x: x)\n",
    "\n",
    "view_all(pv_max_over_7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.4 str values\n",
    "\n",
    "98 features in str type are analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_attr_str = in_attr[in_attr.Value_type == str]\n",
    "print ('number of features in str type:', in_attr_str.Attribute.nunique())\n",
    "print ('number of intersection features in int & str type:',\n",
    "       len(set(in_attr_str.Attribute.unique()).intersection(set(in_attr_num.Attribute.unique()))), '\\n')\n",
    "\n",
    "pv_attr = pd.pivot_table(\n",
    "    in_attr_str,\n",
    "    index = ['Attribute', 'Description', 'Meaning'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: list(x))\n",
    "\n",
    "pv_attr.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the case of 2 numbers paired to mean 'unknown' is detected, pivot table is remade disregarding this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_attr_str = in_attr_str[in_attr_str.Meaning != 'unknown']\n",
    "\n",
    "pv_attr_str = pd.pivot_table(\n",
    "    in_attr_str,\n",
    "    index = ['Attribute', 'Description'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: x)\n",
    "\n",
    "view_all(pv_attr_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMEO_DEU_2015 and OST_WEST_KZ contain discrete data.    \n",
    "There are 7 continuous data detected, of which skewness should be checked for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_feature = in_attr_str[in_attr_str['Meaning'].str.contains('numeric value')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_feature = in_attr_str[in_attr_str['Meaning'].str.contains('numeric value')]\n",
    "cont_ft = list(cont_feature['Attribute'].unique())\n",
    "p_process['3.2.2'] = 'check skewness for scaling of ' + ', '.join(cont_ft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset exploration\n",
    "\n",
    "Based on understandings on data contents from information files in section2 above,   \n",
    "I will explore 2 main datasets to finalize preparation for pre-processing.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create a concatenated dataframe of 2 datasets for temporary use\n",
    "'''\n",
    "concat_data = pd.concat([customers, azdias], axis=0)\n",
    "concat_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 numeric features\n",
    "\n",
    "For numeric features, concat_numeric including int and float type data is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_numeric = concat_data.select_dtypes(include = ['int', 'float']).copy()\n",
    "concat_num = concat_numeric.copy()\n",
    "concat_num.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And concat_num with the summary statistics is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract summary statistics of concat_numeric by applying describe and transpose\n",
    "concat_stat = concat_num.describe().T.reset_index()\n",
    "\n",
    "# merge Desc (information on Attribute) from feature_desc and add min_max_cat\n",
    "concat_stat = concat_stat.rename(columns = {'index' : 'Attribute'})\n",
    "concat_stat = vlookup(concat_stat, feature_desc, 'Attribute', 'Desc')\n",
    "concat_stat['min_max_cat'] = concat_stat[\n",
    "    'min'].apply(lambda x: '{:_.0f}'.format(x)).astype(str) + ' to ' + concat_stat[\n",
    "    'max'].apply(lambda x: '{:_.0f}'.format(x)).astype(str)\n",
    "\n",
    "concat_stat.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From min/max values\n",
    "* min Value -1 should be replaced to NaN - noted in 2.3.3\n",
    "* most features has max values not exceeding 40 and can be regarded as discrete - checked in 2.3.3\n",
    "* features with max values over 40 will be further analyzed below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_stat.min_max_cat.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 detailed feature check: numeric features with max over 40 & continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features with max values over 40 from concat_stat\n",
    "concat_high_max = concat_stat[concat_stat['max'] > 40][['Attribute', 'Desc']]\n",
    "ft_to_check = list(concat_high_max.Attribute.unique())\n",
    "\n",
    "# compare list of features of high max values with list of continuos features - cont_ft in 2.3.4 \n",
    "set (cont_ft) - set(ft_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_to_check.extend(['ANZ_HH_TITEL', 'ANZ_TITEL'])\n",
    "\n",
    "view_all(concat_stat[concat_stat['Attribute'].isin(ft_to_check)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LNR with the same count of concat_numeric seems to be the serial index of dataset, that it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2'] = 'drop LNR'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features in ft_to_check are analyzed one by one using function view_feature below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function] view_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_feature (feature, view_all = True, view_0_10 = True):\n",
    "    '''\n",
    "    function to view and check continuous numeric data\n",
    "    feature : str, feature name\n",
    "    view_0_10 : boolean for histogram display of value 0 to 10, default as True\n",
    "    '''\n",
    "\n",
    "    min_val = concat_numeric[feature].min()\n",
    "    max_val = concat_numeric[feature].max()\n",
    "    bin_edges = np.arange(min_val, max_val + 10, 10)\n",
    "    desc_val = concat_stat[concat_stat['Attribute'] == feature]['Desc'].values[0]\n",
    "\n",
    "    if view_all:\n",
    "        ax = concat_numeric[feature].plot(\n",
    "            kind = 'hist',\n",
    "            figsize=(10, 1.5),\n",
    "            color='gray',\n",
    "            bins = bin_edges,\n",
    "            align = 'mid',\n",
    "            title = ('histogram - ' + desc_val + ' - ' + feature)\n",
    "            );\n",
    "        ax.set_xlabel('Values - Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "        plt.show()\n",
    "\n",
    "    if view_0_10:    \n",
    "        ax = concat_numeric[feature].plot(\n",
    "            kind = 'hist',\n",
    "            figsize=(10, 1.5),\n",
    "            color='gray',\n",
    "            bins = np.arange(-0.5, 11.5, 1),\n",
    "            align = 'mid',\n",
    "            title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "            );\n",
    "        ax.set_xlabel('Values - Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "        plt.show()\n",
    "    \n",
    "    # define the outlier thresholds by applying multiplier 1.5\n",
    "    q1 = concat_stat[concat_stat['Attribute'] == feature]['25%'].values[0]\n",
    "    q3 = concat_stat[concat_stat['Attribute'] == feature]['75%'].values[0]\n",
    "    iqr = q3 - q1\n",
    "    lower_threshold = q1 - 1.5 * iqr\n",
    "    upper_threshold = q3 + 1.5 * iqr\n",
    "\n",
    "    # identify outliers\n",
    "    col_val = concat_numeric[feature].values\n",
    "    outliers = sorted(\n",
    "        set([feature for feature in col_val if feature < lower_threshold or feature > upper_threshold]),\n",
    "        reverse = True)\n",
    "\n",
    "    # print outliers\n",
    "    count_val = concat_stat[concat_stat['Attribute'] == feature].fillna(0)['count'].values[0]       \n",
    "    outlier_list = [\n",
    "        str(int(j)) + ': ' + '{:.1%}'.format((concat_numeric[feature] == j).sum() / count_val)\n",
    "        for j in outliers\n",
    "        ]\n",
    "    \n",
    "    print('Outliers (Value: %)')\n",
    "    for j in range(0, len(outlier_list), 10):\n",
    "        print (', '.join(outlier_list[j : j+10]))\n",
    "    print ('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_HAUSHALTE_AKTIV  \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "  - Log scale is needed due to high skewness  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_HAUSHALTE_AKTIV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_HAUSHALTE_AKTIV'] = '[Log scale]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_HH_TITEL  \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_HH_TITEL', False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_PERSONEN  \n",
    "  - Value 0: a household can not have 0 person, that Value 0 should be replaced to NaN\n",
    "  - outliers: household with over 10 persons is highly extreme or data error, that Value over 10 should be replaced to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_PERSONEN', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_PERSONEN'] = 'replace Value 0 to NaN / Value > 10 to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_STATISTISCHE_HAUSHALTE   \n",
    "  - No pre-processing needed: Value 0 and max value might be strange or extreme but are possible\n",
    "  - Log scale is needed due to high skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_STATISTISCHE_HAUSHALTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_STATISTISCHE_HAUSHALTE'] = '[Log scale]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANZ_STATISTISCHE_HAUSHALTE   \n",
    "  Value 0 and max value might be strange or extreme but are possible, but this feature is linked to ANZ_PERSONEN above\n",
    "  - replace value to NaN if corresponding ANZ_PERSONEN is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('ANZ_TITEL', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-ANZ_TITEL'] = 'replace value to NaN if ANZ_PERSONEN is NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EINGEZOGENAM_HH_JAHR \n",
    "  - outliers: Eng translation is not completely understandable, but 3 outliers can be replaced to NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('EINGEZOGENAM_HH_JAHR', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_numeric[concat_numeric['EINGEZOGENAM_HH_JAHR'] < 1980]['EINGEZOGENAM_HH_JAHR'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-AEINGEZOGENAM_HH_JAHR'] = 'replace Value < 1980 to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EXTSEL992   \n",
    "  - No pre-processing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('EXTSEL992')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GEBURTSJAHR  \n",
    "  - outliers: birth year can not be 0, that Value < 1900 should be replaced to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('GEBURTSJAHR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-GEBURTSJAHR'] = 'replace Value < 1900 to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GEMEINDETYP   \n",
    "  - No pre-processing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('GEMEINDETYP', True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KBA13_ANZAHL_PKW   \n",
    "  - Log scale is needed due to high skewness as values over 1250 is grouped by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('KBA13_ANZAHL_PKW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.2-KBA13_ANZAHL_PKW'] = '[Log scale]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MIN_GEBAEUDEJAHR   \n",
    "  - No pre-processing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('MIN_GEBAEUDEJAHR', True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VERDICHTUNGSRAUM   \n",
    "  - No pre-processing needed with Eng translation not completely understandable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_feature ('VERDICHTUNGSRAUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2 str values\n",
    "\n",
    "extract 5 features in str type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_str = concat_data.select_dtypes(exclude = ['int', 'float'])\n",
    "print(concat_str.shape)\n",
    "concat_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type EINGEFUEGT_AM should be changed to datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_str = concat_str.drop('EINGEFUEGT_AM', axis = 1)\n",
    "p_process['3.3.2'] = 'change type of EINGEFUEGT_AM to datetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract values of Attribute in list\n",
    "concat_str = pd.DataFrame(\n",
    "    {'Attribute': concat_str.columns,\n",
    "     'value_list': concat_str.values.T.tolist()})\n",
    "concat_str['value_list'] = concat_str['value_list'].apply(\n",
    "    lambda x: list(pd.Series(x).drop_duplicates().dropna()))\n",
    "\n",
    "# merge Desc (information on Attribute) from feature_desc and add min_max_cat\n",
    "concat_str = vlookup(concat_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "\n",
    "view_all(concat_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process['3.3.2-XX'] = 'replace Value X, XX to NaN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_process = {'pre_processing_itmes' : p_process}\n",
    "p_process_items = pd.DataFrame(p_process).reset_index()\n",
    "p_process_items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그 다음에 Preprocessing을 하고 나면 (필요시 극히 비슷한 컬럼 제외),   \n",
    "Imputing, Scaling 하면 PCA, Clustering, 앙상블로 나갈 수 있음** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 alignment of features   \n",
    "as values of data files (azdias & customers) can be readable by explanations of information files (info & attr),   \n",
    "check alignment in column features of data files and equivalent values of column Attribute of information files at first   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 합쳐서 데이터 분석시 참조\n",
    "> 93 데이터에만 있는 Attr은 어떻게 할 것인가?\n",
    "> 데이터 파일에 없고 정보 파일에만 있는 51 Attr은 제외하여 simplify 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 feature description\n",
    "As the data sets do not have information on what each feature (column name) exactly means,   \n",
    "values of information files (info & attr) should be mapped to the features at first,   \n",
    "to see how the data sets are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to know what features datasets have, add information to column names of customers\n",
    "'''\n",
    "\n",
    "feature_desc = pd.DataFrame(customers.columns, columns=['Attribute'])\n",
    "feature_desc = vlookup(feature_desc, info, 'Attribute')\n",
    "\n",
    "'''\n",
    "df_feature has 369 unique Attribute values:\n",
    "105 exclusive values of customers and 264 shared values with customers\n",
    "(see 1.5 alignment of features)\n",
    "''' \n",
    "print ('Attributes missing Description:', feature_desc[feature_desc.Description.isna()].shape[0])\n",
    "print (feature_desc.shape)\n",
    "feature_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To enhance readability of data set - over 100 features are without description -   \n",
    "I added 14 Description values from attr, and made a column of translation   \n",
    "(ger_to_eng) to df_feature using code below.\n",
    "However as this code-running takes somewhat long time, I saved the dataframe processed\n",
    "as df_feature.xlsx in root folder\n",
    "'''\n",
    "\n",
    "# # For values of Attribute without Description, add 14 Description values from attr\n",
    "# df_feature.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# df_feature.update(attr_excl)\n",
    "\n",
    "# df_feature.reset_index(inplace = True)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# df_feature['ger_to_eng'] = np.where(\n",
    "#     df_feature.Description.isnull(),\n",
    "#     df_feature.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# df_feature['Desc'] = df_feature.Description.fillna('') + df_feature.ger_to_eng.fillna('')\n",
    "\n",
    "feature_desc = pd.read_excel('feature_desc.xlsx', index_col = [0])\n",
    "feature_desc.head(10)\n",
    "\n",
    "print ('Attributes missing Desc:', feature_desc[feature_desc.Desc.isna()].shape[0])\n",
    "print (feature_desc.shape)\n",
    "feature_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find all features one by one\n",
    "'''\n",
    "\n",
    "# # not to run\n",
    "\n",
    "# view_all(feature_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(attr.Attribute.unique()) - set(feature_desc.Attribute.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (list(set(attr.Attribute.unique()) - set(feature_desc.Attribute.unique())))\n",
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df = attr.iloc[:, :2].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df[short_df.Attribute.isin(diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[attr.iloc[:, :2].drop_duplicates().Attribute.isin(diff)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 살펴 봤음. 모든 Attribute에 대해서 Description을 completely 이해할 수는 없었으나   \n",
    "> 대체적인 내용 구성을 이해할 수는 있었음   \n",
    "> 비슷한 내용을 나타내는 중복열, 유사열이 많아 공선성 해소, 차원 축소가 필요함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 data values\n",
    "The values of the data sets can also be readable by mapping values of Value and Meaning of attr.   \n",
    "In this section, 데이터 종류 (연속/이산), 이상치, 사실상의 null value 등 데이터 전처리를 위한 데이터의 내용적 측면을 점검하겠음.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check types of values of attr Value, which contains information on data values\n",
    "'''\n",
    "\n",
    "attr['Value_dtype'] = attr.Value.map(type)\n",
    "print (attr.Value_dtype.value_counts())\n",
    "\n",
    "# attr['Meaning_dtype'] = attr.Meaning.map(type)\n",
    "# print (attr.Meaning_dtype.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 attr의 int 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5에서 살펴본 바와 같이 272개 피쳐는 attr을 통해 설명이 가능함 (물론 이것도 데이터를 따로 파악해 봐야 하나)    \n",
    "데이터 셋을 직접 살펴 보는 것은 뒤에 별도록 진행하고 우선 attr을 분석하여 데이터 내용이 어떻게 구성되어 있는지 Basis를 확보해야 함.   \n",
    "\n",
    "2113개의 정수 값이고, 145개는 object로 정수 값으로 정의된 value를 먼저 점검해 보겠음.\n",
    "\n",
    "* numeric data of column Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "numeric data of column Value\n",
    "'''\n",
    "# attr_num with only numeric values in Value\n",
    "attr_num = attr[attr['Value_dtype'] == int].copy()\n",
    "print (attr_num.shape) \n",
    "\n",
    "# add Desc and Information level\n",
    "attr_num = vlookup(attr_num, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "print (attr_num.info())\n",
    "attr_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "customers에는 없고 attr에만 있는 42개 Attribute는 Desc가 없으으로\n",
    "(1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "'''\n",
    "\n",
    "print ('customers에는 없고 attr에만 있는 42개 Attribute 수:', \n",
    "       attr_num[attr_num.Desc.isna() == True].Attribute.nunique(),\n",
    "       '\\n')\n",
    "\n",
    "attr_num = attr_num[attr_num.Desc.isna() == False]\n",
    "print (attr_num.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "numeric data of column Value where 1774 rows have data in int type 으로\n",
    "pivot을 돌려 int type Value가 어떤 min max 값을 가지는 지 정리함.\n",
    "이를 통해 각 feature의 data type을 추정하고, 효과적으로 비정상 값을 찾아내려 함\n",
    "'''\n",
    "\n",
    "# summary of numeric data of Value\n",
    "pv_attr_num = pd.pivot_table(\n",
    "    attr_num,\n",
    "    index = ['Attribute', 'Desc'],\n",
    "    values = 'Value',\n",
    "    aggfunc = [min, max]\n",
    "    )\n",
    "\n",
    "pv_attr_num['min_max_cat'] = pv_attr_num['min'].astype(str) + ' to ' + pv_attr_num['max'].astype(str)\n",
    "pv_attr_num = pv_attr_num.sort_values(by = 'min_max_cat')\n",
    "\n",
    "print (\n",
    "    'min_max category of numeric data in column Value', '\\n',\n",
    "    pv_attr_num.min_max_cat.value_counts())\n",
    "pv_attr_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find all numeric data of column Value one by one\n",
    "'''\n",
    "\n",
    "# # not to run\n",
    "\n",
    "# with pd.option_context(\n",
    "#     'display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(pv_attr_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. value -1, 0의 경우 null 또는 -inf에 상응하는 데이터가 있는지  \n",
    "> 2. max가 7까지는 descrete, 8 이상은 continue 인지 봐야하고 Max의 null 또는 -inf에 상응하는 데이터가 있는지\n",
    "> 3. binary cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. value -1, 0의 경우 null 또는 -inf에 상응하는 데이터가 있는지 \n",
    "'''\n",
    "attr_below_1 = attr_num[attr_num.Value < 1]\n",
    "print(attr_below_1.shape)\n",
    "attr_below_1.Meaning.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "위 값 중 null 가능성이 있는 값만 추려 본결과\n",
    "'''\n",
    "\n",
    "maybe_null = ['unknown',\n",
    "              'no classification possible',\n",
    "              'unknown / no main age detectable',\n",
    "              'no transactions known', \n",
    "              'no transaction known', \n",
    "              'classification not possible',\n",
    "              'none',\n",
    "              'no score calculated'\n",
    "              ]\n",
    "\n",
    "attr_below_1 = attr_below_1[attr_below_1.Meaning.isin(maybe_null)].sort_values(by = 'Meaning')\n",
    "print(attr_below_1.shape)\n",
    "attr_below_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not to run\n",
    "\n",
    "# view_all(attr_below_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Meaning이 maybe_null 이면 모두 nan 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. max가 7까지는 descrete, 8 이상은 continue 인지 봐야하고 Max의 null 또는 -inf에 상응하는 데이터가 있는지\n",
    "'''\n",
    "\n",
    "# pv_attr_num에서 max가 7보다 큰 Attr을 추출함\n",
    "max_over_7_Attr = pv_attr_num[pv_attr_num[('max', 'Value')] > 7].index.get_level_values(0)\n",
    "# view_all(attr_num[attr_num.Attribute.isin(max_over_7_Attr)])\n",
    "max_over_7 = attr_num[attr_num.Attribute.isin(max_over_7_Attr)]\n",
    "pv_max_over_7 = pd.pivot_table(\n",
    "    max_over_7,\n",
    "    index = 'Attribute',\n",
    "    values = 'Meaning',\n",
    "    aggfunc = lambda x: list(x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all(pv_max_over_7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> no continuous data detected, 그러나 최대값에 maybe null이 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_over_7['Value'] = pd.to_numeric(max_over_7['Value'], errors='coerce')\n",
    "\n",
    "max_over_7_idxmax = max_over_7.groupby('Attribute')['Value'].idxmax()\n",
    "\n",
    "max_over_7.loc[max_over_7_idxmax, 'Meaning'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> maybe_null에 'uniformly distributed', ... 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "3. binary cells\n",
    "'''\n",
    "\n",
    "# pv_attr_num에서 max가 4보다 작은 Attr을 추출함\n",
    "max_under_4_Attr = pv_attr_num[pv_attr_num[('max', 'Value')] < 4].index.get_level_values(0)\n",
    "# view_all(attr_num[attr_num.Attribute.isin(max_over_7_Attr)])\n",
    "max_under_4 = attr_num[attr_num.Attribute.isin(max_under_4_Attr)]\n",
    "pv_max_under_4 = pd.pivot_table(\n",
    "    max_under_4,\n",
    "    index = 'Attribute',\n",
    "    values = 'Meaning',\n",
    "    aggfunc = lambda x: list(x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all(pv_max_under_4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 뭐뭐뭐가 이진으로 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 attr의 str 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "str data of column Value\n",
    "'''\n",
    "# attr_num with only numeric values in Value\n",
    "attr_str = attr[attr['Value_dtype'] == str].copy()\n",
    "print (attr_str.shape) \n",
    "\n",
    "# add Desc and Information level\n",
    "attr_str = vlookup(attr_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "print (attr_str.info())\n",
    "attr_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3개 null attribute는 정수와 문자를 모두 값을로 갖는 것들로 42개는 유효하고\n",
    "이 42개는 다음 section에서 볼 예정이므로 (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "'''\n",
    "\n",
    "# attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나...\n",
    "\n",
    "attr_str = attr_str[attr_str.Desc.isna() == False]\n",
    "print (attr_str.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pivot을 돌려 attribute 별로 어떤 str 값을 가지는 지 정리함.\n",
    "이를 통해 각 feature의 data type을 추정하고, 효과적으로 비정상 값을 찾아내려 함\n",
    "'''\n",
    "\n",
    "pv_attr_str = pd.pivot_table(\n",
    "    attr_str,\n",
    "    index = ['Attribute', 'Desc', 'Meaning'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: x\n",
    "    )\n",
    "\n",
    "pv_attr_str.head(10)\n",
    "# view_all(pv_attr_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. unknown을 표현하는 경우, drop에 포함    \n",
    "> 2. 연속형 수치를 표현하는 경우 ... 이는 data set을 직접 보고 파악해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "순수하게 str인 경우만 추출\n",
    "'''\n",
    "# pv_attr_str = pd.DataFrame(pv_attr_str.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_str_to_check = pv_attr_str[pv_attr_str.Meaning.str.contains('numeric value')].Attribute\n",
    "attr_str_to_check = pv_attr_str[\n",
    "    pv_attr_str.index.get_level_values(2).str.contains('numeric value')].index.get_level_values(0)\n",
    "# 먼저 추후 체크할 것들을 뽑아 놓고\n",
    "attr_str_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_attr_str = pd.DataFrame(pv_attr_str[\n",
    "    ~(pv_attr_str.index.get_level_values(0).isin(attr_str_to_check))\n",
    "    &~(pv_attr_str.index.get_level_values(2) == 'unknown')\n",
    "    ].to_records())\n",
    "\n",
    "pv_attr_str = pd.pivot_table(\n",
    "    pv_attr_str,\n",
    "    index = ['Attribute', 'Desc'],\n",
    "    values = 'Value',\n",
    "    aggfunc = lambda x: list(x)\n",
    "    )\n",
    "\n",
    "view_all(pv_attr_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CAMEO_DEU_2015 정상적인 카테고리 데이터... 피쳐   \n",
    "> OST_WEST_KZ은 2진"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 customers에만 있는 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**상당히 해깔리게 되어 있는데, 데이터의 컬럼과 정보 파일의 Attribute 숫자를 좀 정확하게 정리하고   \n",
    "하던데로 커스터머에만 있는 데이터를 정리하면 Wrangling이 끝남** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그 다음에 Preprocessing을 하고 나면 (필요시 극히 비슷한 컬럼 제외),   \n",
    "Imputing, Scaling 하면 PCA, Clustering, 앙상블로 나갈 수 있음** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. attributes_xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_val(attributes_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = list(attributes_xlsx.Meaning.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_words = []\n",
    "\n",
    "for synset in w_list:\n",
    "    if len(synset.lemmas()) > 1:\n",
    "        ambiguous_words.append(synset.name().split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(wordnet.all_synsets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wordnet.synsets('unknown')\n",
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "\n",
    "for synset in synsets:\n",
    "    for lemma in synset.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the target word\n",
    "target_word = 'unidentified'\n",
    "\n",
    "# Retrieve synsets for the target word\n",
    "synsets = wordnet.synsets(target_word)\n",
    "\n",
    "# # Retrieve synonyms for each synset and filter out synonyms containing the target word\n",
    "# filtered_synonyms = []\n",
    "\n",
    "# for synset in synsets:\n",
    "#     synonyms = synset.lemmas()\n",
    "#     filtered_synonyms.extend([synonym.name() for synonym in synonyms if target_word not in synonym.name()])\n",
    "\n",
    "# # Remove duplicate synonyms and sort the list\n",
    "# filtered_synonyms = sorted(set(filtered_synonyms))\n",
    "\n",
    "# print(filtered_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_synonyms = []\n",
    "\n",
    "for synset in synsets:\n",
    "    synonyms = synset.lemmas()\n",
    "    filtered_synonyms.extend([synonym.name() for synonym in synonyms if target_word not in synonym.name()])\n",
    "\n",
    "# Remove duplicate synonyms and sort the list\n",
    "filtered_synonyms = sorted(set(filtered_synonyms))\n",
    "\n",
    "print(filtered_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_word in filtered_synonyms:\n",
    "    # # Define the target word\n",
    "    # target_word = 'unidentified'\n",
    "\n",
    "    # Retrieve synsets for the target word\n",
    "    synsets = wordnet.synsets(target_word)\n",
    "\n",
    "    for synset in synsets:\n",
    "        synonyms = synset.lemmas()\n",
    "        filtered_synonyms.extend([synonym.name() for synonym in synonyms if target_word not in synonym.name()])\n",
    "\n",
    "    # Remove duplicate synonyms and sort the list\n",
    "    filtered_synonyms = sorted(set(filtered_synonyms))\n",
    "\n",
    "print(filtered_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customers\n",
    "\n",
    "print (customers.info())\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes_xlsx\n",
    "\n",
    "print (attributes_xlsx.info())\n",
    "attributes_xlsx.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify attributes_xlsx\n",
    "\n",
    "attributes_xlsx = attributes_xlsx.iloc[:, 1:] # 1st column has no info\n",
    "attributes_xlsx[['Attribute', 'Description']] = attributes_xlsx[\n",
    "    ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "print (attributes_xlsx.info())\n",
    "attributes_xlsx.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information_xlsx\n",
    "\n",
    "print (information_xlsx.info())\n",
    "information_xlsx.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work / Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def col_val (df):\n",
    "#     '''\n",
    "#     function to check values of dataframe columns\n",
    "#     df : dataframe\n",
    "#     '''\n",
    "#     # for i in df.columns:\n",
    "#     #     print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#     #         df[i].value_counts(), '\\n', '*     *     *')\n",
    "#     for i in df.columns:\n",
    "#         print (i, '-', df[i].nunique(), 'values', '\\n',\n",
    "#         list(df[i].unique()), '\\n', '*     *     *')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from googletrans import Translator\n",
    "\n",
    "# # Create an instance of the Translator\n",
    "# translator = Translator(service_urls=['translate.google.com'])\n",
    "\n",
    "# # Text to be translated\n",
    "# text = \"AGER_TYP\"\n",
    "\n",
    "# # Translate the text from German to English\n",
    "# translation = translator.translate(text, src='de', dest='en')\n",
    "\n",
    "# # Print the translated text\n",
    "# print(\"Original text (German):\", text)\n",
    "# print(\"Translated text (English):\", translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def miss_val_hist(df, axis_val, x_bin = 10):\n",
    "#     '''\n",
    "#     function to display missing value histogram by column or raw\n",
    "#     df: dataframe\n",
    "#     axis_val: str, one of 'column' or 'row'\n",
    "#     x_bin: size of xtick bin, 10 as default \n",
    "#     '''\n",
    "#     # axis value\n",
    "#     axis_num = 0 if axis_val == 'column' else 1\n",
    "    \n",
    "#     # % of missing values\n",
    "#     missing_pct = df.isnull().mean(axis = axis_num) * 100\n",
    "\n",
    "#     # max % of missing values by column\n",
    "#     missing_pct_max = missing_pct.max()\n",
    "#     print ('max % of missing values by ' + axis_val + ': ', missing_pct_max)\n",
    "\n",
    "#     # plot missing values by column\n",
    "    \n",
    "#     print (missing_pct.describe())\n",
    "    \n",
    "#     x_range = ((missing_pct_max + x_bin * 2) // x_bin) * x_bin\n",
    "\n",
    "#     ax = missing_pct.plot(\n",
    "#         kind = 'hist', figsize=(10, 3), color='gray',\n",
    "#         bins = np.arange(0, x_range, 10),\n",
    "#         title = (df.name + ': missing value by ' + axis_val)\n",
    "#         )\n",
    "#     ax.set_xlabel('% of missing value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이써닉 하지 못한 코드\n",
    "# # missing value overview\n",
    "# for i in range(0, ((azdias.shape[1] + 100) // 100) * 100, 100):\n",
    "#     msno.matrix(azdias.iloc[:, i : i + 99],\n",
    "#                 figsize=(10, 3), fontsize = 12, labels = False, sparkline = False)\n",
    "#     plt.title('missing value overview: col ' + str (i) + ' to ' + str (min(i + 99, azdias.shape[1] - 1)),\n",
    "#               fontsize = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of over 30%\n",
    "# (azdias.isnull().mean() * 100 > 30).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % of columns with missing values of 25% to 30%\n",
    "# ((azdias.isnull().mean() * 100 > 25) & (30 >= azdias.isnull().mean() * 100)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아마도 쓰지 않을 plot\n",
    "# plot = azdias_col_missing_pct.plot(\n",
    "#     kind = 'bar', figsize=(10, 3), color='dimgray', xticks = [],\n",
    "#     title = 'azdias_col_missing_pct',\n",
    "#     xlabel = '366 columns',\n",
    "#     ylabel = '% of missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_df = customers.select_dtypes(include=['float', 'int64']).iloc[:, 1:]\n",
    "# num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pd.unique(customers.select_dtypes(include='float').values.flatten()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\n",
    "#     'display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(pd.DataFrame(attr.apply(lambda x: x.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_not_null = ~(attr.Attribute.isna())\n",
    "# attr.loc[attr_not_null, 'Description'] = attr.loc[\n",
    "#     attr_not_null, 'Description'] + ' ' + attr.loc[attr_not_null, 'desc_shift']\n",
    "\n",
    "# desc_to_null = (attr.Attribute.isna()) & ~(attr.Description.isna())\n",
    "# attr.loc[desc_to_null, 'Description'] = np.nan\n",
    "# attr = attr.drop(columns = 'desc_shift')\n",
    "# attr.loc[attr_with_value.shift(-1, fill_value = True), 'Description']\n",
    "# attr_shift = attr_null.shift\n",
    "# attr[attr_null.shift, 'Description'] = attr.loc[\n",
    "#     attr_null.shift(fill_value = False), 'Description'] + ' ' + attr[attr_null, 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# info_mg = info_mg.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "# # attr_excl = attr[attr.Attribute.isin(attr_excl)].copy()\n",
    "# attr_not_null = attr.dropna(subset = 'Attribute').copy()\n",
    "# attr_not_null = attr_not_null.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_not_null[['Attribute', 'Description']]],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 313 rows of Attribute in info, add 15 values exclusively in attr,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# info_mg = info.iloc[:, 1:].copy()\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_excl)][['Attribute', 'Description']].copy()\n",
    "\n",
    "# info_mg = pd.concat(\n",
    "#     [info_mg, attr_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# info_mg = info_mg.drop_duplicates(subset = ['Attribute', 'Description'])\n",
    "# # info_mg = info_mg.sort_values(by = list(info_mg.columns), ascending=False)\n",
    "# # # info_mg = info_mg.drop_duplicates(subset='Attribute')\n",
    "\n",
    "# info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(info_mg.info())\n",
    "# info_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_all(info_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# to 2258 rows of Attribute in attr, add 13 values exclusively in info,\n",
    "# and remove 52 values exclusively in information files\n",
    "# '''\n",
    "\n",
    "# attr_mg = attr.copy()\n",
    "# info_excl = info[info.Attribute.isin(info_excl)][['Attribute', 'Description']].copy()\n",
    "# info_excl['Value'] = 'form info'\n",
    "# info_excl['Meaning'] = 'form info'\n",
    "\n",
    "# attr_mg = pd.concat(\n",
    "#     [attr_mg, info_excl],\n",
    "#     ignore_index  = True,\n",
    "#     axis = 0\n",
    "#     )\n",
    "# # info_mg = info_mg.drop_duplicates()\n",
    "\n",
    "# # info_mg = info_mg[~(info_mg.Attribute.isin(infofile_excl))]\n",
    "\n",
    "# print(attr_mg.info())\n",
    "# attr_mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_mg.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# there are 93 and 51 exclusive values in data and information files\n",
    "# '''\n",
    "\n",
    "# datafile_Attr = azdias_Attr.union(customers_Attr)\n",
    "# infofile_Attr = info_Attr.union(attr_Attr)\n",
    "\n",
    "# datafile_excl = datafile_Attr - infofile_Attr\n",
    "# infofile_excl = infofile_Attr - datafile_Attr\n",
    "\n",
    "# print (len(datafile_excl), 'Attribute value(s) exclusively in data files:',\n",
    "#        '\\n', datafile_excl)\n",
    "# print (len(infofile_excl), 'Attribute value(s) exclusively in information files:',\n",
    "#        '\\n', infofile_excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# info['Information level'] = info['Information level'].fillna(method = 'ffill')\n",
    "\n",
    "# info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 한 것으로 보임\n",
    "# '''\n",
    "# fill null cells as only 1st lines of information have values\n",
    "# '''\n",
    "\n",
    "# attr[['Attribute', 'Description']] = attr[\n",
    "#     ['Attribute', 'Description']].fillna(method = 'ffill')\n",
    "\n",
    "# attr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# To enhance readability of data set - over 100 features are without description -   \n",
    "# I added 14 Description values from attr, and made a column of translation   \n",
    "# (ger_to_eng) to df_feature using code below.\n",
    "# However as this code-running takes somewhat long time, I saved the dataframe processed\n",
    "# as df_feature.xlsx in root folder\n",
    "# '''\n",
    "\n",
    "# # For values of Attribute without Description, add 14 Description values from attr\n",
    "# df_feature.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# df_feature.update(attr_excl)\n",
    "\n",
    "# df_feature.reset_index(inplace = True)\n",
    "\n",
    "# # For values of Attribute without Description, make colum of translation (ger_to_eng)\n",
    "# def ger_to_eng (ger_text):\n",
    "#     '''\n",
    "#     function to translate German text\n",
    "#     '''    \n",
    "#     translator = Translator(service_urls=['translate.google.com'])    \n",
    "#     try:\n",
    "#         translation = translator.translate(ger_text, src='de', dest='en')\n",
    "#         return translation.text        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# df_feature['ger_to_eng'] = np.where(\n",
    "#     df_feature.Description.isnull(),\n",
    "#     df_feature.Attribute.str.replace('_', ' ').apply(ger_to_eng),\n",
    "#     np.nan)\n",
    "# df_feature['Desc'] = df_feature.Description.fillna('') + df_feature.ger_to_eng.fillna('')\n",
    "\n",
    "# # # sort by Attribute and Information level\n",
    "# # df_feature.sort_values(by = ['Attribute', 'Information level'], inplace= True)\n",
    "\n",
    "# # df_feature = pd.read_excel('df_feature.xlsx', index_col = [0])\n",
    "# # df_feature.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_excl = attr[attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# df_feature.Description = df_feature.Description.mask(\n",
    "#     df_feature.Attribute == attr_excl.Attribute,\n",
    "#     attr_excl.Description\n",
    "#     )\n",
    "# print ('Attributes missing Description:', df_feature[df_feature.Description.isna()].shape[0])\n",
    "# print (df_feature.shape)\n",
    "# df_feature.head()\n",
    "\n",
    "# ValueError: Can only compare identically-labeled Series objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_desc.set_index('Attribute', inplace = True)\n",
    "\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_excl = attr[\n",
    "#     attr.Attribute.isin(attr_Attr - info_Attr)][['Attribute', 'Description']].copy()\n",
    "# attr_excl.set_index('Attribute', inplace = True)\n",
    "# feature_desc.update(attr_excl)\n",
    "\n",
    "# feature_desc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between data files: \n",
    "# df customers has 3 more exclusive columns\n",
    "# '''\n",
    "\n",
    "# azdias_Attr = set(azdias.columns)\n",
    "# customers_Attr = set(customers.columns)\n",
    "\n",
    "# print(azdias_Attr - customers_Attr)\n",
    "# print(customers_Attr - azdias_Attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# alignment of features between information files\n",
    "# '''\n",
    "# info_Attr = set(info.Attribute.dropna().unique())\n",
    "# attr_Attr = set(attr.Attribute.dropna().unique())\n",
    "\n",
    "# # info_excl = info_Attr - attr_Attr\n",
    "# # attr_excl = attr_Attr - info_Attr\n",
    "\n",
    "# print (len(info_Attr - attr_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - info_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - info_Attr)\n",
    "# '''\n",
    "# alignment of features between customers and information files\n",
    "# '''\n",
    "# print ('Attribute between customers and info')\n",
    "# print (len(customers_Attr - info_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - info_Attr)\n",
    "# print (len(info_Attr - customers_Attr), 'Attribute value(s) exclusively in info:',\n",
    "#        '\\n', info_Attr - customers_Attr)\n",
    "# print ('In', len(info_Attr), 'features of info,', \n",
    "#        len(info_Attr) - len(info_Attr - customers_Attr), 'features are in Attribute of customers', '\\n')\n",
    "\n",
    "# print ('Attribute between customers and attr')\n",
    "# print (len(customers_Attr - attr_Attr), 'feature(s) exclusively in customers:',\n",
    "#        '\\n', customers_Attr - attr_Attr)\n",
    "# print (len(attr_Attr - customers_Attr), 'Attribute value(s) exclusively in attr:',\n",
    "#        '\\n', attr_Attr - customers_Attr)\n",
    "# print ('In', len(attr_Attr), 'features of attr,',\n",
    "#        len(attr_Attr) - len(attr_Attr - customers_Attr), 'features are in Attribute of customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(feature_dict.keys())[0]\n",
    "# feature_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = np.array([1, 2, 3, 4])\n",
    "\n",
    "# subtractions = np.subtract.outer(values, values)[np.triu_indices(len(values), k=1)]\n",
    "\n",
    "# for result in subtractions:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (k, v) in enumerate(feature_dict.items()):\n",
    "#     for j in range(i + 1, 4):\n",
    "#             result = values[i] - values[j]\n",
    "#             print(f\"{values[i]} - {values[j]} = {result}\")\n",
    "#     print (i, k, v)\n",
    "    \n",
    "# for i, (k, v) in enumerate(zip(list(feature_dict.keys()), list(feature_dict.values()))):\n",
    "#     print (i, (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# customers에는 없고 attr에만 있는 42개 Attribute는 Desc가 없으으로\n",
    "# (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# print ('customers에는 없고 attr에만 있는 42개 Attribute 수:', \n",
    "#        attr_num[attr_num.Desc.isna() == True].Attribute.nunique(),\n",
    "#        '\\n')\n",
    "\n",
    "# attr_num = attr_num[attr_num.Desc.isna() == False]\n",
    "# print (attr_num.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# str data of column Value\n",
    "# '''\n",
    "# # attr_num with only numeric values in Value\n",
    "# attr_str = attr[attr['Value_dtype'] == str].copy()\n",
    "# print (attr_str.shape) \n",
    "\n",
    "# # add Desc and Information level\n",
    "# attr_str = vlookup(attr_str, feature_desc, 'Attribute', ['Desc', 'Additional notes'])\n",
    "# print (attr_str.info())\n",
    "# attr_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 3개 null attribute는 정수와 문자를 모두 값을로 갖는 것들로 42개는 유효하고\n",
    "# 이 42개는 다음 section에서 볼 예정이므로 (1.5 alignment) desc가 null이 아닌 행만 keep\n",
    "# '''\n",
    "\n",
    "# # attr_str[attr_str.Desc.isna() == True] # int와 str을 모두 갖는 셀. 따라서 42는 맞음.... 이 별로 중요하지도 않은 것을 남겨야 하나...\n",
    "\n",
    "# attr_str = attr_str[attr_str.Desc.isna() == False]\n",
    "# print (attr_str.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attr_str_to_check = pv_attr_str[pv_attr_str.Meaning.str.contains('numeric value')].Attribute\n",
    "# attr_str_to_check = pv_attr_str[\n",
    "#     pv_attr_str.index.get_level_values(2).str.contains('numeric value')].index.get_level_values(0)\n",
    "# # 먼저 추후 체크할 것들을 뽑아 놓고\n",
    "# attr_str_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pv_attr_str = pd.DataFrame(pv_attr_str[\n",
    "#     ~(pv_attr_str.index.get_level_values(0).isin(attr_str_to_check))\n",
    "#     &~(pv_attr_str.index.get_level_values(2) == 'unknown')\n",
    "#     ].to_records())\n",
    "\n",
    "# pv_attr_str = pd.pivot_table(\n",
    "#     pv_attr_str,\n",
    "#     index = ['Attribute', 'Desc'],\n",
    "#     values = 'Value',\n",
    "#     aggfunc = lambda x: list(x)\n",
    "#     )\n",
    "\n",
    "# view_all(pv_attr_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(not_in_attr_str.values.T.shape)\n",
    "# not_in_attr_str.values.T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + bin_interval, bin_interval)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 5.0\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 5.0 * iqr\n",
    "#     upper_threshold = q3 + 5.0 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(set([i for i in col_val if i < lower_threshold or i > upper_threshold]), reverse = True)\n",
    "#     for j in outliers:\n",
    "#         print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "#     print ('\\n', '==========' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in concat_cont_ft[1:]:\n",
    "\n",
    "#     min_val = cc_num[i].min()\n",
    "#     max_val = cc_num[i].max()\n",
    "#     # bin_interval = 1\n",
    "#     bin_edges = np.arange(min_val, max_val + 10, 10)\n",
    "\n",
    "#     desc_val = concat_num[concat_num['Attribute'] == i]['Desc'].values[0]\n",
    "#     count_val = int(concat_num[concat_num['Attribute'] == i].fillna(0)['count'].values[0])\n",
    "\n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = bin_edges,\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' ' + i)\n",
    "#         );\n",
    "#     ax.set_xlabel('Values: Min: ' + str(int(min_val)) + ', Max: ' + str(int(max_val)));\n",
    "#     plt.show()\n",
    "    \n",
    "#     ax = cc_num[i].plot(\n",
    "#         kind = 'hist',\n",
    "#         figsize=(10, 1.5),\n",
    "#         color='gray',\n",
    "#         bins = np.arange(-0.5, 11.5, 1),\n",
    "#         align = 'mid',\n",
    "#         title = ('histogram - ' + desc_val + ' - Value 0 to 10')\n",
    "#         );\n",
    "#     ax.set_xlabel('Values');\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Define the outlier thresholds by applying multiplier 1.5\n",
    "#     q1 = concat_num[concat_num['Attribute'] == i]['25%'].values[0]\n",
    "#     q3 = concat_num[concat_num['Attribute'] == i]['75%'].values[0]\n",
    "#     iqr = q3 - q1\n",
    "#     lower_threshold = q1 - 1.5 * iqr\n",
    "#     upper_threshold = q3 + 1.5 * iqr\n",
    "\n",
    "#     # Identify outliers\n",
    "#     col_val = cc_num[i].values\n",
    "#     outliers = sorted(\n",
    "#         set([i for i in col_val if i < lower_threshold or i > upper_threshold]),\n",
    "#         reverse = True)\n",
    "#     # for j in outliers:\n",
    "#     #     print (int(j), '{:.1%}'.format(((cc_num[i] == j).sum())/count_val*100), end = ' ')\n",
    "        \n",
    "#     outlier_list = [str(int(j)) + ': ' + '{:.1%}'.format((cc_num[i] == j).sum() / count_val)\n",
    "#                     for j in outliers]\n",
    "    \n",
    "#     print('Outliers (Value: %)')\n",
    "#     for j in range(0, len(outlier_list), 10):\n",
    "#         print (', '.join(outlier_list[j: j+10]))\n",
    "#     print ('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
